{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5_PersonAttrubutes_vanilla16.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ckgpeace/EIP4/blob/master/Assignment5/AssignmentOther_model/Assignment5_PersonAttrubutes_vanilla16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El-Dopn8pLpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "aa2ae783-6d86-4940-b991-b2a27f3dc24f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "from keras.layers import Activation, Flatten, Dense, Dropout, GlobalAveragePooling2D, AveragePooling2D, Add\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, SeparableConv2D\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input, BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "c58e4cf9-587a-44c5-c7c8-be7b67b083c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "4a4daf7a-43a4-4035-b4c4-c6cad2c653cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=64, shuffle=True, augmentation = None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.resize(cv2.imread(item[\"image_path\"]), (112,112)) for _, item in items.iterrows()])\n",
        "        #Image Normalization\n",
        "        if self.augmentation is not None:\n",
        "          self.augmentation.fit(image)\n",
        "          image = self.augmentation.flow(image,shuffle=False, batch_size = 64 ).next()\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "553964e5-936f-4883-87d2-101f3f8b664b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15, random_state = 404)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "ce722e35-741d-4530-cd26-bae86ba4465a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10980</th>\n",
              "      <td>resized/10982.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11135</th>\n",
              "      <td>resized/11137.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1784</th>\n",
              "      <td>resized/1785.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7758</th>\n",
              "      <td>resized/7759.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4692</th>\n",
              "      <td>resized/4693.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "10980  resized/10982.jpg              0  ...                        1              0\n",
              "11135  resized/11137.jpg              0  ...                        0              1\n",
              "1784    resized/1785.jpg              0  ...                        1              0\n",
              "7758    resized/7759.jpg              0  ...                        0              1\n",
              "4692    resized/4693.jpg              0  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=64, shuffle=True, \n",
        "                                augmentation = ImageDataGenerator(horizontal_flip=True, \n",
        "                                                                  width_shift_range=0.2,\n",
        "                                                                  height_shift_range=0.2,\n",
        "                                                                  rotation_range=15,\n",
        "                                                                  zoom_range=0.2,\n",
        "                                                                  featurewise_center=True, featurewise_std_normalization=True))\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False, augmentation= ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "456f1060-6fb1-4fb6-902c-23fd89274123",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "outputId": "cbf38906-6352-4f8d-92fa-cd4854410f4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "## Vanilla10 - skip connection\n",
        "\n",
        "inp = Input(shape = (112,112,3))\n",
        "x = inp\n",
        "### block 1\n",
        "\n",
        "x = SeparableConv2D(filters=32, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=32, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=64, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=64, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# Pooling \n",
        "x = AveragePooling2D()(x)\n",
        "\n",
        "# ================================================================\n",
        "\n",
        "### block\n",
        "\n",
        "x_short = x\n",
        "x = SeparableConv2D(filters=64, kernel_size=(3, 3), padding='same')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=64, kernel_size=(3, 3), padding='same')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=128, kernel_size=(3, 3), padding='same')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=128, kernel_size=(3, 3), padding='same')(x)\n",
        "\n",
        "x_short =  SeparableConv2D(filters=128, kernel_size=(3, 3), padding='same')(x_short) \n",
        "x = Add()([x, x_short])\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# Pooling \n",
        "x = AveragePooling2D()(x)\n",
        "\n",
        "# ================================================================\n",
        "\n",
        "### block\n",
        "x_short = x\n",
        "x = SeparableConv2D(filters=128, kernel_size=(3, 3), padding='same')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=128, kernel_size=(3, 3), padding='same')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=256, kernel_size=(3, 3), padding='same')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.05)(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=256, kernel_size=(3, 3), padding='same')(x)\n",
        "\n",
        "x_short =  SeparableConv2D(filters=256, kernel_size=(3, 3), padding='same')(x_short) \n",
        "x = Add()([x, x_short])\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.05)(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# Pooling \n",
        "x = AveragePooling2D()(x)\n",
        "\n",
        "# ================================================================\n",
        "# block  - Last CNN\n",
        "\n",
        "x = SeparableConv2D(filters=512, kernel_size=(3, 3), padding='valid')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=512, kernel_size=(3, 3), padding='valid')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# GAP\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Adding Dense layer\n",
        "def final(in_layer, num_units, class_name, output_name):\n",
        "  x = Dense(128, activation=\"relu\" )(in_layer)\n",
        "  x = Dense(128, activation=\"relu\" )(x)\n",
        "  x = Dense(num_units[class_name], activation=\"softmax\", name = output_name )(x)\n",
        "  return x\n",
        "\n",
        "gender = final(in_layer = x, num_units = num_units,  class_name = \"gender\", output_name = \"gender_output\")\n",
        "image_quality = final(in_layer = x, num_units = num_units,  class_name = \"image_quality\", output_name = \"image_quality_output\")\n",
        "age = final(in_layer = x, num_units = num_units,  class_name = \"age\", output_name = \"age_output\")\n",
        "weight = final(in_layer = x, num_units = num_units,  class_name = \"weight\", output_name = \"weight_output\")\n",
        "bag = final(in_layer = x, num_units = num_units,  class_name = \"bag\", output_name = \"bag_output\")\n",
        "footwear = final(in_layer = x, num_units = num_units,  class_name = \"footwear\", output_name = \"footwear_output\")\n",
        "emotion = final(in_layer = x, num_units = num_units,  class_name = \"emotion\", output_name = \"emotion_output\")\n",
        "pose = final(in_layer = x, num_units = num_units,  class_name = \"pose\", output_name = \"pose_output\")\n",
        "\n",
        "model = Model(inputs = inp,outputs=[gender, image_quality, age, weight, bag, pose, footwear, emotion])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            (None, 112, 112, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_136 (Separable (None, 110, 110, 32) 155         input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_100 (Activation)     (None, 110, 110, 32) 0           separable_conv2d_136[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_100 (BatchN (None, 110, 110, 32) 128         activation_100[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_137 (Separable (None, 108, 108, 32) 1344        batch_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_101 (Activation)     (None, 108, 108, 32) 0           separable_conv2d_137[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, 108, 108, 32) 128         activation_101[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_138 (Separable (None, 106, 106, 64) 2400        batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_102 (Activation)     (None, 106, 106, 64) 0           separable_conv2d_138[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, 106, 106, 64) 256         activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_139 (Separable (None, 104, 104, 64) 4736        batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_103 (Activation)     (None, 104, 104, 64) 0           separable_conv2d_139[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, 104, 104, 64) 256         activation_103[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_18 (AveragePo (None, 52, 52, 64)   0           batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_140 (Separable (None, 52, 52, 64)   4736        average_pooling2d_18[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_104 (Activation)     (None, 52, 52, 64)   0           separable_conv2d_140[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, 52, 52, 64)   256         activation_104[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_141 (Separable (None, 52, 52, 64)   4736        batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_105 (Activation)     (None, 52, 52, 64)   0           separable_conv2d_141[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 52, 52, 64)   256         activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_142 (Separable (None, 52, 52, 128)  8896        batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_106 (Activation)     (None, 52, 52, 128)  0           separable_conv2d_142[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, 52, 52, 128)  512         activation_106[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_143 (Separable (None, 52, 52, 128)  17664       batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_144 (Separable (None, 52, 52, 128)  8896        average_pooling2d_18[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 52, 52, 128)  0           separable_conv2d_143[0][0]       \n",
            "                                                                 separable_conv2d_144[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_107 (Activation)     (None, 52, 52, 128)  0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, 52, 52, 128)  512         activation_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_19 (AveragePo (None, 26, 26, 128)  0           batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_145 (Separable (None, 26, 26, 128)  17664       average_pooling2d_19[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_108 (Activation)     (None, 26, 26, 128)  0           separable_conv2d_145[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_108 (BatchN (None, 26, 26, 128)  512         activation_108[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_146 (Separable (None, 26, 26, 128)  17664       batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_109 (Activation)     (None, 26, 26, 128)  0           separable_conv2d_146[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_109 (BatchN (None, 26, 26, 128)  512         activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_147 (Separable (None, 26, 26, 256)  34176       batch_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, 26, 26, 256)  0           separable_conv2d_147[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 26, 26, 256)  0           activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_110 (BatchN (None, 26, 26, 256)  1024        dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_148 (Separable (None, 26, 26, 256)  68096       batch_normalization_110[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_149 (Separable (None, 26, 26, 256)  34176       average_pooling2d_19[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 26, 26, 256)  0           separable_conv2d_148[0][0]       \n",
            "                                                                 separable_conv2d_149[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 26, 26, 256)  0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 26, 26, 256)  0           activation_111[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, 26, 26, 256)  1024        dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_20 (AveragePo (None, 13, 13, 256)  0           batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_150 (Separable (None, 11, 11, 512)  133888      average_pooling2d_20[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 11, 11, 512)  0           separable_conv2d_150[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, 11, 11, 512)  2048        activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_151 (Separable (None, 9, 9, 512)    267264      batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 9, 9, 512)    0           separable_conv2d_151[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_113 (BatchN (None, 9, 9, 512)    2048        activation_113[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_27 (Gl (None, 512)          0           batch_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_33 (Dense)                (None, 128)          65664       global_average_pooling2d_27[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_35 (Dense)                (None, 128)          65664       global_average_pooling2d_27[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_37 (Dense)                (None, 128)          65664       global_average_pooling2d_27[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_39 (Dense)                (None, 128)          65664       global_average_pooling2d_27[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_41 (Dense)                (None, 128)          65664       global_average_pooling2d_27[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_47 (Dense)                (None, 128)          65664       global_average_pooling2d_27[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_43 (Dense)                (None, 128)          65664       global_average_pooling2d_27[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_45 (Dense)                (None, 128)          65664       global_average_pooling2d_27[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_34 (Dense)                (None, 128)          16512       dense_33[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_36 (Dense)                (None, 128)          16512       dense_35[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_38 (Dense)                (None, 128)          16512       dense_37[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_40 (Dense)                (None, 128)          16512       dense_39[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_42 (Dense)                (None, 128)          16512       dense_41[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_48 (Dense)                (None, 128)          16512       dense_47[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_44 (Dense)                (None, 128)          16512       dense_43[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_46 (Dense)                (None, 128)          16512       dense_45[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Dense)           (None, 2)            258         dense_34[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "image_quality_output (Dense)    (None, 3)            387         dense_36[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "age_output (Dense)              (None, 5)            645         dense_38[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "weight_output (Dense)           (None, 4)            516         dense_40[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bag_output (Dense)              (None, 3)            387         dense_42[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pose_output (Dense)             (None, 3)            387         dense_48[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "footwear_output (Dense)         (None, 3)            387         dense_44[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "emotion_output (Dense)          (None, 4)            516         dense_46[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 1,296,854\n",
            "Trainable params: 1,292,118\n",
            "Non-trainable params: 4,736\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkqMc7GkN2sl",
        "colab_type": "code",
        "outputId": "20088251-ee7f-4300-977c-0eabf6201da0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# from keras.utils import plot_model\n",
        "# plot_model(model)\n",
        "import time, psutil\n",
        "uptime = time.time() - psutil.boot_time()\n",
        "remain = 12*60*60 - uptime\n",
        "remain/(60*60)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.204058134555817"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1dcoduDN2pd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses = {\n",
        "\t\"gender_output\": \"categorical_crossentropy\",\n",
        "\t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "\t\"age_output\": \"categorical_crossentropy\",\n",
        "\t\"weight_output\": \"categorical_crossentropy\",\n",
        "  \"bag_output\":  \"categorical_crossentropy\",\n",
        "  \"pose_output\": \"categorical_crossentropy\",\n",
        "  \"footwear_output\": \"categorical_crossentropy\",\n",
        "  \"emotion_output\": \"categorical_crossentropy\"\n",
        "}\n",
        "\n",
        "loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0, \"weight_output\" :1.0,  \"bag_output\": 1.0, \"pose_output\": 1.0,  \"footwear_output\": 1.0, \"emotion_output\": 1.0 }\n",
        "\n",
        "\n",
        "\n",
        "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping\n",
        "\n",
        "decay_factor =  0.007\n",
        "def scheduler(epoch, lr):\n",
        "  return round(lr * 1/(1 + decay_factor * epoch), 10)\n",
        "\n",
        "opt = SGD(lr = 0.01, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile( optimizer=opt, loss = losses, loss_weights=loss_weights, metrics=[\"accuracy\"])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyOyW5EOQAEJ",
        "colab_type": "code",
        "outputId": "92eb3b5d-8234-4ab1-d334-f98b0965b26a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# model_path = '/content/gdrive/VGG16_vanila_1.h5'\n",
        "# checkpoint = ModelCheckpoint(model_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='max')\n",
        "\n",
        "import os\n",
        "# Checkpoint saving\n",
        "save_dir = os.path.join(os.getcwd(), \"/gdrive/My\\\\Drive/saved_models/\")\n",
        "model_name = \"model.{epoch:03d}.h5\"\n",
        "if not os.path.isdir(save_dir):\n",
        "  os.makedirs(save_dir)\n",
        "\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "\n",
        "checkpoint = ModelCheckpoint(model_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False)\n",
        "\n",
        "early = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=11, verbose=1, mode='min')\n",
        "\n",
        "# from seqeval.metrics import f1_score, classification_report\n",
        "# id2label = {1: 'B-LOC', 2: 'I-LOC'}\n",
        "# f1score = F1Metrics(id2label)\n",
        "\n",
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=100,\n",
        "    verbose=1,\n",
        "    callbacks=[checkpoint, early, LearningRateScheduler(scheduler, verbose=1)]\n",
        "    # callbacks=[checkpoint, early, LearningRateScheduler(scheduler, verbose=1), f1score ]\n",
        "    # callbacks=[checkpoint, early, LearningRateScheduler(scheduler, verbose=1),TensorBoardColabCallback(tbc)]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.0099999998.\n",
            "180/180 [==============================] - 123s 685ms/step - loss: 7.9351 - gender_output_loss: 0.6774 - image_quality_output_loss: 0.9981 - age_output_loss: 1.4413 - weight_output_loss: 1.0074 - bag_output_loss: 0.9284 - pose_output_loss: 0.9358 - footwear_output_loss: 0.9996 - emotion_output_loss: 0.9472 - gender_output_acc: 0.5740 - image_quality_output_acc: 0.5448 - age_output_acc: 0.3904 - weight_output_acc: 0.6283 - bag_output_acc: 0.5575 - pose_output_acc: 0.6181 - footwear_output_acc: 0.5060 - emotion_output_acc: 0.6982 - val_loss: 14.6973 - val_gender_output_loss: 2.3925 - val_image_quality_output_loss: 1.3077 - val_age_output_loss: 1.6615 - val_weight_output_loss: 1.4185 - val_bag_output_loss: 1.5819 - val_pose_output_loss: 1.2208 - val_footwear_output_loss: 3.9463 - val_emotion_output_loss: 1.1681 - val_gender_output_acc: 0.5504 - val_image_quality_output_acc: 0.5524 - val_age_output_acc: 0.2550 - val_weight_output_acc: 0.3417 - val_bag_output_acc: 0.5444 - val_pose_output_acc: 0.5958 - val_footwear_output_acc: 0.3629 - val_emotion_output_acc: 0.7303\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100Epoch 2/100\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0099304864.\n",
            "180/180 [==============================] - 107s 592ms/step - loss: 7.7607 - gender_output_loss: 0.6605 - image_quality_output_loss: 0.9811 - age_output_loss: 1.4171 - weight_output_loss: 0.9843 - bag_output_loss: 0.9068 - pose_output_loss: 0.9217 - footwear_output_loss: 0.9691 - emotion_output_loss: 0.9200 - gender_output_acc: 0.6012 - image_quality_output_acc: 0.5523 - age_output_acc: 0.3960 - weight_output_acc: 0.6338 - bag_output_acc: 0.5674 - pose_output_acc: 0.6217 - footwear_output_acc: 0.5355 - emotion_output_acc: 0.7079 - val_loss: 7.8920 - val_gender_output_loss: 0.6864 - val_image_quality_output_loss: 0.9874 - val_age_output_loss: 1.4575 - val_weight_output_loss: 1.0038 - val_bag_output_loss: 0.9308 - val_pose_output_loss: 0.9653 - val_footwear_output_loss: 0.9700 - val_emotion_output_loss: 0.8910 - val_gender_output_acc: 0.5978 - val_image_quality_output_acc: 0.5413 - val_age_output_acc: 0.3821 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.5464 - val_pose_output_acc: 0.5958 - val_footwear_output_acc: 0.5524 - val_emotion_output_acc: 0.7329\n",
            "\n",
            "Epoch 3/100\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0097933795.\n",
            "180/180 [==============================] - 106s 588ms/step - loss: 7.6884 - gender_output_loss: 0.6439 - image_quality_output_loss: 0.9749 - age_output_loss: 1.4114 - weight_output_loss: 0.9798 - bag_output_loss: 0.8967 - pose_output_loss: 0.9155 - footwear_output_loss: 0.9499 - emotion_output_loss: 0.9163 - gender_output_acc: 0.6222 - image_quality_output_acc: 0.5502 - age_output_acc: 0.3925 - weight_output_acc: 0.6340 - bag_output_acc: 0.5714 - pose_output_acc: 0.6220 - footwear_output_acc: 0.5522 - emotion_output_acc: 0.7076 - val_loss: 9.6056 - val_gender_output_loss: 0.6578 - val_image_quality_output_loss: 1.0392 - val_age_output_loss: 1.4452 - val_weight_output_loss: 1.0697 - val_bag_output_loss: 0.9758 - val_pose_output_loss: 1.0692 - val_footwear_output_loss: 2.4018 - val_emotion_output_loss: 0.9468 - val_gender_output_acc: 0.6260 - val_image_quality_output_acc: 0.3800 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.5484 - val_pose_output_acc: 0.5958 - val_footwear_output_acc: 0.3629 - val_emotion_output_acc: 0.7314\n",
            "Epoch 4/100\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0095919484.\n",
            "Epoch 3/100\n",
            "180/180 [==============================] - 107s 593ms/step - loss: 7.6269 - gender_output_loss: 0.6319 - image_quality_output_loss: 0.9688 - age_output_loss: 1.4023 - weight_output_loss: 0.9756 - bag_output_loss: 0.8916 - pose_output_loss: 0.9042 - footwear_output_loss: 0.9385 - emotion_output_loss: 0.9140 - gender_output_acc: 0.6331 - image_quality_output_acc: 0.5525 - age_output_acc: 0.3987 - weight_output_acc: 0.6346 - bag_output_acc: 0.5734 - pose_output_acc: 0.6222 - footwear_output_acc: 0.5510 - emotion_output_acc: 0.7076 - val_loss: 8.1778 - val_gender_output_loss: 0.6518 - val_image_quality_output_loss: 0.9739 - val_age_output_loss: 1.4551 - val_weight_output_loss: 1.0068 - val_bag_output_loss: 0.9399 - val_pose_output_loss: 0.9546 - val_footwear_output_loss: 1.2913 - val_emotion_output_loss: 0.9044 - val_gender_output_acc: 0.6285 - val_image_quality_output_acc: 0.5514 - val_age_output_acc: 0.3911 - val_weight_output_acc: 0.6371 - val_bag_output_acc: 0.5373 - val_pose_output_acc: 0.5953 - val_footwear_output_acc: 0.3720 - val_emotion_output_acc: 0.7324\n",
            "Epoch 5/100\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.009330689.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 7.5784 - gender_output_loss: 0.6219 - image_quality_output_loss: 0.9661 - age_output_loss: 1.3986 - weight_output_loss: 0.9711 - bag_output_loss: 0.8852 - pose_output_loss: 0.8978 - footwear_output_loss: 0.9278 - emotion_output_loss: 0.9099 - gender_output_acc: 0.6440 - image_quality_output_acc: 0.5537 - age_output_acc: 0.4013 - weight_output_acc: 0.6343 - bag_output_acc: 0.5813 - pose_output_acc: 0.6217 - footwear_output_acc: 0.5632 - emotion_output_acc: 0.7081Epoch 5/100\n",
            "180/180 [==============================] - 106s 588ms/step - loss: 7.5794 - gender_output_loss: 0.6214 - image_quality_output_loss: 0.9665 - age_output_loss: 1.3993 - weight_output_loss: 0.9717 - bag_output_loss: 0.8850 - pose_output_loss: 0.8976 - footwear_output_loss: 0.9275 - emotion_output_loss: 0.9103 - gender_output_acc: 0.6447 - image_quality_output_acc: 0.5535 - age_output_acc: 0.4010 - weight_output_acc: 0.6343 - bag_output_acc: 0.5811 - pose_output_acc: 0.6219 - footwear_output_acc: 0.5629 - emotion_output_acc: 0.7078 - val_loss: 7.7095 - val_gender_output_loss: 0.6382 - val_image_quality_output_loss: 0.9917 - val_age_output_loss: 1.4246 - val_weight_output_loss: 0.9621 - val_bag_output_loss: 0.9110 - val_pose_output_loss: 0.9833 - val_footwear_output_loss: 0.9343 - val_emotion_output_loss: 0.8642 - val_gender_output_acc: 0.6346 - val_image_quality_output_acc: 0.5509 - val_age_output_acc: 0.3952 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.5691 - val_pose_output_acc: 0.5958 - val_footwear_output_acc: 0.5449 - val_emotion_output_acc: 0.7329\n",
            "Epoch 6/100\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0090151584.\n",
            "180/180 [==============================] - 106s 587ms/step - loss: 7.5146 - gender_output_loss: 0.6057 - image_quality_output_loss: 0.9598 - age_output_loss: 1.3980 - weight_output_loss: 0.9685 - bag_output_loss: 0.8769 - pose_output_loss: 0.8859 - footwear_output_loss: 0.9126 - emotion_output_loss: 0.9070 - gender_output_acc: 0.6661 - image_quality_output_acc: 0.5552 - age_output_acc: 0.4000 - weight_output_acc: 0.6351 - bag_output_acc: 0.5840 - pose_output_acc: 0.6219 - footwear_output_acc: 0.5674 - emotion_output_acc: 0.7075 - val_loss: 9.4060 - val_gender_output_loss: 0.7247 - val_image_quality_output_loss: 0.9905 - val_age_output_loss: 1.4249 - val_weight_output_loss: 0.9888 - val_bag_output_loss: 0.9493 - val_pose_output_loss: 1.0003 - val_footwear_output_loss: 2.4045 - val_emotion_output_loss: 0.9230 - val_gender_output_acc: 0.5433 - val_image_quality_output_acc: 0.5549 - val_age_output_acc: 0.3836 - val_weight_output_acc: 0.6416 - val_bag_output_acc: 0.5509 - val_pose_output_acc: 0.5958 - val_footwear_output_acc: 0.3629 - val_emotion_output_acc: 0.7329\n",
            "\n",
            "Epoch 7/100\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0086517838.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 7.4719 - gender_output_loss: 0.5966 - image_quality_output_loss: 0.9584 - age_output_loss: 1.3930 - weight_output_loss: 0.9646 - bag_output_loss: 0.8737 - pose_output_loss: 0.8746 - footwear_output_loss: 0.9036 - emotion_output_loss: 0.9074 - gender_output_acc: 0.6734 - image_quality_output_acc: 0.5541 - age_output_acc: 0.3991 - weight_output_acc: 0.6350 - bag_output_acc: 0.5863 - pose_output_acc: 0.6219 - footwear_output_acc: 0.5793 - emotion_output_acc: 0.7077\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0086517838.\n",
            "180/180 [==============================] - 106s 588ms/step - loss: 7.4699 - gender_output_loss: 0.5963 - image_quality_output_loss: 0.9578 - age_output_loss: 1.3930 - weight_output_loss: 0.9647 - bag_output_loss: 0.8729 - pose_output_loss: 0.8749 - footwear_output_loss: 0.9031 - emotion_output_loss: 0.9071 - gender_output_acc: 0.6736 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3990 - weight_output_acc: 0.6350 - bag_output_acc: 0.5872 - pose_output_acc: 0.6218 - footwear_output_acc: 0.5799 - emotion_output_acc: 0.7077 - val_loss: 7.7592 - val_gender_output_loss: 0.6371 - val_image_quality_output_loss: 1.0266 - val_age_output_loss: 1.4212 - val_weight_output_loss: 0.9607 - val_bag_output_loss: 0.9454 - val_pose_output_loss: 0.9935 - val_footwear_output_loss: 0.9098 - val_emotion_output_loss: 0.8649 - val_gender_output_acc: 0.6210 - val_image_quality_output_acc: 0.5192 - val_age_output_acc: 0.3901 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.5534 - val_pose_output_acc: 0.5912 - val_footwear_output_acc: 0.5741 - val_emotion_output_acc: 0.7334\n",
            "Epoch 8/100\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0082476489.\n",
            "180/180 [==============================] - 106s 588ms/step - loss: 7.3957 - gender_output_loss: 0.5830 - image_quality_output_loss: 0.9535 - age_output_loss: 1.3869 - weight_output_loss: 0.9614 - bag_output_loss: 0.8648 - pose_output_loss: 0.8448 - footwear_output_loss: 0.8989 - emotion_output_loss: 0.9023 - gender_output_acc: 0.6816 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4012 - weight_output_acc: 0.6345 - bag_output_acc: 0.5922 - pose_output_acc: 0.6306 - footwear_output_acc: 0.5809 - emotion_output_acc: 0.7080 - val_loss: 7.8135 - val_gender_output_loss: 0.5571 - val_image_quality_output_loss: 1.0318 - val_age_output_loss: 1.4157 - val_weight_output_loss: 0.9749 - val_bag_output_loss: 0.9082 - val_pose_output_loss: 1.0755 - val_footwear_output_loss: 0.9834 - val_emotion_output_loss: 0.8670 - val_gender_output_acc: 0.7087 - val_image_quality_output_acc: 0.5257 - val_age_output_acc: 0.3765 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.5726 - val_pose_output_acc: 0.6104 - val_footwear_output_acc: 0.5524 - val_emotion_output_acc: 0.7324\n",
            "Epoch 8/100\n",
            "Epoch 9/100\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.007810274.\n",
            "180/180 [==============================] - 106s 589ms/step - loss: 7.3299 - gender_output_loss: 0.5659 - image_quality_output_loss: 0.9476 - age_output_loss: 1.3843 - weight_output_loss: 0.9612 - bag_output_loss: 0.8606 - pose_output_loss: 0.8214 - footwear_output_loss: 0.8867 - emotion_output_loss: 0.9022 - gender_output_acc: 0.7036 - image_quality_output_acc: 0.5547 - age_output_acc: 0.4030 - weight_output_acc: 0.6353 - bag_output_acc: 0.6023 - pose_output_acc: 0.6392 - footwear_output_acc: 0.5880 - emotion_output_acc: 0.7080 - val_loss: 8.2197 - val_gender_output_loss: 0.6093 - val_image_quality_output_loss: 0.9621 - val_age_output_loss: 1.4065 - val_weight_output_loss: 0.9809 - val_bag_output_loss: 0.9089 - val_pose_output_loss: 0.9533 - val_footwear_output_loss: 1.5413 - val_emotion_output_loss: 0.8574 - val_gender_output_acc: 0.6487 - val_image_quality_output_acc: 0.5469 - val_age_output_acc: 0.3931 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.5801 - val_pose_output_acc: 0.5932 - val_footwear_output_acc: 0.3816 - val_emotion_output_acc: 0.7329\n",
            "Epoch 10/100\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.007810274.\n",
            "Epoch 9/100\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0073473887.\n",
            "180/180 [==============================] - 106s 587ms/step - loss: 7.2630 - gender_output_loss: 0.5576 - image_quality_output_loss: 0.9415 - age_output_loss: 1.3804 - weight_output_loss: 0.9571 - bag_output_loss: 0.8542 - pose_output_loss: 0.7942 - footwear_output_loss: 0.8818 - emotion_output_loss: 0.8962 - gender_output_acc: 0.7062 - image_quality_output_acc: 0.5566 - age_output_acc: 0.4028 - weight_output_acc: 0.6347 - bag_output_acc: 0.6047 - pose_output_acc: 0.6536 - footwear_output_acc: 0.5884 - emotion_output_acc: 0.7081 - val_loss: 7.5389 - val_gender_output_loss: 0.5817 - val_image_quality_output_loss: 0.9547 - val_age_output_loss: 1.4067 - val_weight_output_loss: 0.9663 - val_bag_output_loss: 0.8875 - val_pose_output_loss: 0.8335 - val_footwear_output_loss: 1.0536 - val_emotion_output_loss: 0.8551 - val_gender_output_acc: 0.6820 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3957 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.6018 - val_pose_output_acc: 0.6154 - val_footwear_output_acc: 0.4970 - val_emotion_output_acc: 0.7329\n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0068667184.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 7.2207 - gender_output_loss: 0.5488 - image_quality_output_loss: 0.9361 - age_output_loss: 1.3783 - weight_output_loss: 0.9596 - bag_output_loss: 0.8536 - pose_output_loss: 0.7733 - footwear_output_loss: 0.8760 - emotion_output_loss: 0.8950 - gender_output_acc: 0.7181 - image_quality_output_acc: 0.5601 - age_output_acc: 0.4022 - weight_output_acc: 0.6344 - bag_output_acc: 0.6063 - pose_output_acc: 0.6597 - footwear_output_acc: 0.5908 - emotion_output_acc: 0.7078Epoch 11/100\n",
            "180/180 [==============================] - 106s 587ms/step - loss: 7.2192 - gender_output_loss: 0.5482 - image_quality_output_loss: 0.9362 - age_output_loss: 1.3781 - weight_output_loss: 0.9592 - bag_output_loss: 0.8528 - pose_output_loss: 0.7732 - footwear_output_loss: 0.8762 - emotion_output_loss: 0.8953 - gender_output_acc: 0.7186 - image_quality_output_acc: 0.5600 - age_output_acc: 0.4022 - weight_output_acc: 0.6345 - bag_output_acc: 0.6069 - pose_output_acc: 0.6600 - footwear_output_acc: 0.5906 - emotion_output_acc: 0.7076 - val_loss: 7.7693 - val_gender_output_loss: 0.5572 - val_image_quality_output_loss: 0.9773 - val_age_output_loss: 1.4286 - val_weight_output_loss: 0.9915 - val_bag_output_loss: 0.8793 - val_pose_output_loss: 0.8822 - val_footwear_output_loss: 1.1994 - val_emotion_output_loss: 0.8538 - val_gender_output_acc: 0.7172 - val_image_quality_output_acc: 0.5176 - val_age_output_acc: 0.3841 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.5978 - val_pose_output_acc: 0.6190 - val_footwear_output_acc: 0.4511 - val_emotion_output_acc: 0.7329\n",
            "Epoch 12/100\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0063757829.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 7.1235 - gender_output_loss: 0.5276 - image_quality_output_loss: 0.9319 - age_output_loss: 1.3736 - weight_output_loss: 0.9560 - bag_output_loss: 0.8468 - pose_output_loss: 0.7303 - footwear_output_loss: 0.8667 - emotion_output_loss: 0.8906 - gender_output_acc: 0.7293 - image_quality_output_acc: 0.5628 - age_output_acc: 0.4070 - weight_output_acc: 0.6356 - bag_output_acc: 0.6138 - pose_output_acc: 0.6828 - footwear_output_acc: 0.6006 - emotion_output_acc: 0.7078\n",
            "180/180 [==============================] - 106s 588ms/step - loss: 7.1263 - gender_output_loss: 0.5283 - image_quality_output_loss: 0.9319 - age_output_loss: 1.3742 - weight_output_loss: 0.9565 - bag_output_loss: 0.8466 - pose_output_loss: 0.7307 - footwear_output_loss: 0.8674 - emotion_output_loss: 0.8907 - gender_output_acc: 0.7287 - image_quality_output_acc: 0.5628 - age_output_acc: 0.4065 - weight_output_acc: 0.6352 - bag_output_acc: 0.6141 - pose_output_acc: 0.6825 - footwear_output_acc: 0.5999 - emotion_output_acc: 0.7078 - val_loss: 7.1645 - val_gender_output_loss: 0.4937 - val_image_quality_output_loss: 0.9426 - val_age_output_loss: 1.4051 - val_weight_output_loss: 0.9480 - val_bag_output_loss: 0.8685 - val_pose_output_loss: 0.7945 - val_footwear_output_loss: 0.8631 - val_emotion_output_loss: 0.8490 - val_gender_output_acc: 0.7586 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.3876 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.6028 - val_pose_output_acc: 0.6542 - val_footwear_output_acc: 0.6099 - val_emotion_output_acc: 0.7329\n",
            "Epoch 13/100\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0058817187.\n",
            "180/180 [==============================] - 106s 588ms/step - loss: 7.0461 - gender_output_loss: 0.5077 - image_quality_output_loss: 0.9258 - age_output_loss: 1.3688 - weight_output_loss: 0.9518 - bag_output_loss: 0.8390 - pose_output_loss: 0.7001 - footwear_output_loss: 0.8673 - emotion_output_loss: 0.8856 - gender_output_acc: 0.7456 - image_quality_output_acc: 0.5625 - age_output_acc: 0.4089 - weight_output_acc: 0.6364 - bag_output_acc: 0.6223 - pose_output_acc: 0.7000 - footwear_output_acc: 0.5969 - emotion_output_acc: 0.7078 - val_loss: 7.2207 - val_gender_output_loss: 0.5278 - val_image_quality_output_loss: 0.9596 - val_age_output_loss: 1.3994 - val_weight_output_loss: 0.9536 - val_bag_output_loss: 0.8644 - val_pose_output_loss: 0.7801 - val_footwear_output_loss: 0.8687 - val_emotion_output_loss: 0.8669 - val_gender_output_acc: 0.7278 - val_image_quality_output_acc: 0.5413 - val_age_output_acc: 0.3906 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.6008 - val_pose_output_acc: 0.6608 - val_footwear_output_acc: 0.5993 - val_emotion_output_acc: 0.7314\n",
            "Epoch 14/100\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0053911263.\n",
            "180/180 [==============================] - 106s 588ms/step - loss: 6.9914 - gender_output_loss: 0.4922 - image_quality_output_loss: 0.9237 - age_output_loss: 1.3625 - weight_output_loss: 0.9504 - bag_output_loss: 0.8328 - pose_output_loss: 0.6837 - footwear_output_loss: 0.8620 - emotion_output_loss: 0.8841 - gender_output_acc: 0.7572 - image_quality_output_acc: 0.5637 - age_output_acc: 0.4085 - weight_output_acc: 0.6362 - bag_output_acc: 0.6208 - pose_output_acc: 0.7075 - footwear_output_acc: 0.6017 - emotion_output_acc: 0.7082 - val_loss: 7.2161 - val_gender_output_loss: 0.6436 - val_image_quality_output_loss: 0.9462 - val_age_output_loss: 1.3863 - val_weight_output_loss: 0.9491 - val_bag_output_loss: 0.8984 - val_pose_output_loss: 0.6658 - val_footwear_output_loss: 0.8831 - val_emotion_output_loss: 0.8437 - val_gender_output_acc: 0.6870 - val_image_quality_output_acc: 0.5595 - val_age_output_acc: 0.3987 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.5847 - val_pose_output_acc: 0.7097 - val_footwear_output_acc: 0.6018 - val_emotion_output_acc: 0.7329\n",
            "Epoch 14/100\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0053911263.Epoch 15/100\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0049099513.\n",
            "180/180 [==============================] - 106s 589ms/step - loss: 6.9363 - gender_output_loss: 0.4854 - image_quality_output_loss: 0.9182 - age_output_loss: 1.3613 - weight_output_loss: 0.9483 - bag_output_loss: 0.8260 - pose_output_loss: 0.6627 - footwear_output_loss: 0.8555 - emotion_output_loss: 0.8787 - gender_output_acc: 0.7615 - image_quality_output_acc: 0.5674 - age_output_acc: 0.4115 - weight_output_acc: 0.6372 - bag_output_acc: 0.6259 - pose_output_acc: 0.7168 - footwear_output_acc: 0.6046 - emotion_output_acc: 0.7080 - val_loss: 7.0735 - val_gender_output_loss: 0.4605 - val_image_quality_output_loss: 0.9708 - val_age_output_loss: 1.3909 - val_weight_output_loss: 0.9471 - val_bag_output_loss: 0.8437 - val_pose_output_loss: 0.7046 - val_footwear_output_loss: 0.9236 - val_emotion_output_loss: 0.8323 - val_gender_output_acc: 0.7797 - val_image_quality_output_acc: 0.5398 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6442 - val_bag_output_acc: 0.6195 - val_pose_output_acc: 0.6930 - val_footwear_output_acc: 0.5625 - val_emotion_output_acc: 0.7329\n",
            "Epoch 16/100\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0044433948.\n",
            "180/180 [==============================] - 105s 586ms/step - loss: 6.8697 - gender_output_loss: 0.4662 - image_quality_output_loss: 0.9160 - age_output_loss: 1.3563 - weight_output_loss: 0.9432 - bag_output_loss: 0.8204 - pose_output_loss: 0.6457 - footwear_output_loss: 0.8435 - emotion_output_loss: 0.8783 - gender_output_acc: 0.7721 - image_quality_output_acc: 0.5688 - age_output_acc: 0.4103 - weight_output_acc: 0.6371 - bag_output_acc: 0.6336 - pose_output_acc: 0.7287 - footwear_output_acc: 0.6153 - emotion_output_acc: 0.7079 - val_loss: 7.3648 - val_gender_output_loss: 0.4756 - val_image_quality_output_loss: 0.9970 - val_age_output_loss: 1.3943 - val_weight_output_loss: 0.9453 - val_bag_output_loss: 0.8610 - val_pose_output_loss: 0.6988 - val_footwear_output_loss: 1.1438 - val_emotion_output_loss: 0.8490 - val_gender_output_acc: 0.7807 - val_image_quality_output_acc: 0.5197 - val_age_output_acc: 0.4068 - val_weight_output_acc: 0.6457 - val_bag_output_acc: 0.6331 - val_pose_output_acc: 0.6986 - val_footwear_output_acc: 0.4783 - val_emotion_output_acc: 0.7329\n",
            "Epoch 17/100\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.0039958588.\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0044433948.\n",
            "180/180 [==============================] - 106s 590ms/step - loss: 6.8078 - gender_output_loss: 0.4606 - image_quality_output_loss: 0.9101 - age_output_loss: 1.3521 - weight_output_loss: 0.9415 - bag_output_loss: 0.8126 - pose_output_loss: 0.6151 - footwear_output_loss: 0.8412 - emotion_output_loss: 0.8746 - gender_output_acc: 0.7755 - image_quality_output_acc: 0.5688 - age_output_acc: 0.4090 - weight_output_acc: 0.6378 - bag_output_acc: 0.6378 - pose_output_acc: 0.7385 - footwear_output_acc: 0.6168 - emotion_output_acc: 0.7079 - val_loss: 6.8943 - val_gender_output_loss: 0.4482 - val_image_quality_output_loss: 0.9648 - val_age_output_loss: 1.3823 - val_weight_output_loss: 0.9390 - val_bag_output_loss: 0.8345 - val_pose_output_loss: 0.6323 - val_footwear_output_loss: 0.8483 - val_emotion_output_loss: 0.8450 - val_gender_output_acc: 0.7954 - val_image_quality_output_acc: 0.5474 - val_age_output_acc: 0.4012 - val_weight_output_acc: 0.6447 - val_bag_output_acc: 0.6290 - val_pose_output_acc: 0.7349 - val_footwear_output_acc: 0.6129 - val_emotion_output_acc: 0.7329\n",
            "Epoch 18/100\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0035709192.\n",
            "180/180 [==============================] - 106s 586ms/step - loss: 6.7591 - gender_output_loss: 0.4452 - image_quality_output_loss: 0.9051 - age_output_loss: 1.3481 - weight_output_loss: 0.9369 - bag_output_loss: 0.8102 - pose_output_loss: 0.6031 - footwear_output_loss: 0.8352 - emotion_output_loss: 0.8753 - gender_output_acc: 0.7878 - image_quality_output_acc: 0.5711 - age_output_acc: 0.4138 - weight_output_acc: 0.6393 - bag_output_acc: 0.6362 - pose_output_acc: 0.7460 - footwear_output_acc: 0.6203 - emotion_output_acc: 0.7079 - val_loss: 7.3930 - val_gender_output_loss: 0.4639 - val_image_quality_output_loss: 1.0899 - val_age_output_loss: 1.4059 - val_weight_output_loss: 0.9475 - val_bag_output_loss: 0.8717 - val_pose_output_loss: 0.7741 - val_footwear_output_loss: 0.9947 - val_emotion_output_loss: 0.8453 - val_gender_output_acc: 0.7777 - val_image_quality_output_acc: 0.4834 - val_age_output_acc: 0.3916 - val_weight_output_acc: 0.6472 - val_bag_output_acc: 0.6084 - val_pose_output_acc: 0.6638 - val_footwear_output_acc: 0.5302 - val_emotion_output_acc: 0.7329\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0031713314.\n",
            "180/180 [==============================] - 106s 586ms/step - loss: 6.7088 - gender_output_loss: 0.4391 - image_quality_output_loss: 0.9058 - age_output_loss: 1.3465 - weight_output_loss: 0.9331 - bag_output_loss: 0.7993 - pose_output_loss: 0.5891 - footwear_output_loss: 0.8264 - emotion_output_loss: 0.8694 - gender_output_acc: 0.7908 - image_quality_output_acc: 0.5727 - age_output_acc: 0.4154 - weight_output_acc: 0.6391 - bag_output_acc: 0.6512 - pose_output_acc: 0.7566 - footwear_output_acc: 0.6247 - emotion_output_acc: 0.7079 - val_loss: 7.0186 - val_gender_output_loss: 0.4196 - val_image_quality_output_loss: 1.0289 - val_age_output_loss: 1.3847 - val_weight_output_loss: 0.9364 - val_bag_output_loss: 0.8343 - val_pose_output_loss: 0.6036 - val_footwear_output_loss: 0.9461 - val_emotion_output_loss: 0.8649 - val_gender_output_acc: 0.8170 - val_image_quality_output_acc: 0.5040 - val_age_output_acc: 0.3997 - val_weight_output_acc: 0.6452 - val_bag_output_acc: 0.6321 - val_pose_output_acc: 0.7434 - val_footwear_output_acc: 0.5418 - val_emotion_output_acc: 0.7308\n",
            "Epoch 20/100\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0027990569.\n",
            "180/180 [==============================] - 106s 587ms/step - loss: 6.6673 - gender_output_loss: 0.4282 - image_quality_output_loss: 0.8999 - age_output_loss: 1.3387 - weight_output_loss: 0.9314 - bag_output_loss: 0.7975 - pose_output_loss: 0.5746 - footwear_output_loss: 0.8272 - emotion_output_loss: 0.8697 - gender_output_acc: 0.7946 - image_quality_output_acc: 0.5711 - age_output_acc: 0.4141 - weight_output_acc: 0.6421 - bag_output_acc: 0.6504 - pose_output_acc: 0.7641 - footwear_output_acc: 0.6178 - emotion_output_acc: 0.7078 - val_loss: 6.8599 - val_gender_output_loss: 0.4039 - val_image_quality_output_loss: 0.9505 - val_age_output_loss: 1.3850 - val_weight_output_loss: 0.9370 - val_bag_output_loss: 0.8298 - val_pose_output_loss: 0.5976 - val_footwear_output_loss: 0.9210 - val_emotion_output_loss: 0.8351 - val_gender_output_acc: 0.8206 - val_image_quality_output_acc: 0.5459 - val_age_output_acc: 0.4128 - val_weight_output_acc: 0.6467 - val_bag_output_acc: 0.6326 - val_pose_output_acc: 0.7535 - val_footwear_output_acc: 0.5842 - val_emotion_output_acc: 0.7329\n",
            "Epoch 21/100\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0024553131.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 6.6166 - gender_output_loss: 0.4169 - image_quality_output_loss: 0.8973 - age_output_loss: 1.3326 - weight_output_loss: 0.9221 - bag_output_loss: 0.7927 - pose_output_loss: 0.5722 - footwear_output_loss: 0.8171 - emotion_output_loss: 0.8657 - gender_output_acc: 0.8028 - image_quality_output_acc: 0.5784 - age_output_acc: 0.4237 - weight_output_acc: 0.6434 - bag_output_acc: 0.6522 - pose_output_acc: 0.7653 - footwear_output_acc: 0.6310 - emotion_output_acc: 0.7079Epoch 21/100\n",
            "180/180 [==============================] - 106s 590ms/step - loss: 6.6175 - gender_output_loss: 0.4165 - image_quality_output_loss: 0.8977 - age_output_loss: 1.3323 - weight_output_loss: 0.9224 - bag_output_loss: 0.7924 - pose_output_loss: 0.5730 - footwear_output_loss: 0.8164 - emotion_output_loss: 0.8668 - gender_output_acc: 0.8032 - image_quality_output_acc: 0.5782 - age_output_acc: 0.4239 - weight_output_acc: 0.6431 - bag_output_acc: 0.6523 - pose_output_acc: 0.7651 - footwear_output_acc: 0.6313 - emotion_output_acc: 0.7076 - val_loss: 6.8414 - val_gender_output_loss: 0.4246 - val_image_quality_output_loss: 1.0066 - val_age_output_loss: 1.3819 - val_weight_output_loss: 0.9338 - val_bag_output_loss: 0.8271 - val_pose_output_loss: 0.5699 - val_footwear_output_loss: 0.8481 - val_emotion_output_loss: 0.8495 - val_gender_output_acc: 0.8115 - val_image_quality_output_acc: 0.5121 - val_age_output_acc: 0.4017 - val_weight_output_acc: 0.6457 - val_bag_output_acc: 0.6436 - val_pose_output_acc: 0.7596 - val_footwear_output_acc: 0.6144 - val_emotion_output_acc: 0.7324\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0021406391.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 6.5683 - gender_output_loss: 0.4075 - image_quality_output_loss: 0.8949 - age_output_loss: 1.3320 - weight_output_loss: 0.9223 - bag_output_loss: 0.7845 - pose_output_loss: 0.5491 - footwear_output_loss: 0.8123 - emotion_output_loss: 0.8655 - gender_output_acc: 0.8114 - image_quality_output_acc: 0.5761 - age_output_acc: 0.4178 - weight_output_acc: 0.6450 - bag_output_acc: 0.6530 - pose_output_acc: 0.7737 - footwear_output_acc: 0.6336 - emotion_output_acc: 0.7074\n",
            "180/180 [==============================] - 106s 589ms/step - loss: 6.5689 - gender_output_loss: 0.4085 - image_quality_output_loss: 0.8949 - age_output_loss: 1.3313 - weight_output_loss: 0.9215 - bag_output_loss: 0.7850 - pose_output_loss: 0.5492 - footwear_output_loss: 0.8132 - emotion_output_loss: 0.8653 - gender_output_acc: 0.8107 - image_quality_output_acc: 0.5762 - age_output_acc: 0.4184 - weight_output_acc: 0.6457 - bag_output_acc: 0.6532 - pose_output_acc: 0.7734 - footwear_output_acc: 0.6332 - emotion_output_acc: 0.7077 - val_loss: 6.7472 - val_gender_output_loss: 0.4465 - val_image_quality_output_loss: 0.9557 - val_age_output_loss: 1.3731 - val_weight_output_loss: 0.9257 - val_bag_output_loss: 0.8210 - val_pose_output_loss: 0.5551 - val_footwear_output_loss: 0.8310 - val_emotion_output_loss: 0.8390 - val_gender_output_acc: 0.7944 - val_image_quality_output_acc: 0.5358 - val_age_output_acc: 0.4108 - val_weight_output_acc: 0.6467 - val_bag_output_acc: 0.6431 - val_pose_output_acc: 0.7747 - val_footwear_output_acc: 0.6305 - val_emotion_output_acc: 0.7329\n",
            "Epoch 23/100\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0018549732.\n",
            "180/180 [==============================] - 106s 589ms/step - loss: 6.5377 - gender_output_loss: 0.3955 - image_quality_output_loss: 0.8935 - age_output_loss: 1.3250 - weight_output_loss: 0.9169 - bag_output_loss: 0.7851 - pose_output_loss: 0.5493 - footwear_output_loss: 0.8088 - emotion_output_loss: 0.8635 - gender_output_acc: 0.8139 - image_quality_output_acc: 0.5783 - age_output_acc: 0.4237 - weight_output_acc: 0.6437 - bag_output_acc: 0.6582 - pose_output_acc: 0.7747 - footwear_output_acc: 0.6284 - emotion_output_acc: 0.7076 - val_loss: 7.9911 - val_gender_output_loss: 0.4970 - val_image_quality_output_loss: 1.0118 - val_age_output_loss: 1.4696 - val_weight_output_loss: 0.9691 - val_bag_output_loss: 0.8600 - val_pose_output_loss: 0.6447 - val_footwear_output_loss: 1.6868 - val_emotion_output_loss: 0.8522 - val_gender_output_acc: 0.7354 - val_image_quality_output_acc: 0.5035 - val_age_output_acc: 0.3221 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.6245 - val_pose_output_acc: 0.7283 - val_footwear_output_acc: 0.4138 - val_emotion_output_acc: 0.7329\n",
            "Epoch 24/100Epoch 23/100\n",
            "\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0015977374.\n",
            "180/180 [==============================] - 106s 590ms/step - loss: 6.5077 - gender_output_loss: 0.3924 - image_quality_output_loss: 0.8932 - age_output_loss: 1.3259 - weight_output_loss: 0.9153 - bag_output_loss: 0.7792 - pose_output_loss: 0.5365 - footwear_output_loss: 0.8032 - emotion_output_loss: 0.8618 - gender_output_acc: 0.8181 - image_quality_output_acc: 0.5751 - age_output_acc: 0.4213 - weight_output_acc: 0.6445 - bag_output_acc: 0.6611 - pose_output_acc: 0.7842 - footwear_output_acc: 0.6362 - emotion_output_acc: 0.7079 - val_loss: 6.6990 - val_gender_output_loss: 0.3868 - val_image_quality_output_loss: 0.9608 - val_age_output_loss: 1.3742 - val_weight_output_loss: 0.9258 - val_bag_output_loss: 0.8151 - val_pose_output_loss: 0.5618 - val_footwear_output_loss: 0.8398 - val_emotion_output_loss: 0.8346 - val_gender_output_acc: 0.8322 - val_image_quality_output_acc: 0.5423 - val_age_output_acc: 0.4088 - val_weight_output_acc: 0.6467 - val_bag_output_acc: 0.6447 - val_pose_output_acc: 0.7747 - val_footwear_output_acc: 0.6205 - val_emotion_output_acc: 0.7329\n",
            "Epoch 25/100\n",
            "\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0013679259.\n",
            "180/180 [==============================] - 106s 588ms/step - loss: 6.4606 - gender_output_loss: 0.3818 - image_quality_output_loss: 0.8920 - age_output_loss: 1.3262 - weight_output_loss: 0.9107 - bag_output_loss: 0.7749 - pose_output_loss: 0.5164 - footwear_output_loss: 0.8002 - emotion_output_loss: 0.8584 - gender_output_acc: 0.8286 - image_quality_output_acc: 0.5773 - age_output_acc: 0.4262 - weight_output_acc: 0.6479 - bag_output_acc: 0.6624 - pose_output_acc: 0.7894 - footwear_output_acc: 0.6352 - emotion_output_acc: 0.7080 - val_loss: 7.5766 - val_gender_output_loss: 0.4262 - val_image_quality_output_loss: 0.9884 - val_age_output_loss: 1.4869 - val_weight_output_loss: 0.9834 - val_bag_output_loss: 0.8391 - val_pose_output_loss: 0.6225 - val_footwear_output_loss: 1.3770 - val_emotion_output_loss: 0.8531 - val_gender_output_acc: 0.7949 - val_image_quality_output_acc: 0.5358 - val_age_output_acc: 0.3125 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.6406 - val_pose_output_acc: 0.7429 - val_footwear_output_acc: 0.4632 - val_emotion_output_acc: 0.7329\n",
            "Epoch 26/100\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0011641923.\n",
            "\n",
            "179/180 [============================>.] - ETA: 0s - loss: 6.4316 - gender_output_loss: 0.3784 - image_quality_output_loss: 0.8835 - age_output_loss: 1.3184 - weight_output_loss: 0.9067 - bag_output_loss: 0.7694 - pose_output_loss: 0.5169 - footwear_output_loss: 0.8005 - emotion_output_loss: 0.8578 - gender_output_acc: 0.8257 - image_quality_output_acc: 0.5840 - age_output_acc: 0.4237 - weight_output_acc: 0.6459 - bag_output_acc: 0.6672 - pose_output_acc: 0.7902 - footwear_output_acc: 0.6364 - emotion_output_acc: 0.7079Epoch 26/100\n",
            "180/180 [==============================] - 106s 591ms/step - loss: 6.4337 - gender_output_loss: 0.3787 - image_quality_output_loss: 0.8837 - age_output_loss: 1.3186 - weight_output_loss: 0.9065 - bag_output_loss: 0.7697 - pose_output_loss: 0.5175 - footwear_output_loss: 0.8014 - emotion_output_loss: 0.8577 - gender_output_acc: 0.8255 - image_quality_output_acc: 0.5835 - age_output_acc: 0.4233 - weight_output_acc: 0.6463 - bag_output_acc: 0.6671 - pose_output_acc: 0.7900 - footwear_output_acc: 0.6354 - emotion_output_acc: 0.7079 - val_loss: 7.3597 - val_gender_output_loss: 0.4179 - val_image_quality_output_loss: 1.0498 - val_age_output_loss: 1.4363 - val_weight_output_loss: 0.9398 - val_bag_output_loss: 0.8419 - val_pose_output_loss: 0.6001 - val_footwear_output_loss: 1.2255 - val_emotion_output_loss: 0.8485 - val_gender_output_acc: 0.8075 - val_image_quality_output_acc: 0.5116 - val_age_output_acc: 0.3664 - val_weight_output_acc: 0.6467 - val_bag_output_acc: 0.6341 - val_pose_output_acc: 0.7505 - val_footwear_output_acc: 0.4965 - val_emotion_output_acc: 0.7329\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0009849343.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 6.3954 - gender_output_loss: 0.3704 - image_quality_output_loss: 0.8830 - age_output_loss: 1.3140 - weight_output_loss: 0.9064 - bag_output_loss: 0.7652 - pose_output_loss: 0.5086 - footwear_output_loss: 0.7920 - emotion_output_loss: 0.8558 - gender_output_acc: 0.8326 - image_quality_output_acc: 0.5793 - age_output_acc: 0.4252 - weight_output_acc: 0.6478 - bag_output_acc: 0.6687 - pose_output_acc: 0.7964 - footwear_output_acc: 0.6389 - emotion_output_acc: 0.7084\n",
            "180/180 [==============================] - 106s 588ms/step - loss: 6.3970 - gender_output_loss: 0.3710 - image_quality_output_loss: 0.8829 - age_output_loss: 1.3140 - weight_output_loss: 0.9056 - bag_output_loss: 0.7657 - pose_output_loss: 0.5087 - footwear_output_loss: 0.7928 - emotion_output_loss: 0.8563 - gender_output_acc: 0.8326 - image_quality_output_acc: 0.5796 - age_output_acc: 0.4253 - weight_output_acc: 0.6482 - bag_output_acc: 0.6688 - pose_output_acc: 0.7965 - footwear_output_acc: 0.6383 - emotion_output_acc: 0.7080 - val_loss: 6.7568 - val_gender_output_loss: 0.3924 - val_image_quality_output_loss: 0.9890 - val_age_output_loss: 1.3754 - val_weight_output_loss: 0.9371 - val_bag_output_loss: 0.8237 - val_pose_output_loss: 0.5526 - val_footwear_output_loss: 0.8421 - val_emotion_output_loss: 0.8444 - val_gender_output_acc: 0.8291 - val_image_quality_output_acc: 0.5297 - val_age_output_acc: 0.4068 - val_weight_output_acc: 0.6366 - val_bag_output_acc: 0.6381 - val_pose_output_acc: 0.7772 - val_footwear_output_acc: 0.6225 - val_emotion_output_acc: 0.7324\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.000828372.\n",
            "180/180 [==============================] - 106s 588ms/step - loss: 6.3726 - gender_output_loss: 0.3610 - image_quality_output_loss: 0.8832 - age_output_loss: 1.3102 - weight_output_loss: 0.9012 - bag_output_loss: 0.7647 - pose_output_loss: 0.5057 - footwear_output_loss: 0.7923 - emotion_output_loss: 0.8542 - gender_output_acc: 0.8376 - image_quality_output_acc: 0.5830 - age_output_acc: 0.4253 - weight_output_acc: 0.6483 - bag_output_acc: 0.6710 - pose_output_acc: 0.7972 - footwear_output_acc: 0.6409 - emotion_output_acc: 0.7080 - val_loss: 6.6895 - val_gender_output_loss: 0.3676 - val_image_quality_output_loss: 0.9587 - val_age_output_loss: 1.3820 - val_weight_output_loss: 0.9284 - val_bag_output_loss: 0.8161 - val_pose_output_loss: 0.5547 - val_footwear_output_loss: 0.8412 - val_emotion_output_loss: 0.8407 - val_gender_output_acc: 0.8377 - val_image_quality_output_acc: 0.5368 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6477 - val_bag_output_acc: 0.6497 - val_pose_output_acc: 0.7732 - val_footwear_output_acc: 0.6220 - val_emotion_output_acc: 0.7319\n",
            "Epoch 29/100\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0006926187.\n",
            "180/180 [==============================] - 106s 589ms/step - loss: 6.3403 - gender_output_loss: 0.3595 - image_quality_output_loss: 0.8775 - age_output_loss: 1.3074 - weight_output_loss: 0.8951 - bag_output_loss: 0.7612 - pose_output_loss: 0.4923 - footwear_output_loss: 0.7906 - emotion_output_loss: 0.8568 - gender_output_acc: 0.8404 - image_quality_output_acc: 0.5845 - age_output_acc: 0.4345 - weight_output_acc: 0.6494 - bag_output_acc: 0.6745 - pose_output_acc: 0.8030 - footwear_output_acc: 0.6391 - emotion_output_acc: 0.7077 - val_loss: 6.7535 - val_gender_output_loss: 0.3960 - val_image_quality_output_loss: 1.0026 - val_age_output_loss: 1.3736 - val_weight_output_loss: 0.9261 - val_bag_output_loss: 0.8203 - val_pose_output_loss: 0.5447 - val_footwear_output_loss: 0.8485 - val_emotion_output_loss: 0.8417 - val_gender_output_acc: 0.8306 - val_image_quality_output_acc: 0.5247 - val_age_output_acc: 0.4183 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.6522 - val_pose_output_acc: 0.7802 - val_footwear_output_acc: 0.6114 - val_emotion_output_acc: 0.7324\n",
            "Epoch 30/100\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0005757429.\n",
            "180/180 [==============================] - 106s 590ms/step - loss: 6.3166 - gender_output_loss: 0.3585 - image_quality_output_loss: 0.8754 - age_output_loss: 1.3049 - weight_output_loss: 0.8960 - bag_output_loss: 0.7566 - pose_output_loss: 0.4907 - footwear_output_loss: 0.7832 - emotion_output_loss: 0.8513 - gender_output_acc: 0.8406 - image_quality_output_acc: 0.5884 - age_output_acc: 0.4291 - weight_output_acc: 0.6480 - bag_output_acc: 0.6743 - pose_output_acc: 0.8043 - footwear_output_acc: 0.6425 - emotion_output_acc: 0.7082 - val_loss: 6.7087 - val_gender_output_loss: 0.3883 - val_image_quality_output_loss: 0.9667 - val_age_output_loss: 1.3809 - val_weight_output_loss: 0.9227 - val_bag_output_loss: 0.8256 - val_pose_output_loss: 0.5452 - val_footwear_output_loss: 0.8362 - val_emotion_output_loss: 0.8432 - val_gender_output_acc: 0.8337 - val_image_quality_output_acc: 0.5449 - val_age_output_acc: 0.4108 - val_weight_output_acc: 0.6522 - val_bag_output_acc: 0.6497 - val_pose_output_acc: 0.7812 - val_footwear_output_acc: 0.6205 - val_emotion_output_acc: 0.7324\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0004758206.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 6.3155 - gender_output_loss: 0.3526 - image_quality_output_loss: 0.8769 - age_output_loss: 1.3043 - weight_output_loss: 0.8918 - bag_output_loss: 0.7617 - pose_output_loss: 0.4879 - footwear_output_loss: 0.7885 - emotion_output_loss: 0.8517 - gender_output_acc: 0.8429 - image_quality_output_acc: 0.5873 - age_output_acc: 0.4316 - weight_output_acc: 0.6501 - bag_output_acc: 0.6748 - pose_output_acc: 0.8052 - footwear_output_acc: 0.6445 - emotion_output_acc: 0.7080Epoch 31/100\n",
            "180/180 [==============================] - 106s 589ms/step - loss: 6.3162 - gender_output_loss: 0.3534 - image_quality_output_loss: 0.8768 - age_output_loss: 1.3039 - weight_output_loss: 0.8921 - bag_output_loss: 0.7615 - pose_output_loss: 0.4881 - footwear_output_loss: 0.7883 - emotion_output_loss: 0.8521 - gender_output_acc: 0.8426 - image_quality_output_acc: 0.5872 - age_output_acc: 0.4323 - weight_output_acc: 0.6502 - bag_output_acc: 0.6747 - pose_output_acc: 0.8049 - footwear_output_acc: 0.6444 - emotion_output_acc: 0.7080 - val_loss: 6.7228 - val_gender_output_loss: 0.3760 - val_image_quality_output_loss: 0.9910 - val_age_output_loss: 1.3747 - val_weight_output_loss: 0.9239 - val_bag_output_loss: 0.8161 - val_pose_output_loss: 0.5508 - val_footwear_output_loss: 0.8422 - val_emotion_output_loss: 0.8482 - val_gender_output_acc: 0.8432 - val_image_quality_output_acc: 0.5192 - val_age_output_acc: 0.4083 - val_weight_output_acc: 0.6472 - val_bag_output_acc: 0.6487 - val_pose_output_acc: 0.7828 - val_footwear_output_acc: 0.6099 - val_emotion_output_acc: 0.7329\n",
            "Epoch 32/100\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0003909783.\n",
            "180/180 [==============================] - 106s 588ms/step - loss: 6.2964 - gender_output_loss: 0.3467 - image_quality_output_loss: 0.8780 - age_output_loss: 1.3066 - weight_output_loss: 0.8913 - bag_output_loss: 0.7525 - pose_output_loss: 0.4845 - footwear_output_loss: 0.7844 - emotion_output_loss: 0.8523 - gender_output_acc: 0.8505 - image_quality_output_acc: 0.5860 - age_output_acc: 0.4319 - weight_output_acc: 0.6501 - bag_output_acc: 0.6780 - pose_output_acc: 0.8064 - footwear_output_acc: 0.6431 - emotion_output_acc: 0.7081 - val_loss: 6.7281 - val_gender_output_loss: 0.3705 - val_image_quality_output_loss: 0.9979 - val_age_output_loss: 1.3867 - val_weight_output_loss: 0.9198 - val_bag_output_loss: 0.8151 - val_pose_output_loss: 0.5405 - val_footwear_output_loss: 0.8535 - val_emotion_output_loss: 0.8441 - val_gender_output_acc: 0.8443 - val_image_quality_output_acc: 0.5197 - val_age_output_acc: 0.4012 - val_weight_output_acc: 0.6512 - val_bag_output_acc: 0.6557 - val_pose_output_acc: 0.7843 - val_footwear_output_acc: 0.6119 - val_emotion_output_acc: 0.7324\n",
            "Epoch 33/100\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.0003194267.\n",
            "180/180 [==============================] - 106s 589ms/step - loss: 6.2884 - gender_output_loss: 0.3502 - image_quality_output_loss: 0.8767 - age_output_loss: 1.3015 - weight_output_loss: 0.8921 - bag_output_loss: 0.7518 - pose_output_loss: 0.4820 - footwear_output_loss: 0.7816 - emotion_output_loss: 0.8526 - gender_output_acc: 0.8420 - image_quality_output_acc: 0.5867 - age_output_acc: 0.4340 - weight_output_acc: 0.6502 - bag_output_acc: 0.6783 - pose_output_acc: 0.8057 - footwear_output_acc: 0.6406 - emotion_output_acc: 0.7082 - val_loss: 6.9559 - val_gender_output_loss: 0.3883 - val_image_quality_output_loss: 0.9662 - val_age_output_loss: 1.4188 - val_weight_output_loss: 0.9551 - val_bag_output_loss: 0.8298 - val_pose_output_loss: 0.5908 - val_footwear_output_loss: 0.9612 - val_emotion_output_loss: 0.8457 - val_gender_output_acc: 0.8286 - val_image_quality_output_acc: 0.5489 - val_age_output_acc: 0.3604 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.6431 - val_pose_output_acc: 0.7646 - val_footwear_output_acc: 0.5948 - val_emotion_output_acc: 0.7324\n",
            "Epoch 34/100\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002594855.\n",
            "180/180 [==============================] - 106s 589ms/step - loss: 6.2752 - gender_output_loss: 0.3499 - image_quality_output_loss: 0.8744 - age_output_loss: 1.2992 - weight_output_loss: 0.8910 - bag_output_loss: 0.7562 - pose_output_loss: 0.4777 - footwear_output_loss: 0.7774 - emotion_output_loss: 0.8494 - gender_output_acc: 0.8429 - image_quality_output_acc: 0.5843 - age_output_acc: 0.4332 - weight_output_acc: 0.6507 - bag_output_acc: 0.6753 - pose_output_acc: 0.8151 - footwear_output_acc: 0.6481 - emotion_output_acc: 0.7081 - val_loss: 6.7026 - val_gender_output_loss: 0.3699 - val_image_quality_output_loss: 0.9699 - val_age_output_loss: 1.3814 - val_weight_output_loss: 0.9206 - val_bag_output_loss: 0.8183 - val_pose_output_loss: 0.5495 - val_footwear_output_loss: 0.8514 - val_emotion_output_loss: 0.8416 - val_gender_output_acc: 0.8483 - val_image_quality_output_acc: 0.5373 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6502 - val_bag_output_acc: 0.6467 - val_pose_output_acc: 0.7812 - val_footwear_output_acc: 0.6255 - val_emotion_output_acc: 0.7324\n",
            "Epoch 35/100\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.0002096006.\n",
            "180/180 [==============================] - 107s 592ms/step - loss: 6.2798 - gender_output_loss: 0.3518 - image_quality_output_loss: 0.8750 - age_output_loss: 1.3007 - weight_output_loss: 0.8927 - bag_output_loss: 0.7535 - pose_output_loss: 0.4790 - footwear_output_loss: 0.7790 - emotion_output_loss: 0.8482 - gender_output_acc: 0.8411 - image_quality_output_acc: 0.5845 - age_output_acc: 0.4295 - weight_output_acc: 0.6502 - bag_output_acc: 0.6770 - pose_output_acc: 0.8071 - footwear_output_acc: 0.6484 - emotion_output_acc: 0.7076 - val_loss: 6.7085 - val_gender_output_loss: 0.3646 - val_image_quality_output_loss: 0.9892 - val_age_output_loss: 1.3805 - val_weight_output_loss: 0.9228 - val_bag_output_loss: 0.8167 - val_pose_output_loss: 0.5412 - val_footwear_output_loss: 0.8513 - val_emotion_output_loss: 0.8424 - val_gender_output_acc: 0.8503 - val_image_quality_output_acc: 0.5282 - val_age_output_acc: 0.3982 - val_weight_output_acc: 0.6502 - val_bag_output_acc: 0.6527 - val_pose_output_acc: 0.7878 - val_footwear_output_acc: 0.6134 - val_emotion_output_acc: 0.7319\n",
            "Epoch 36/100\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0001683539.\n",
            "180/180 [==============================] - 106s 591ms/step - loss: 6.2530 - gender_output_loss: 0.3372 - image_quality_output_loss: 0.8708 - age_output_loss: 1.2936 - weight_output_loss: 0.8878 - bag_output_loss: 0.7516 - pose_output_loss: 0.4795 - footwear_output_loss: 0.7824 - emotion_output_loss: 0.8502 - gender_output_acc: 0.8536 - image_quality_output_acc: 0.5881 - age_output_acc: 0.4359 - weight_output_acc: 0.6510 - bag_output_acc: 0.6757 - pose_output_acc: 0.8049 - footwear_output_acc: 0.6491 - emotion_output_acc: 0.7076 - val_loss: 6.6768 - val_gender_output_loss: 0.3682 - val_image_quality_output_loss: 0.9929 - val_age_output_loss: 1.3706 - val_weight_output_loss: 0.9252 - val_bag_output_loss: 0.8130 - val_pose_output_loss: 0.5357 - val_footwear_output_loss: 0.8298 - val_emotion_output_loss: 0.8415 - val_gender_output_acc: 0.8463 - val_image_quality_output_acc: 0.5227 - val_age_output_acc: 0.4108 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.6512 - val_pose_output_acc: 0.7823 - val_footwear_output_acc: 0.6220 - val_emotion_output_acc: 0.7324\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.000134468.\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0001683539.\n",
            "180/180 [==============================] - 107s 592ms/step - loss: 6.2385 - gender_output_loss: 0.3379 - image_quality_output_loss: 0.8713 - age_output_loss: 1.2958 - weight_output_loss: 0.8890 - bag_output_loss: 0.7471 - pose_output_loss: 0.4685 - footwear_output_loss: 0.7801 - emotion_output_loss: 0.8487 - gender_output_acc: 0.8517 - image_quality_output_acc: 0.5886 - age_output_acc: 0.4359 - weight_output_acc: 0.6514 - bag_output_acc: 0.6829 - pose_output_acc: 0.8115 - footwear_output_acc: 0.6510 - emotion_output_acc: 0.7078 - val_loss: 6.7235 - val_gender_output_loss: 0.3684 - val_image_quality_output_loss: 0.9708 - val_age_output_loss: 1.3836 - val_weight_output_loss: 0.9243 - val_bag_output_loss: 0.8169 - val_pose_output_loss: 0.5485 - val_footwear_output_loss: 0.8714 - val_emotion_output_loss: 0.8395 - val_gender_output_acc: 0.8463 - val_image_quality_output_acc: 0.5393 - val_age_output_acc: 0.3942 - val_weight_output_acc: 0.6457 - val_bag_output_acc: 0.6507 - val_pose_output_acc: 0.7812 - val_footwear_output_acc: 0.6129 - val_emotion_output_acc: 0.7324\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.000134468.\n",
            "Epoch 38/100\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0001068054.\n",
            "180/180 [==============================] - 106s 591ms/step - loss: 6.2341 - gender_output_loss: 0.3397 - image_quality_output_loss: 0.8719 - age_output_loss: 1.2941 - weight_output_loss: 0.8866 - bag_output_loss: 0.7471 - pose_output_loss: 0.4678 - footwear_output_loss: 0.7796 - emotion_output_loss: 0.8473 - gender_output_acc: 0.8474 - image_quality_output_acc: 0.5913 - age_output_acc: 0.4347 - weight_output_acc: 0.6516 - bag_output_acc: 0.6831 - pose_output_acc: 0.8126 - footwear_output_acc: 0.6479 - emotion_output_acc: 0.7078 - val_loss: 6.7520 - val_gender_output_loss: 0.3798 - val_image_quality_output_loss: 0.9866 - val_age_output_loss: 1.3827 - val_weight_output_loss: 0.9253 - val_bag_output_loss: 0.8234 - val_pose_output_loss: 0.5410 - val_footwear_output_loss: 0.8713 - val_emotion_output_loss: 0.8420 - val_gender_output_acc: 0.8402 - val_image_quality_output_acc: 0.5323 - val_age_output_acc: 0.4007 - val_weight_output_acc: 0.6416 - val_bag_output_acc: 0.6492 - val_pose_output_acc: 0.7888 - val_footwear_output_acc: 0.6084 - val_emotion_output_acc: 0.7319\n",
            "Epoch 39/100Epoch 38/100\n",
            "\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 8.43645e-05.\n",
            "180/180 [==============================] - 106s 591ms/step - loss: 6.2501 - gender_output_loss: 0.3395 - image_quality_output_loss: 0.8721 - age_output_loss: 1.2963 - weight_output_loss: 0.8888 - bag_output_loss: 0.7476 - pose_output_loss: 0.4771 - footwear_output_loss: 0.7783 - emotion_output_loss: 0.8506 - gender_output_acc: 0.8500 - image_quality_output_acc: 0.5900 - age_output_acc: 0.4343 - weight_output_acc: 0.6510 - bag_output_acc: 0.6827 - pose_output_acc: 0.8102 - footwear_output_acc: 0.6457 - emotion_output_acc: 0.7077 - val_loss: 6.7047 - val_gender_output_loss: 0.3625 - val_image_quality_output_loss: 0.9985 - val_age_output_loss: 1.3741 - val_weight_output_loss: 0.9256 - val_bag_output_loss: 0.8157 - val_pose_output_loss: 0.5383 - val_footwear_output_loss: 0.8477 - val_emotion_output_loss: 0.8422 - val_gender_output_acc: 0.8483 - val_image_quality_output_acc: 0.5232 - val_age_output_acc: 0.4042 - val_weight_output_acc: 0.6386 - val_bag_output_acc: 0.6512 - val_pose_output_acc: 0.7868 - val_footwear_output_acc: 0.6119 - val_emotion_output_acc: 0.7319\n",
            "\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 6.62722e-05.\n",
            "180/180 [==============================] - 106s 589ms/step - loss: 6.2346 - gender_output_loss: 0.3405 - image_quality_output_loss: 0.8719 - age_output_loss: 1.2965 - weight_output_loss: 0.8867 - bag_output_loss: 0.7456 - pose_output_loss: 0.4751 - footwear_output_loss: 0.7712 - emotion_output_loss: 0.8471 - gender_output_acc: 0.8483 - image_quality_output_acc: 0.5878 - age_output_acc: 0.4326 - weight_output_acc: 0.6523 - bag_output_acc: 0.6826 - pose_output_acc: 0.8089 - footwear_output_acc: 0.6519 - emotion_output_acc: 0.7079 - val_loss: 6.7312 - val_gender_output_loss: 0.3669 - val_image_quality_output_loss: 0.9803 - val_age_output_loss: 1.3820 - val_weight_output_loss: 0.9240 - val_bag_output_loss: 0.8195 - val_pose_output_loss: 0.5453 - val_footwear_output_loss: 0.8723 - val_emotion_output_loss: 0.8409 - val_gender_output_acc: 0.8468 - val_image_quality_output_acc: 0.5373 - val_age_output_acc: 0.4027 - val_weight_output_acc: 0.6457 - val_bag_output_acc: 0.6502 - val_pose_output_acc: 0.7828 - val_footwear_output_acc: 0.6023 - val_emotion_output_acc: 0.7319\n",
            "Epoch 41/100\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 5.17752e-05.\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 6.62722e-05.\n",
            "180/180 [==============================] - 106s 591ms/step - loss: 6.2276 - gender_output_loss: 0.3370 - image_quality_output_loss: 0.8725 - age_output_loss: 1.2959 - weight_output_loss: 0.8873 - bag_output_loss: 0.7435 - pose_output_loss: 0.4649 - footwear_output_loss: 0.7769 - emotion_output_loss: 0.8496 - gender_output_acc: 0.8497 - image_quality_output_acc: 0.5871 - age_output_acc: 0.4364 - weight_output_acc: 0.6520 - bag_output_acc: 0.6812 - pose_output_acc: 0.8181 - footwear_output_acc: 0.6445 - emotion_output_acc: 0.7080 - val_loss: 6.7262 - val_gender_output_loss: 0.3711 - val_image_quality_output_loss: 0.9876 - val_age_output_loss: 1.3795 - val_weight_output_loss: 0.9271 - val_bag_output_loss: 0.8189 - val_pose_output_loss: 0.5436 - val_footwear_output_loss: 0.8571 - val_emotion_output_loss: 0.8411 - val_gender_output_acc: 0.8443 - val_image_quality_output_acc: 0.5302 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.6527 - val_pose_output_acc: 0.7858 - val_footwear_output_acc: 0.6134 - val_emotion_output_acc: 0.7319\n",
            "Epoch 42/100\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 4.02294e-05.\n",
            "180/180 [==============================] - 106s 589ms/step - loss: 6.2196 - gender_output_loss: 0.3330 - image_quality_output_loss: 0.8742 - age_output_loss: 1.2946 - weight_output_loss: 0.8875 - bag_output_loss: 0.7444 - pose_output_loss: 0.4625 - footwear_output_loss: 0.7743 - emotion_output_loss: 0.8492 - gender_output_acc: 0.8532 - image_quality_output_acc: 0.5909 - age_output_acc: 0.4368 - weight_output_acc: 0.6501 - bag_output_acc: 0.6864 - pose_output_acc: 0.8165 - footwear_output_acc: 0.6528 - emotion_output_acc: 0.7082 - val_loss: 6.6983 - val_gender_output_loss: 0.3668 - val_image_quality_output_loss: 0.9842 - val_age_output_loss: 1.3757 - val_weight_output_loss: 0.9235 - val_bag_output_loss: 0.8177 - val_pose_output_loss: 0.5410 - val_footwear_output_loss: 0.8494 - val_emotion_output_loss: 0.8400 - val_gender_output_acc: 0.8432 - val_image_quality_output_acc: 0.5312 - val_age_output_acc: 0.4017 - val_weight_output_acc: 0.6442 - val_bag_output_acc: 0.6487 - val_pose_output_acc: 0.7868 - val_footwear_output_acc: 0.6169 - val_emotion_output_acc: 0.7319\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 4.02294e-05.\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 3.10892e-05.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 6.2178 - gender_output_loss: 0.3340 - image_quality_output_loss: 0.8712 - age_output_loss: 1.2941 - weight_output_loss: 0.8885 - bag_output_loss: 0.7418 - pose_output_loss: 0.4641 - footwear_output_loss: 0.7758 - emotion_output_loss: 0.8484 - gender_output_acc: 0.8552 - image_quality_output_acc: 0.5867 - age_output_acc: 0.4329 - weight_output_acc: 0.6523 - bag_output_acc: 0.6833 - pose_output_acc: 0.8154 - footwear_output_acc: 0.6487 - emotion_output_acc: 0.7081Epoch 43/100\n",
            "180/180 [==============================] - 106s 591ms/step - loss: 6.2182 - gender_output_loss: 0.3342 - image_quality_output_loss: 0.8708 - age_output_loss: 1.2942 - weight_output_loss: 0.8880 - bag_output_loss: 0.7415 - pose_output_loss: 0.4640 - footwear_output_loss: 0.7767 - emotion_output_loss: 0.8488 - gender_output_acc: 0.8553 - image_quality_output_acc: 0.5871 - age_output_acc: 0.4331 - weight_output_acc: 0.6522 - bag_output_acc: 0.6834 - pose_output_acc: 0.8158 - footwear_output_acc: 0.6478 - emotion_output_acc: 0.7079 - val_loss: 6.7008 - val_gender_output_loss: 0.3676 - val_image_quality_output_loss: 0.9918 - val_age_output_loss: 1.3747 - val_weight_output_loss: 0.9242 - val_bag_output_loss: 0.8164 - val_pose_output_loss: 0.5400 - val_footwear_output_loss: 0.8449 - val_emotion_output_loss: 0.8412 - val_gender_output_acc: 0.8453 - val_image_quality_output_acc: 0.5247 - val_age_output_acc: 0.4027 - val_weight_output_acc: 0.6406 - val_bag_output_acc: 0.6507 - val_pose_output_acc: 0.7863 - val_footwear_output_acc: 0.6210 - val_emotion_output_acc: 0.7319\n",
            "Epoch 44/100\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 2.38964e-05.\n",
            "180/180 [==============================] - 107s 592ms/step - loss: 6.2239 - gender_output_loss: 0.3362 - image_quality_output_loss: 0.8731 - age_output_loss: 1.2893 - weight_output_loss: 0.8894 - bag_output_loss: 0.7498 - pose_output_loss: 0.4683 - footwear_output_loss: 0.7735 - emotion_output_loss: 0.8443 - gender_output_acc: 0.8549 - image_quality_output_acc: 0.5901 - age_output_acc: 0.4439 - weight_output_acc: 0.6515 - bag_output_acc: 0.6758 - pose_output_acc: 0.8156 - footwear_output_acc: 0.6515 - emotion_output_acc: 0.7078 - val_loss: 6.6936 - val_gender_output_loss: 0.3664 - val_image_quality_output_loss: 0.9866 - val_age_output_loss: 1.3746 - val_weight_output_loss: 0.9240 - val_bag_output_loss: 0.8168 - val_pose_output_loss: 0.5392 - val_footwear_output_loss: 0.8454 - val_emotion_output_loss: 0.8405 - val_gender_output_acc: 0.8443 - val_image_quality_output_acc: 0.5292 - val_age_output_acc: 0.4012 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.6507 - val_pose_output_acc: 0.7868 - val_footwear_output_acc: 0.6179 - val_emotion_output_acc: 0.7319\n",
            "Epoch 45/100\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 1.82694e-05.\n",
            "180/180 [==============================] - 107s 593ms/step - loss: 6.2171 - gender_output_loss: 0.3332 - image_quality_output_loss: 0.8709 - age_output_loss: 1.2915 - weight_output_loss: 0.8864 - bag_output_loss: 0.7446 - pose_output_loss: 0.4678 - footwear_output_loss: 0.7764 - emotion_output_loss: 0.8465 - gender_output_acc: 0.8553 - image_quality_output_acc: 0.5936 - age_output_acc: 0.4365 - weight_output_acc: 0.6522 - bag_output_acc: 0.6850 - pose_output_acc: 0.8170 - footwear_output_acc: 0.6485 - emotion_output_acc: 0.7078 - val_loss: 6.7139 - val_gender_output_loss: 0.3690 - val_image_quality_output_loss: 0.9889 - val_age_output_loss: 1.3777 - val_weight_output_loss: 0.9240 - val_bag_output_loss: 0.8177 - val_pose_output_loss: 0.5408 - val_footwear_output_loss: 0.8548 - val_emotion_output_loss: 0.8411 - val_gender_output_acc: 0.8448 - val_image_quality_output_acc: 0.5252 - val_age_output_acc: 0.3972 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.6517 - val_pose_output_acc: 0.7858 - val_footwear_output_acc: 0.6179 - val_emotion_output_acc: 0.7319\n",
            "Epoch 46/100\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 1.38931e-05.\n",
            "180/180 [==============================] - 107s 593ms/step - loss: 6.2271 - gender_output_loss: 0.3359 - image_quality_output_loss: 0.8710 - age_output_loss: 1.2936 - weight_output_loss: 0.8894 - bag_output_loss: 0.7457 - pose_output_loss: 0.4701 - footwear_output_loss: 0.7741 - emotion_output_loss: 0.8472 - gender_output_acc: 0.8496 - image_quality_output_acc: 0.5921 - age_output_acc: 0.4371 - weight_output_acc: 0.6508 - bag_output_acc: 0.6800 - pose_output_acc: 0.8135 - footwear_output_acc: 0.6490 - emotion_output_acc: 0.7080 - val_loss: 6.7174 - val_gender_output_loss: 0.3694 - val_image_quality_output_loss: 0.9859 - val_age_output_loss: 1.3789 - val_weight_output_loss: 0.9239 - val_bag_output_loss: 0.8187 - val_pose_output_loss: 0.5418 - val_footwear_output_loss: 0.8580 - val_emotion_output_loss: 0.8407 - val_gender_output_acc: 0.8443 - val_image_quality_output_acc: 0.5292 - val_age_output_acc: 0.4032 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.6527 - val_pose_output_acc: 0.7873 - val_footwear_output_acc: 0.6139 - val_emotion_output_acc: 0.7319\n",
            "Epoch 47/100\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 1.05092e-05.\n",
            "180/180 [==============================] - 107s 593ms/step - loss: 6.2156 - gender_output_loss: 0.3368 - image_quality_output_loss: 0.8713 - age_output_loss: 1.2914 - weight_output_loss: 0.8826 - bag_output_loss: 0.7413 - pose_output_loss: 0.4688 - footwear_output_loss: 0.7770 - emotion_output_loss: 0.8463 - gender_output_acc: 0.8492 - image_quality_output_acc: 0.5897 - age_output_acc: 0.4402 - weight_output_acc: 0.6557 - bag_output_acc: 0.6897 - pose_output_acc: 0.8156 - footwear_output_acc: 0.6505 - emotion_output_acc: 0.7077 - val_loss: 6.7075 - val_gender_output_loss: 0.3688 - val_image_quality_output_loss: 0.9871 - val_age_output_loss: 1.3771 - val_weight_output_loss: 0.9240 - val_bag_output_loss: 0.8179 - val_pose_output_loss: 0.5402 - val_footwear_output_loss: 0.8516 - val_emotion_output_loss: 0.8409 - val_gender_output_acc: 0.8453 - val_image_quality_output_acc: 0.5287 - val_age_output_acc: 0.4007 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.6527 - val_pose_output_acc: 0.7888 - val_footwear_output_acc: 0.6144 - val_emotion_output_acc: 0.7319\n",
            "Epoch 00047: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f149af2f940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9Qcz9_GQANw",
        "colab_type": "code",
        "outputId": "a10d89ba-3e12-428f-84ad-754fc4b482b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "def evaluate_model(model):\n",
        "  results = model.evaluate_generator(valid_gen, verbose=1)\n",
        "  accuracies = {}\n",
        "  losses = {}\n",
        "  for k, v in zip(model.metrics_names, results):\n",
        "    if k.endswith('acc'):\n",
        "      accuracies[k] = round(v * 100, 4)\n",
        "    else:\n",
        "      losses[k] = v\n",
        "  return accuracies\n",
        "\n",
        "evaluate_model(model)\n",
        "\n",
        "# results = model.evaluate_generator(valid_gen, verbose =1)\n",
        "# dict(zip(model.metrics_names, results))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31/31 [==============================] - 5s 165ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output_acc': 40.0706,\n",
              " 'bag_output_acc': 65.2722,\n",
              " 'emotion_output_acc': 73.1855,\n",
              " 'footwear_output_acc': 61.4415,\n",
              " 'gender_output_acc': 84.5262,\n",
              " 'image_quality_output_acc': 52.873,\n",
              " 'pose_output_acc': 78.881,\n",
              " 'weight_output_acc': 63.9617}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-nJYdSOQAGn",
        "colab_type": "code",
        "outputId": "93d7a3b7-1b22-40cc-ba25-47f07b60fdfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(1,1,figsize=(15,5))\n",
        "    # summarize history for accuracy\n",
        "    # axs[0].plot(range(1,len(model_history.history.history['acc'])+1),model_history.history['acc'])\n",
        "    # axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    # axs[0].set_title('Model Accuracy')\n",
        "    # axs[0].set_ylabel('Accuracy')\n",
        "    # axs[0].set_xlabel('Epoch')\n",
        "    # axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    # axs[0].legend(['train', 'val'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs.plot(range(1,len(model_history.history.history['loss'])+1),model_history.history.history['loss'])\n",
        "    axs.plot(range(1,len(model_history.history.history['val_loss'])+1),model_history.history.history['val_loss'])\n",
        "    axs.set_title('Model Loss')\n",
        "    axs.set_ylabel('Loss')\n",
        "    axs.set_xlabel('Epoch')\n",
        "    axs.set_xticks(np.arange(1,len(model_history.history.history['loss'])+1),len(model_history.history.history['loss'])/10)\n",
        "    axs.legend(['train', 'val'], loc='best')\n",
        "    plt.show()\n",
        "# plot model history\n",
        "plot_model_history(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3UAAAFNCAYAAACnuEbJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA7Vki4-QAQx",
        "colab_type": "code",
        "outputId": "b16aa630-0663-4ab6-d45d-1923d80a7e62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "  return round(lr * 1/(1 + decay_factor * epoch), 10)\n",
        "\n",
        "lr = 0.004\n",
        "decay_factor = 0.001\n",
        "epoch = 100\n",
        "for i in range(epoch):\n",
        "  lr_new = scheduler(i, lr)\n",
        "  lr = lr_new\n",
        "  if i%5 == 0:\n",
        "    print(\"the epoch number is: \" + str(i) + \" and LR is: \" + str(round(lr,10)))\n",
        "  i = i+1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the epoch number is: 0 and LR is: 0.004\n",
            "the epoch number is: 5 and LR is: 0.0039405558\n",
            "the epoch number is: 10 and LR is: 0.0037866656\n",
            "the epoch number is: 15 and LR is: 0.0035498651\n",
            "the epoch number is: 20 and LR is: 0.0032469459\n",
            "the epoch number is: 25 and LR is: 0.0028980039\n",
            "the epoch number is: 30 and LR is: 0.0025242679\n",
            "the epoch number is: 35 and LR is: 0.0021460304\n",
            "the epoch number is: 40 and LR is: 0.0017809475\n",
            "the epoch number is: 45 and LR is: 0.0014428845\n",
            "the epoch number is: 50 and LR is: 0.0011413717\n",
            "the epoch number is: 55 and LR is: 0.0008816318\n",
            "the epoch number is: 60 and LR is: 0.00066506\n",
            "the epoch number is: 65 and LR is: 0.0004900005\n",
            "the epoch number is: 70 and LR is: 0.0003526487\n",
            "the epoch number is: 75 and LR is: 0.0002479393\n",
            "the epoch number is: 80 and LR is: 0.0001703153\n",
            "the epoch number is: 85 and LR is: 0.0001143177\n",
            "the epoch number is: 90 and LR is: 7.49844e-05\n",
            "the epoch number is: 95 and LR is: 4.80699e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1WB81PaN2ug",
        "colab_type": "code",
        "outputId": "48e431ff-9dc6-48ca-b5db-60ce4a3cf7da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.17.4)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.2.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.3.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=2a502271871d15a28a4911679cc2be4a7a950b473d199d51b8e52516e0a749fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-0.0.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgYCMe1chjKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.callbacks import Callback\n",
        "from seqeval.metrics import f1_score, classification_report\n",
        "\n",
        "\n",
        "class F1Metrics(Callback):\n",
        "\n",
        "    def __init__(self, id2label, pad_value=0, validation_data=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            id2label (dict): id to label mapping.\n",
        "            (e.g. {1: 'B-LOC', 2: 'I-LOC'})\n",
        "            pad_value (int): padding value.\n",
        "        \"\"\"\n",
        "        super(F1Metrics, self).__init__()\n",
        "        self.id2label = id2label\n",
        "        self.pad_value = pad_value\n",
        "        self.validation_data = validation_data\n",
        "        self.is_fit = validation_data is None\n",
        "\n",
        "    def find_pad_index(self, array):\n",
        "        \"\"\"Find padding index.\n",
        "        Args:\n",
        "            array (list): integer list.\n",
        "        Returns:\n",
        "            idx: padding index.\n",
        "        Examples:\n",
        "             >>> array = [1, 2, 0]\n",
        "             >>> self.find_pad_index(array)\n",
        "             2\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return list(array).index(self.pad_value)\n",
        "        except ValueError:\n",
        "            return len(array)\n",
        "\n",
        "    def get_length(self, y):\n",
        "        \"\"\"Get true length of y.\n",
        "        Args:\n",
        "            y (list): padded list.\n",
        "        Returns:\n",
        "            lens: true length of y.\n",
        "        Examples:\n",
        "            >>> y = [[1, 0, 0], [1, 1, 0], [1, 1, 1]]\n",
        "            >>> self.get_length(y)\n",
        "            [1, 2, 3]\n",
        "        \"\"\"\n",
        "        lens = [self.find_pad_index(row) for row in y]\n",
        "        return lens\n",
        "\n",
        "    def convert_idx_to_name(self, y, lens):\n",
        "        \"\"\"Convert label index to name.\n",
        "        Args:\n",
        "            y (list): label index list.\n",
        "            lens (list): true length of y.\n",
        "        Returns:\n",
        "            y: label name list.\n",
        "        Examples:\n",
        "            >>> # assumes that id2label = {1: 'B-LOC', 2: 'I-LOC'}\n",
        "            >>> y = [[1, 0, 0], [1, 2, 0], [1, 1, 1]]\n",
        "            >>> lens = [1, 2, 3]\n",
        "            >>> self.convert_idx_to_name(y, lens)\n",
        "            [['B-LOC'], ['B-LOC', 'I-LOC'], ['B-LOC', 'B-LOC', 'B-LOC']]\n",
        "        \"\"\"\n",
        "        y = [[self.id2label[idx] for idx in row[:l]]\n",
        "             for row, l in zip(y, lens)]\n",
        "        return y\n",
        "\n",
        "    def predict(self, X, y):\n",
        "        \"\"\"Predict sequences.\n",
        "        Args:\n",
        "            X (list): input data.\n",
        "            y (list): tags.\n",
        "        Returns:\n",
        "            y_true: true sequences.\n",
        "            y_pred: predicted sequences.\n",
        "        \"\"\"\n",
        "        y_pred = self.model.predict_on_batch(X)\n",
        "\n",
        "        # reduce dimension.\n",
        "        y_true = np.argmax(y, -1)\n",
        "        y_pred = np.argmax(y_pred, -1)\n",
        "\n",
        "        lens = self.get_length(y_true)\n",
        "\n",
        "        y_true = self.convert_idx_to_name(y_true, lens)\n",
        "        y_pred = self.convert_idx_to_name(y_pred, lens)\n",
        "\n",
        "        return y_true, y_pred\n",
        "\n",
        "    def score(self, y_true, y_pred):\n",
        "        \"\"\"Calculate f1 score.\n",
        "        Args:\n",
        "            y_true (list): true sequences.\n",
        "            y_pred (list): predicted sequences.\n",
        "        Returns:\n",
        "            score: f1 score.\n",
        "        \"\"\"\n",
        "        score = f1_score(y_true, y_pred)\n",
        "        print(' - f1: {:04.2f}'.format(score * 100))\n",
        "        print(classification_report(y_true, y_pred, digits=4))\n",
        "        return score\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if self.is_fit:\n",
        "            self.on_epoch_end_fit(epoch, logs)\n",
        "        else:\n",
        "            self.on_epoch_end_fit_generator(epoch, logs)\n",
        "\n",
        "    def on_epoch_end_fit(self, epoch, logs={}):\n",
        "        X = self.validation_data[0]\n",
        "        y = self.validation_data[1]\n",
        "        y_true, y_pred = self.predict(X, y)\n",
        "        score = self.score(y_true, y_pred)\n",
        "        logs['f1'] = score\n",
        "\n",
        "    def on_epoch_end_fit_generator(self, epoch, logs={}):\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        for X, y in self.validation_data:\n",
        "            y_true_batch, y_pred_batch = self.predict(X, y)\n",
        "            y_true.extend(y_true_batch)\n",
        "            y_pred.extend(y_pred_batch)\n",
        "        score = self.score(y_true, y_pred)\n",
        "        logs['f1'] = score"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}