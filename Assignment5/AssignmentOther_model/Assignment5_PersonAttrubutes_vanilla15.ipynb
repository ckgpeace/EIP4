{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5_PersonAttrubutes_vanilla15.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ckgpeace/EIP4/blob/master/Assignment5/AssignmentOther_model/Assignment5_PersonAttrubutes_vanilla15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El-Dopn8pLpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "b08c9ab6-0e59-4a4b-8908-b99a1105bcf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "outputId": "326b50a5-120c-4d7d-e5ca-da99709f77e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "from keras.layers import Activation, Flatten, Dense, Dropout, GlobalAveragePooling2D, AveragePooling2D, Add\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, SeparableConv2D\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input, BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "fced3ef1-79c7-4c13-efa3-7607182ea8b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "0155bd8e-f538-46fd-c03f-251d381829d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True, augmentation = None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.resize(cv2.imread(item[\"image_path\"]), (112,112)) for _, item in items.iterrows()])\n",
        "        #Image Normalization\n",
        "        if self.augmentation is not None:\n",
        "          self.augmentation.fit(image)\n",
        "          image = self.augmentation.flow(image,shuffle=False, batch_size = 32 ).next()\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "4455d754-f7a0-468b-8926-7f83e35dcca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15, random_state = 404)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "ebe4fe40-ad19-4556-b916-df677c165872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10980</th>\n",
              "      <td>resized/10982.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11135</th>\n",
              "      <td>resized/11137.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1784</th>\n",
              "      <td>resized/1785.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7758</th>\n",
              "      <td>resized/7759.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4692</th>\n",
              "      <td>resized/4693.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "10980  resized/10982.jpg              0  ...                        1              0\n",
              "11135  resized/11137.jpg              0  ...                        0              1\n",
              "1784    resized/1785.jpg              0  ...                        1              0\n",
              "7758    resized/7759.jpg              0  ...                        0              1\n",
              "4692    resized/4693.jpg              0  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32, shuffle=True, \n",
        "                                augmentation = ImageDataGenerator(horizontal_flip=True, \n",
        "                                                                  width_shift_range=0.2,\n",
        "                                                                  height_shift_range=0.2,\n",
        "                                                                  rotation_range=15,\n",
        "                                                                  zoom_range=0.2,\n",
        "                                                                  featurewise_center=True, featurewise_std_normalization=True))\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=32, shuffle=False, augmentation= ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "c071fc4c-8d82-4de9-fbbb-cac4a9dee964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "outputId": "1137ad18-faa8-408d-ec2b-9bf89196ea27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "inp = Input(shape = (112,112,3))\n",
        "x = inp\n",
        "### block 1\n",
        "\n",
        "x = SeparableConv2D(filters=32, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=32, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=64, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=64, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# Pooling \n",
        "x = AveragePooling2D()(x)\n",
        "\n",
        "# ================================================================\n",
        "\n",
        "### block 2\n",
        "\n",
        "x = SeparableConv2D(filters=64, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=64, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=128, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=256, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# ================================================================\n",
        "# Pooling \n",
        "x = AveragePooling2D()(x)\n",
        "\n",
        "# Apply 1x1\n",
        "x = SeparableConv2D(filters=32, kernel_size=(1, 1), padding='valid')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# ================================================================\n",
        "\n",
        "### block 3\n",
        "\n",
        "x = SeparableConv2D(filters=64, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=128, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=256, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# ================================================================\n",
        "# Pooling \n",
        "x = AveragePooling2D()(x)\n",
        "\n",
        "# Apply 1x1\n",
        "x = SeparableConv2D(filters=64, kernel_size=(1, 1), padding='valid')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "# ================================================================\n",
        "\n",
        "\n",
        "\n",
        "# Adding Dense layer\n",
        "def final(in_layer, num_units, class_name, output_name):\n",
        "  x = SeparableConv2D(filters=128, kernel_size=(3, 3), padding='valid')(in_layer) \n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  x = SeparableConv2D(filters=256, kernel_size=(3, 3), padding='valid')(x) \n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  # Conv with class size\n",
        "  x = SeparableConv2D(filters=num_units[class_name], kernel_size=(1, 1), padding='valid')(x)\n",
        "  # GAP\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  x = Activation('softmax', name = output_name)(x)\n",
        "  return x\n",
        "\n",
        "gender = final(in_layer = x, num_units = num_units,  class_name = \"gender\", output_name = \"gender_output\")\n",
        "image_quality = final(in_layer = x, num_units = num_units,  class_name = \"image_quality\", output_name = \"image_quality_output\")\n",
        "age = final(in_layer = x, num_units = num_units,  class_name = \"age\", output_name = \"age_output\")\n",
        "weight = final(in_layer = x, num_units = num_units,  class_name = \"weight\", output_name = \"weight_output\")\n",
        "bag = final(in_layer = x, num_units = num_units,  class_name = \"bag\", output_name = \"bag_output\")\n",
        "footwear = final(in_layer = x, num_units = num_units,  class_name = \"footwear\", output_name = \"footwear_output\")\n",
        "emotion = final(in_layer = x, num_units = num_units,  class_name = \"emotion\", output_name = \"emotion_output\")\n",
        "pose = final(in_layer = x, num_units = num_units,  class_name = \"pose\", output_name = \"pose_output\")\n",
        "\n",
        "model = Model(inputs = inp,outputs=[gender, image_quality, age, weight, bag, pose, footwear, emotion])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 112, 112, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_1 (SeparableCo (None, 110, 110, 32) 155         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 110, 110, 32) 0           separable_conv2d_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 110, 110, 32) 128         activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_2 (SeparableCo (None, 108, 108, 32) 1344        batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 108, 108, 32) 0           separable_conv2d_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 108, 108, 32) 128         activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_3 (SeparableCo (None, 106, 106, 64) 2400        batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 106, 106, 64) 0           separable_conv2d_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 106, 106, 64) 256         activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_4 (SeparableCo (None, 104, 104, 64) 4736        batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 104, 104, 64) 0           separable_conv2d_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 104, 104, 64) 256         activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 52, 52, 64)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_5 (SeparableCo (None, 50, 50, 32)   2656        average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 50, 50, 32)   0           separable_conv2d_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 50, 50, 32)   128         activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_6 (SeparableCo (None, 48, 48, 64)   2400        batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 48, 48, 64)   0           separable_conv2d_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 48, 48, 64)   256         activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_7 (SeparableCo (None, 46, 46, 128)  8896        batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 46, 46, 128)  0           separable_conv2d_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 46, 46, 128)  512         activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_8 (SeparableCo (None, 44, 44, 256)  34176       batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 44, 44, 256)  0           separable_conv2d_8[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 44, 44, 256)  1024        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 22, 22, 256)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_9 (SeparableCo (None, 22, 22, 32)   8480        average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 22, 22, 32)   0           separable_conv2d_9[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 22, 22, 32)   128         activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_10 (SeparableC (None, 20, 20, 64)   2400        batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 20, 20, 64)   0           separable_conv2d_10[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 20, 20, 64)   256         activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_11 (SeparableC (None, 18, 18, 128)  8896        batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 18, 18, 128)  0           separable_conv2d_11[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 18, 18, 128)  512         activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_12 (SeparableC (None, 16, 16, 256)  34176       batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 256)  0           separable_conv2d_12[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 256)  1024        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 8, 8, 256)    0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_13 (SeparableC (None, 8, 8, 64)     16704       average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 8, 8, 64)     0           separable_conv2d_13[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 64)     256         activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_14 (SeparableC (None, 6, 6, 128)    8896        batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_17 (SeparableC (None, 6, 6, 128)    8896        batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_20 (SeparableC (None, 6, 6, 128)    8896        batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_23 (SeparableC (None, 6, 6, 128)    8896        batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_26 (SeparableC (None, 6, 6, 128)    8896        batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_35 (SeparableC (None, 6, 6, 128)    8896        batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_29 (SeparableC (None, 6, 6, 128)    8896        batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_32 (SeparableC (None, 6, 6, 128)    8896        batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 6, 6, 128)    0           separable_conv2d_14[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 6, 6, 128)    0           separable_conv2d_17[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 6, 6, 128)    0           separable_conv2d_20[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 6, 6, 128)    0           separable_conv2d_23[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 6, 6, 128)    0           separable_conv2d_26[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 6, 6, 128)    0           separable_conv2d_35[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 6, 6, 128)    0           separable_conv2d_29[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 6, 6, 128)    0           separable_conv2d_32[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 6, 6, 128)    512         activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 6, 6, 128)    512         activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 6, 6, 128)    512         activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 6, 6, 128)    512         activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 6, 6, 128)    512         activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 6, 6, 128)    512         activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 6, 6, 128)    512         activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 6, 6, 128)    512         activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_15 (SeparableC (None, 4, 4, 256)    34176       batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_18 (SeparableC (None, 4, 4, 256)    34176       batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_21 (SeparableC (None, 4, 4, 256)    34176       batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_24 (SeparableC (None, 4, 4, 256)    34176       batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_27 (SeparableC (None, 4, 4, 256)    34176       batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_36 (SeparableC (None, 4, 4, 256)    34176       batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_30 (SeparableC (None, 4, 4, 256)    34176       batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_33 (SeparableC (None, 4, 4, 256)    34176       batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 4, 4, 256)    0           separable_conv2d_15[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 4, 4, 256)    0           separable_conv2d_18[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 4, 4, 256)    0           separable_conv2d_21[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 4, 4, 256)    0           separable_conv2d_24[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 4, 4, 256)    0           separable_conv2d_27[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 4, 4, 256)    0           separable_conv2d_36[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 4, 4, 256)    0           separable_conv2d_30[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 4, 4, 256)    0           separable_conv2d_33[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 4, 4, 256)    1024        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 4, 4, 256)    1024        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 4, 4, 256)    1024        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 4, 4, 256)    1024        activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 4, 4, 256)    1024        activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 4, 4, 256)    1024        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 4, 4, 256)    1024        activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 4, 4, 256)    1024        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_16 (SeparableC (None, 4, 4, 2)      770         batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_19 (SeparableC (None, 4, 4, 3)      1027        batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_22 (SeparableC (None, 4, 4, 5)      1541        batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_25 (SeparableC (None, 4, 4, 4)      1284        batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_28 (SeparableC (None, 4, 4, 3)      1027        batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_37 (SeparableC (None, 4, 4, 3)      1027        batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_31 (SeparableC (None, 4, 4, 3)      1027        batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_34 (SeparableC (None, 4, 4, 4)      1284        batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 2)            0           separable_conv2d_16[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_2 (Glo (None, 3)            0           separable_conv2d_19[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_3 (Glo (None, 5)            0           separable_conv2d_22[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_4 (Glo (None, 4)            0           separable_conv2d_25[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_5 (Glo (None, 3)            0           separable_conv2d_28[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_8 (Glo (None, 3)            0           separable_conv2d_37[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_6 (Glo (None, 3)            0           separable_conv2d_31[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_7 (Glo (None, 4)            0           separable_conv2d_34[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Activation)      (None, 2)            0           global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "image_quality_output (Activatio (None, 3)            0           global_average_pooling2d_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "age_output (Activation)         (None, 5)            0           global_average_pooling2d_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "weight_output (Activation)      (None, 4)            0           global_average_pooling2d_4[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "bag_output (Activation)         (None, 3)            0           global_average_pooling2d_5[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "pose_output (Activation)        (None, 3)            0           global_average_pooling2d_8[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "footwear_output (Activation)    (None, 3)            0           global_average_pooling2d_6[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "emotion_output (Activation)     (None, 4)            0           global_average_pooling2d_7[0][0] \n",
            "==================================================================================================\n",
            "Total params: 498,134\n",
            "Trainable params: 489,558\n",
            "Non-trainable params: 8,576\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkqMc7GkN2sl",
        "colab_type": "code",
        "outputId": "3dbb24ca-f107-43c9-cd32-5f61ff8cd3ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# from keras.utils import plot_model\n",
        "# plot_model(model)\n",
        "import time, psutil\n",
        "uptime = time.time() - psutil.boot_time()\n",
        "remain = 12*60*60 - uptime\n",
        "remain/(60*60)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11.356531367368168"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1dcoduDN2pd",
        "colab_type": "code",
        "outputId": "682fdd53-37bf-45e4-b414-bdb7c2b6af5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "losses = {\n",
        "\t\"gender_output\": \"categorical_crossentropy\",\n",
        "\t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "\t\"age_output\": \"categorical_crossentropy\",\n",
        "\t\"weight_output\": \"categorical_crossentropy\",\n",
        "  \"bag_output\":  \"categorical_crossentropy\",\n",
        "  \"pose_output\": \"categorical_crossentropy\",\n",
        "  \"footwear_output\": \"categorical_crossentropy\",\n",
        "  \"emotion_output\": \"categorical_crossentropy\"\n",
        "}\n",
        "\n",
        "loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0, \"weight_output\" :1.0,  \"bag_output\": 1.0, \"pose_output\": 1.0,  \"footwear_output\": 1.0, \"emotion_output\": 1.0 }\n",
        "\n",
        "\n",
        "\n",
        "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping\n",
        "\n",
        "decay_factor =  0.001\n",
        "def scheduler(epoch, lr):\n",
        "  return round(lr * 1/(1 + decay_factor * epoch), 10)\n",
        "\n",
        "opt = SGD(lr = 0.004, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile( optimizer=opt, loss = losses, loss_weights=loss_weights, metrics=[\"accuracy\"])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyOyW5EOQAEJ",
        "colab_type": "code",
        "outputId": "f2f73617-552a-4aec-b9f9-626bf4494497",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# model_path = '/content/gdrive/VGG16_vanila_1.h5'\n",
        "# checkpoint = ModelCheckpoint(model_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='max')\n",
        "\n",
        "import os\n",
        "# Checkpoint saving\n",
        "save_dir = os.path.join(os.getcwd(), \"/gdrive/My\\\\Drive/saved_models/\")\n",
        "model_name = \"model.{epoch:03d}.h5\"\n",
        "if not os.path.isdir(save_dir):\n",
        "  os.makedirs(save_dir)\n",
        "\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "\n",
        "checkpoint = ModelCheckpoint(model_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False)\n",
        "\n",
        "early = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=11, verbose=1, mode='min')\n",
        "\n",
        "# from seqeval.metrics import f1_score, classification_report\n",
        "# id2label = {1: 'B-LOC', 2: 'I-LOC'}\n",
        "# f1score = F1Metrics(id2label)\n",
        "\n",
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=100,\n",
        "    verbose=1,\n",
        "    callbacks=[checkpoint, early, LearningRateScheduler(scheduler, verbose=1)]\n",
        "    # callbacks=[checkpoint, early, LearningRateScheduler(scheduler, verbose=1), f1score ]\n",
        "    # callbacks=[checkpoint, early, LearningRateScheduler(scheduler, verbose=1),TensorBoardColabCallback(tbc)]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.0040000002.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.0360 - gender_output_loss: 0.6713 - image_quality_output_loss: 0.9945 - age_output_loss: 1.4584 - weight_output_loss: 1.0421 - bag_output_loss: 0.9372 - pose_output_loss: 0.9397 - footwear_output_loss: 1.0188 - emotion_output_loss: 0.9739 - gender_output_acc: 0.5722 - image_quality_output_acc: 0.5480 - age_output_acc: 0.3935 - weight_output_acc: 0.6281 - bag_output_acc: 0.5605 - pose_output_acc: 0.6168 - footwear_output_acc: 0.4938 - emotion_output_acc: 0.6999\n",
            "360/360 [==============================] - 124s 345ms/step - loss: 8.0360 - gender_output_loss: 0.6712 - image_quality_output_loss: 0.9947 - age_output_loss: 1.4584 - weight_output_loss: 1.0427 - bag_output_loss: 0.9369 - pose_output_loss: 0.9391 - footwear_output_loss: 1.0183 - emotion_output_loss: 0.9746 - gender_output_acc: 0.5723 - image_quality_output_acc: 0.5476 - age_output_acc: 0.3933 - weight_output_acc: 0.6278 - bag_output_acc: 0.5608 - pose_output_acc: 0.6172 - footwear_output_acc: 0.4945 - emotion_output_acc: 0.6996 - val_loss: 8.2476 - val_gender_output_loss: 0.6924 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 1.4497 - val_weight_output_loss: 0.9875 - val_bag_output_loss: 0.9901 - val_pose_output_loss: 0.9594 - val_footwear_output_loss: 1.3106 - val_emotion_output_loss: 0.8763 - val_gender_output_acc: 0.5759 - val_image_quality_output_acc: 0.5541 - val_age_output_acc: 0.3661 - val_weight_output_acc: 0.6409 - val_bag_output_acc: 0.5446 - val_pose_output_acc: 0.5942 - val_footwear_output_acc: 0.3869 - val_emotion_output_acc: 0.7326\n",
            "Epoch 2/100\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0039960042.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "360/360 [==============================] - 110s 305ms/step - loss: 7.7456 - gender_output_loss: 0.6584 - image_quality_output_loss: 0.9787 - age_output_loss: 1.4163 - weight_output_loss: 0.9792 - bag_output_loss: 0.9044 - pose_output_loss: 0.9194 - footwear_output_loss: 0.9766 - emotion_output_loss: 0.9125 - gender_output_acc: 0.5957 - image_quality_output_acc: 0.5534 - age_output_acc: 0.3973 - weight_output_acc: 0.6350 - bag_output_acc: 0.5672 - pose_output_acc: 0.6220 - footwear_output_acc: 0.5289 - emotion_output_acc: 0.7078 - val_loss: 7.7850 - val_gender_output_loss: 0.6446 - val_image_quality_output_loss: 0.9942 - val_age_output_loss: 1.4241 - val_weight_output_loss: 0.9708 - val_bag_output_loss: 0.9459 - val_pose_output_loss: 0.9523 - val_footwear_output_loss: 0.9841 - val_emotion_output_loss: 0.8690 - val_gender_output_acc: 0.6290 - val_image_quality_output_acc: 0.5402 - val_age_output_acc: 0.3884 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5441 - val_pose_output_acc: 0.5942 - val_footwear_output_acc: 0.5303 - val_emotion_output_acc: 0.7326\n",
            "Epoch 3/100\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0039880283.\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.7075 - gender_output_loss: 0.6519 - image_quality_output_loss: 0.9787 - age_output_loss: 1.4105 - weight_output_loss: 0.9747 - bag_output_loss: 0.9007 - pose_output_loss: 0.9155 - footwear_output_loss: 0.9656 - emotion_output_loss: 0.9099 - gender_output_acc: 0.6077 - image_quality_output_acc: 0.5533 - age_output_acc: 0.3989 - weight_output_acc: 0.6352 - bag_output_acc: 0.5689 - pose_output_acc: 0.6217 - footwear_output_acc: 0.5416 - emotion_output_acc: 0.7081\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0039880283.\n",
            "360/360 [==============================] - 109s 303ms/step - loss: 7.7076 - gender_output_loss: 0.6518 - image_quality_output_loss: 0.9787 - age_output_loss: 1.4105 - weight_output_loss: 0.9754 - bag_output_loss: 0.9008 - pose_output_loss: 0.9152 - footwear_output_loss: 0.9653 - emotion_output_loss: 0.9100 - gender_output_acc: 0.6078 - image_quality_output_acc: 0.5532 - age_output_acc: 0.3989 - weight_output_acc: 0.6348 - bag_output_acc: 0.5689 - pose_output_acc: 0.6219 - footwear_output_acc: 0.5419 - emotion_output_acc: 0.7081 - val_loss: 7.7763 - val_gender_output_loss: 0.6773 - val_image_quality_output_loss: 0.9790 - val_age_output_loss: 1.4469 - val_weight_output_loss: 0.9701 - val_bag_output_loss: 0.9347 - val_pose_output_loss: 0.9450 - val_footwear_output_loss: 0.9564 - val_emotion_output_loss: 0.8669 - val_gender_output_acc: 0.5521 - val_image_quality_output_acc: 0.5541 - val_age_output_acc: 0.3963 - val_weight_output_acc: 0.6394 - val_bag_output_acc: 0.5446 - val_pose_output_acc: 0.5942 - val_footwear_output_acc: 0.5496 - val_emotion_output_acc: 0.7326\n",
            "Epoch 4/100\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0039761002.\n",
            "360/360 [==============================] - 109s 304ms/step - loss: 7.6709 - gender_output_loss: 0.6385 - image_quality_output_loss: 0.9766 - age_output_loss: 1.4082 - weight_output_loss: 0.9763 - bag_output_loss: 0.8967 - pose_output_loss: 0.9119 - footwear_output_loss: 0.9538 - emotion_output_loss: 0.9089 - gender_output_acc: 0.6254 - image_quality_output_acc: 0.5533 - age_output_acc: 0.3998 - weight_output_acc: 0.6343 - bag_output_acc: 0.5683 - pose_output_acc: 0.6218 - footwear_output_acc: 0.5470 - emotion_output_acc: 0.7077 - val_loss: 7.8888 - val_gender_output_loss: 0.6515 - val_image_quality_output_loss: 0.9933 - val_age_output_loss: 1.4263 - val_weight_output_loss: 0.9743 - val_bag_output_loss: 0.9418 - val_pose_output_loss: 0.9502 - val_footwear_output_loss: 1.0859 - val_emotion_output_loss: 0.8655 - val_gender_output_acc: 0.6200 - val_image_quality_output_acc: 0.5446 - val_age_output_acc: 0.3765 - val_weight_output_acc: 0.6384 - val_bag_output_acc: 0.5481 - val_pose_output_acc: 0.5942 - val_footwear_output_acc: 0.4459 - val_emotion_output_acc: 0.7326\n",
            "Epoch 5/100\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0039602591.\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0039761002.\n",
            "360/360 [==============================] - 109s 304ms/step - loss: 7.6396 - gender_output_loss: 0.6311 - image_quality_output_loss: 0.9746 - age_output_loss: 1.4051 - weight_output_loss: 0.9737 - bag_output_loss: 0.8924 - pose_output_loss: 0.9097 - footwear_output_loss: 0.9443 - emotion_output_loss: 0.9087 - gender_output_acc: 0.6365 - image_quality_output_acc: 0.5530 - age_output_acc: 0.3982 - weight_output_acc: 0.6352 - bag_output_acc: 0.5706 - pose_output_acc: 0.6218 - footwear_output_acc: 0.5543 - emotion_output_acc: 0.7077 - val_loss: 7.6378 - val_gender_output_loss: 0.6158 - val_image_quality_output_loss: 0.9824 - val_age_output_loss: 1.4265 - val_weight_output_loss: 0.9669 - val_bag_output_loss: 0.9151 - val_pose_output_loss: 0.9386 - val_footwear_output_loss: 0.9258 - val_emotion_output_loss: 0.8667 - val_gender_output_acc: 0.6587 - val_image_quality_output_acc: 0.5570 - val_age_output_acc: 0.3924 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5516 - val_pose_output_acc: 0.5942 - val_footwear_output_acc: 0.5729 - val_emotion_output_acc: 0.7326\n",
            "Epoch 6/100\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0039405565.\n",
            "360/360 [==============================] - 110s 306ms/step - loss: 7.6037 - gender_output_loss: 0.6239 - image_quality_output_loss: 0.9713 - age_output_loss: 1.4028 - weight_output_loss: 0.9729 - bag_output_loss: 0.8893 - pose_output_loss: 0.9032 - footwear_output_loss: 0.9329 - emotion_output_loss: 0.9074 - gender_output_acc: 0.6453 - image_quality_output_acc: 0.5534 - age_output_acc: 0.4000 - weight_output_acc: 0.6344 - bag_output_acc: 0.5740 - pose_output_acc: 0.6218 - footwear_output_acc: 0.5609 - emotion_output_acc: 0.7076 - val_loss: 7.6080 - val_gender_output_loss: 0.5981 - val_image_quality_output_loss: 0.9785 - val_age_output_loss: 1.4113 - val_weight_output_loss: 0.9599 - val_bag_output_loss: 0.9123 - val_pose_output_loss: 0.9291 - val_footwear_output_loss: 0.9546 - val_emotion_output_loss: 0.8642 - val_gender_output_acc: 0.6582 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3889 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5575 - val_pose_output_acc: 0.5947 - val_footwear_output_acc: 0.5213 - val_emotion_output_acc: 0.7326\n",
            "Epoch 7/100\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0039170543.\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0039405565.\n",
            "Epoch 6/100\n",
            "360/360 [==============================] - 110s 307ms/step - loss: 7.5793 - gender_output_loss: 0.6197 - image_quality_output_loss: 0.9669 - age_output_loss: 1.4016 - weight_output_loss: 0.9714 - bag_output_loss: 0.8879 - pose_output_loss: 0.8980 - footwear_output_loss: 0.9274 - emotion_output_loss: 0.9064 - gender_output_acc: 0.6536 - image_quality_output_acc: 0.5523 - age_output_acc: 0.4006 - weight_output_acc: 0.6346 - bag_output_acc: 0.5765 - pose_output_acc: 0.6222 - footwear_output_acc: 0.5646 - emotion_output_acc: 0.7077 - val_loss: 8.0234 - val_gender_output_loss: 0.6347 - val_image_quality_output_loss: 1.0059 - val_age_output_loss: 1.4178 - val_weight_output_loss: 0.9694 - val_bag_output_loss: 0.9128 - val_pose_output_loss: 0.9395 - val_footwear_output_loss: 1.2699 - val_emotion_output_loss: 0.8733 - val_gender_output_acc: 0.6384 - val_image_quality_output_acc: 0.5546 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6394 - val_bag_output_acc: 0.5590 - val_pose_output_acc: 0.5942 - val_footwear_output_acc: 0.3899 - val_emotion_output_acc: 0.7326\n",
            "Epoch 8/100\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0038898255.\n",
            "360/360 [==============================] - 110s 306ms/step - loss: 7.5524 - gender_output_loss: 0.6115 - image_quality_output_loss: 0.9639 - age_output_loss: 1.4012 - weight_output_loss: 0.9697 - bag_output_loss: 0.8869 - pose_output_loss: 0.8917 - footwear_output_loss: 0.9210 - emotion_output_loss: 0.9066 - gender_output_acc: 0.6596 - image_quality_output_acc: 0.5539 - age_output_acc: 0.4024 - weight_output_acc: 0.6350 - bag_output_acc: 0.5770 - pose_output_acc: 0.6220 - footwear_output_acc: 0.5720 - emotion_output_acc: 0.7078 - val_loss: 7.5395 - val_gender_output_loss: 0.6015 - val_image_quality_output_loss: 0.9849 - val_age_output_loss: 1.4096 - val_weight_output_loss: 0.9601 - val_bag_output_loss: 0.9063 - val_pose_output_loss: 0.9185 - val_footwear_output_loss: 0.8929 - val_emotion_output_loss: 0.8656 - val_gender_output_acc: 0.6637 - val_image_quality_output_acc: 0.5407 - val_age_output_acc: 0.3919 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5561 - val_pose_output_acc: 0.5942 - val_footwear_output_acc: 0.5883 - val_emotion_output_acc: 0.7326\n",
            "Epoch 9/100\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0038589538.\n",
            "360/360 [==============================] - 110s 305ms/step - loss: 7.5250 - gender_output_loss: 0.6035 - image_quality_output_loss: 0.9641 - age_output_loss: 1.3993 - weight_output_loss: 0.9696 - bag_output_loss: 0.8827 - pose_output_loss: 0.8841 - footwear_output_loss: 0.9181 - emotion_output_loss: 0.9037 - gender_output_acc: 0.6679 - image_quality_output_acc: 0.5530 - age_output_acc: 0.3984 - weight_output_acc: 0.6343 - bag_output_acc: 0.5781 - pose_output_acc: 0.6212 - footwear_output_acc: 0.5693 - emotion_output_acc: 0.7080 - val_loss: 7.7135 - val_gender_output_loss: 0.6134 - val_image_quality_output_loss: 0.9690 - val_age_output_loss: 1.4247 - val_weight_output_loss: 0.9622 - val_bag_output_loss: 0.9241 - val_pose_output_loss: 0.8968 - val_footwear_output_loss: 1.0560 - val_emotion_output_loss: 0.8674 - val_gender_output_acc: 0.6612 - val_image_quality_output_acc: 0.5556 - val_age_output_acc: 0.3720 - val_weight_output_acc: 0.6404 - val_bag_output_acc: 0.5501 - val_pose_output_acc: 0.5952 - val_footwear_output_acc: 0.4891 - val_emotion_output_acc: 0.7326\n",
            "Epoch 10/100\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0038245329.\n",
            "360/360 [==============================] - 110s 306ms/step - loss: 7.4894 - gender_output_loss: 0.6008 - image_quality_output_loss: 0.9605 - age_output_loss: 1.3966 - weight_output_loss: 0.9665 - bag_output_loss: 0.8797 - pose_output_loss: 0.8734 - footwear_output_loss: 0.9077 - emotion_output_loss: 0.9041 - gender_output_acc: 0.6694 - image_quality_output_acc: 0.5536 - age_output_acc: 0.3977 - weight_output_acc: 0.6348 - bag_output_acc: 0.5861 - pose_output_acc: 0.6217 - footwear_output_acc: 0.5734 - emotion_output_acc: 0.7079 - val_loss: 7.4420 - val_gender_output_loss: 0.5645 - val_image_quality_output_loss: 0.9750 - val_age_output_loss: 1.4000 - val_weight_output_loss: 0.9550 - val_bag_output_loss: 0.8957 - val_pose_output_loss: 0.9150 - val_footwear_output_loss: 0.8705 - val_emotion_output_loss: 0.8664 - val_gender_output_acc: 0.6999 - val_image_quality_output_acc: 0.5506 - val_age_output_acc: 0.4033 - val_weight_output_acc: 0.6404 - val_bag_output_acc: 0.5744 - val_pose_output_acc: 0.5967 - val_footwear_output_acc: 0.5908 - val_emotion_output_acc: 0.7326\n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0037866663.\n",
            "360/360 [==============================] - 110s 305ms/step - loss: 7.4698 - gender_output_loss: 0.5997 - image_quality_output_loss: 0.9547 - age_output_loss: 1.3951 - weight_output_loss: 0.9692 - bag_output_loss: 0.8788 - pose_output_loss: 0.8611 - footwear_output_loss: 0.9086 - emotion_output_loss: 0.9026 - gender_output_acc: 0.6715 - image_quality_output_acc: 0.5530 - age_output_acc: 0.3997 - weight_output_acc: 0.6348 - bag_output_acc: 0.5844 - pose_output_acc: 0.6211 - footwear_output_acc: 0.5733 - emotion_output_acc: 0.7080 - val_loss: 7.4988 - val_gender_output_loss: 0.5701 - val_image_quality_output_loss: 0.9749 - val_age_output_loss: 1.4034 - val_weight_output_loss: 0.9617 - val_bag_output_loss: 0.8937 - val_pose_output_loss: 0.9179 - val_footwear_output_loss: 0.9139 - val_emotion_output_loss: 0.8631 - val_gender_output_acc: 0.6974 - val_image_quality_output_acc: 0.5585 - val_age_output_acc: 0.3839 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5799 - val_pose_output_acc: 0.6017 - val_footwear_output_acc: 0.5744 - val_emotion_output_acc: 0.7326\n",
            "Epoch 12/100\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0037454662.\n",
            "360/360 [==============================] - 110s 305ms/step - loss: 7.4440 - gender_output_loss: 0.5887 - image_quality_output_loss: 0.9521 - age_output_loss: 1.3945 - weight_output_loss: 0.9659 - bag_output_loss: 0.8773 - pose_output_loss: 0.8545 - footwear_output_loss: 0.9066 - emotion_output_loss: 0.9044 - gender_output_acc: 0.6804 - image_quality_output_acc: 0.5560 - age_output_acc: 0.4013 - weight_output_acc: 0.6351 - bag_output_acc: 0.5878 - pose_output_acc: 0.6267 - footwear_output_acc: 0.5766 - emotion_output_acc: 0.7079 - val_loss: 7.3185 - val_gender_output_loss: 0.5599 - val_image_quality_output_loss: 0.9541 - val_age_output_loss: 1.4023 - val_weight_output_loss: 0.9598 - val_bag_output_loss: 0.8812 - val_pose_output_loss: 0.8387 - val_footwear_output_loss: 0.8621 - val_emotion_output_loss: 0.8604 - val_gender_output_acc: 0.7019 - val_image_quality_output_acc: 0.5546 - val_age_output_acc: 0.3919 - val_weight_output_acc: 0.6384 - val_bag_output_acc: 0.5873 - val_pose_output_acc: 0.6081 - val_footwear_output_acc: 0.6081 - val_emotion_output_acc: 0.7326\n",
            "Epoch 13/100\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0037010536.\n",
            "\n",
            "360/360 [==============================] - 110s 307ms/step - loss: 7.3988 - gender_output_loss: 0.5807 - image_quality_output_loss: 0.9488 - age_output_loss: 1.3922 - weight_output_loss: 0.9662 - bag_output_loss: 0.8744 - pose_output_loss: 0.8351 - footwear_output_loss: 0.8988 - emotion_output_loss: 0.9027 - gender_output_acc: 0.6907 - image_quality_output_acc: 0.5564 - age_output_acc: 0.4018 - weight_output_acc: 0.6345 - bag_output_acc: 0.5918 - pose_output_acc: 0.6325 - footwear_output_acc: 0.5875 - emotion_output_acc: 0.7080 - val_loss: 7.4407 - val_gender_output_loss: 0.5751 - val_image_quality_output_loss: 0.9631 - val_age_output_loss: 1.4169 - val_weight_output_loss: 0.9683 - val_bag_output_loss: 0.8884 - val_pose_output_loss: 0.8578 - val_footwear_output_loss: 0.9065 - val_emotion_output_loss: 0.8645 - val_gender_output_acc: 0.7118 - val_image_quality_output_acc: 0.5347 - val_age_output_acc: 0.3765 - val_weight_output_acc: 0.6384 - val_bag_output_acc: 0.5799 - val_pose_output_acc: 0.6186 - val_footwear_output_acc: 0.5858 - val_emotion_output_acc: 0.7312\n",
            "Epoch 14/100\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0036535573.\n",
            "360/360 [==============================] - 110s 306ms/step - loss: 7.3587 - gender_output_loss: 0.5737 - image_quality_output_loss: 0.9503 - age_output_loss: 1.3889 - weight_output_loss: 0.9637 - bag_output_loss: 0.8681 - pose_output_loss: 0.8199 - footwear_output_loss: 0.8927 - emotion_output_loss: 0.9013 - gender_output_acc: 0.6937 - image_quality_output_acc: 0.5550 - age_output_acc: 0.4049 - weight_output_acc: 0.6350 - bag_output_acc: 0.5990 - pose_output_acc: 0.6398 - footwear_output_acc: 0.5844 - emotion_output_acc: 0.7081 - val_loss: 7.9455 - val_gender_output_loss: 0.5687 - val_image_quality_output_loss: 1.0037 - val_age_output_loss: 1.3966 - val_weight_output_loss: 0.9623 - val_bag_output_loss: 0.8694 - val_pose_output_loss: 0.7986 - val_footwear_output_loss: 1.4802 - val_emotion_output_loss: 0.8660 - val_gender_output_acc: 0.7088 - val_image_quality_output_acc: 0.5000 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6424 - val_bag_output_acc: 0.6037 - val_pose_output_acc: 0.6324 - val_footwear_output_acc: 0.3963 - val_emotion_output_acc: 0.7326\n",
            "Epoch 15/100\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0036031137.\n",
            "360/360 [==============================] - 110s 305ms/step - loss: 7.3386 - gender_output_loss: 0.5695 - image_quality_output_loss: 0.9517 - age_output_loss: 1.3882 - weight_output_loss: 0.9640 - bag_output_loss: 0.8690 - pose_output_loss: 0.8036 - footwear_output_loss: 0.8931 - emotion_output_loss: 0.8996 - gender_output_acc: 0.6944 - image_quality_output_acc: 0.5547 - age_output_acc: 0.4019 - weight_output_acc: 0.6352 - bag_output_acc: 0.5974 - pose_output_acc: 0.6473 - footwear_output_acc: 0.5910 - emotion_output_acc: 0.7079 - val_loss: 7.4932 - val_gender_output_loss: 0.5427 - val_image_quality_output_loss: 1.0762 - val_age_output_loss: 1.4092 - val_weight_output_loss: 0.9513 - val_bag_output_loss: 0.8962 - val_pose_output_loss: 0.8294 - val_footwear_output_loss: 0.9014 - val_emotion_output_loss: 0.8868 - val_gender_output_acc: 0.7252 - val_image_quality_output_acc: 0.4350 - val_age_output_acc: 0.3919 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5744 - val_pose_output_acc: 0.6503 - val_footwear_output_acc: 0.5739 - val_emotion_output_acc: 0.7302\n",
            "Epoch 16/100\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0035498658.\n",
            "360/360 [==============================] - 110s 305ms/step - loss: 7.3066 - gender_output_loss: 0.5589 - image_quality_output_loss: 0.9474 - age_output_loss: 1.3861 - weight_output_loss: 0.9645 - bag_output_loss: 0.8679 - pose_output_loss: 0.7854 - footwear_output_loss: 0.8978 - emotion_output_loss: 0.8987 - gender_output_acc: 0.7098 - image_quality_output_acc: 0.5563 - age_output_acc: 0.4035 - weight_output_acc: 0.6352 - bag_output_acc: 0.5966 - pose_output_acc: 0.6527 - footwear_output_acc: 0.5859 - emotion_output_acc: 0.7080 - val_loss: 7.2984 - val_gender_output_loss: 0.5013 - val_image_quality_output_loss: 1.0091 - val_age_output_loss: 1.3985 - val_weight_output_loss: 0.9590 - val_bag_output_loss: 0.8703 - val_pose_output_loss: 0.7830 - val_footwear_output_loss: 0.9149 - val_emotion_output_loss: 0.8623 - val_gender_output_acc: 0.7515 - val_image_quality_output_acc: 0.4950 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.6443 - val_bag_output_acc: 0.6047 - val_pose_output_acc: 0.6587 - val_footwear_output_acc: 0.5873 - val_emotion_output_acc: 0.7316\n",
            "Epoch 17/100\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.0034939625.\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0035498658.\n",
            "\n",
            "360/360 [==============================] - 111s 308ms/step - loss: 7.2619 - gender_output_loss: 0.5471 - image_quality_output_loss: 0.9389 - age_output_loss: 1.3838 - weight_output_loss: 0.9631 - bag_output_loss: 0.8639 - pose_output_loss: 0.7792 - footwear_output_loss: 0.8888 - emotion_output_loss: 0.8971 - gender_output_acc: 0.7178 - image_quality_output_acc: 0.5597 - age_output_acc: 0.4050 - weight_output_acc: 0.6345 - bag_output_acc: 0.5977 - pose_output_acc: 0.6568 - footwear_output_acc: 0.5838 - emotion_output_acc: 0.7081 - val_loss: 7.1439 - val_gender_output_loss: 0.5124 - val_image_quality_output_loss: 0.9500 - val_age_output_loss: 1.4028 - val_weight_output_loss: 0.9546 - val_bag_output_loss: 0.8644 - val_pose_output_loss: 0.7295 - val_footwear_output_loss: 0.8778 - val_emotion_output_loss: 0.8524 - val_gender_output_acc: 0.7401 - val_image_quality_output_acc: 0.5541 - val_age_output_acc: 0.3819 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.6052 - val_pose_output_acc: 0.6741 - val_footwear_output_acc: 0.5918 - val_emotion_output_acc: 0.7326\n",
            "Epoch 18/100\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0034355581.\n",
            "\n",
            "360/360 [==============================] - 111s 308ms/step - loss: 7.2232 - gender_output_loss: 0.5359 - image_quality_output_loss: 0.9397 - age_output_loss: 1.3851 - weight_output_loss: 0.9609 - bag_output_loss: 0.8617 - pose_output_loss: 0.7519 - footwear_output_loss: 0.8908 - emotion_output_loss: 0.8972 - gender_output_acc: 0.7244 - image_quality_output_acc: 0.5574 - age_output_acc: 0.4057 - weight_output_acc: 0.6343 - bag_output_acc: 0.6014 - pose_output_acc: 0.6685 - footwear_output_acc: 0.5870 - emotion_output_acc: 0.7078 - val_loss: 7.3801 - val_gender_output_loss: 0.5143 - val_image_quality_output_loss: 0.9509 - val_age_output_loss: 1.3933 - val_weight_output_loss: 0.9666 - val_bag_output_loss: 0.9059 - val_pose_output_loss: 0.7171 - val_footwear_output_loss: 1.0769 - val_emotion_output_loss: 0.8551 - val_gender_output_acc: 0.7450 - val_image_quality_output_acc: 0.5570 - val_age_output_acc: 0.3819 - val_weight_output_acc: 0.6310 - val_bag_output_acc: 0.5992 - val_pose_output_acc: 0.6895 - val_footwear_output_acc: 0.4995 - val_emotion_output_acc: 0.7326\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0033748116.\n",
            "360/360 [==============================] - 110s 307ms/step - loss: 7.2055 - gender_output_loss: 0.5315 - image_quality_output_loss: 0.9381 - age_output_loss: 1.3832 - weight_output_loss: 0.9614 - bag_output_loss: 0.8584 - pose_output_loss: 0.7511 - footwear_output_loss: 0.8847 - emotion_output_loss: 0.8971 - gender_output_acc: 0.7294 - image_quality_output_acc: 0.5577 - age_output_acc: 0.4030 - weight_output_acc: 0.6352 - bag_output_acc: 0.6037 - pose_output_acc: 0.6733 - footwear_output_acc: 0.5896 - emotion_output_acc: 0.7079 - val_loss: 7.1276 - val_gender_output_loss: 0.5302 - val_image_quality_output_loss: 0.9741 - val_age_output_loss: 1.3920 - val_weight_output_loss: 0.9469 - val_bag_output_loss: 0.8548 - val_pose_output_loss: 0.6761 - val_footwear_output_loss: 0.9000 - val_emotion_output_loss: 0.8535 - val_gender_output_acc: 0.7312 - val_image_quality_output_acc: 0.5308 - val_age_output_acc: 0.3889 - val_weight_output_acc: 0.6434 - val_bag_output_acc: 0.6200 - val_pose_output_acc: 0.6994 - val_footwear_output_acc: 0.5883 - val_emotion_output_acc: 0.7326\n",
            "Epoch 20/100\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0033118857.\n",
            "360/360 [==============================] - 110s 306ms/step - loss: 7.1667 - gender_output_loss: 0.5260 - image_quality_output_loss: 0.9369 - age_output_loss: 1.3780 - weight_output_loss: 0.9619 - bag_output_loss: 0.8576 - pose_output_loss: 0.7270 - footwear_output_loss: 0.8830 - emotion_output_loss: 0.8963 - gender_output_acc: 0.7289 - image_quality_output_acc: 0.5568 - age_output_acc: 0.4062 - weight_output_acc: 0.6353 - bag_output_acc: 0.6060 - pose_output_acc: 0.6822 - footwear_output_acc: 0.5929 - emotion_output_acc: 0.7079 - val_loss: 7.3260 - val_gender_output_loss: 0.5719 - val_image_quality_output_loss: 0.9545 - val_age_output_loss: 1.4469 - val_weight_output_loss: 0.9656 - val_bag_output_loss: 0.8909 - val_pose_output_loss: 0.7155 - val_footwear_output_loss: 0.9277 - val_emotion_output_loss: 0.8528 - val_gender_output_acc: 0.7113 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3651 - val_weight_output_acc: 0.6250 - val_bag_output_acc: 0.5883 - val_pose_output_acc: 0.7133 - val_footwear_output_acc: 0.5699 - val_emotion_output_acc: 0.7312\n",
            "Epoch 21/100\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0032469468.\n",
            "360/360 [==============================] - 110s 305ms/step - loss: 7.1389 - gender_output_loss: 0.5103 - image_quality_output_loss: 0.9337 - age_output_loss: 1.3795 - weight_output_loss: 0.9632 - bag_output_loss: 0.8548 - pose_output_loss: 0.7235 - footwear_output_loss: 0.8802 - emotion_output_loss: 0.8937 - gender_output_acc: 0.7435 - image_quality_output_acc: 0.5619 - age_output_acc: 0.4036 - weight_output_acc: 0.6353 - bag_output_acc: 0.6057 - pose_output_acc: 0.6814 - footwear_output_acc: 0.5917 - emotion_output_acc: 0.7082 - val_loss: 7.1207 - val_gender_output_loss: 0.4721 - val_image_quality_output_loss: 0.9441 - val_age_output_loss: 1.4027 - val_weight_output_loss: 0.9588 - val_bag_output_loss: 0.8634 - val_pose_output_loss: 0.7572 - val_footwear_output_loss: 0.8775 - val_emotion_output_loss: 0.8449 - val_gender_output_acc: 0.7718 - val_image_quality_output_acc: 0.5526 - val_age_output_acc: 0.3834 - val_weight_output_acc: 0.6290 - val_bag_output_acc: 0.6121 - val_pose_output_acc: 0.6776 - val_footwear_output_acc: 0.6081 - val_emotion_output_acc: 0.7321\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0032469468.\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0031801633.\n",
            "360/360 [==============================] - 110s 306ms/step - loss: 7.1242 - gender_output_loss: 0.5165 - image_quality_output_loss: 0.9352 - age_output_loss: 1.3762 - weight_output_loss: 0.9585 - bag_output_loss: 0.8469 - pose_output_loss: 0.7155 - footwear_output_loss: 0.8819 - emotion_output_loss: 0.8935 - gender_output_acc: 0.7380 - image_quality_output_acc: 0.5593 - age_output_acc: 0.4061 - weight_output_acc: 0.6362 - bag_output_acc: 0.6105 - pose_output_acc: 0.6869 - footwear_output_acc: 0.5943 - emotion_output_acc: 0.7080 - val_loss: 7.2310 - val_gender_output_loss: 0.4868 - val_image_quality_output_loss: 0.9997 - val_age_output_loss: 1.3852 - val_weight_output_loss: 0.9594 - val_bag_output_loss: 0.9065 - val_pose_output_loss: 0.6899 - val_footwear_output_loss: 0.9499 - val_emotion_output_loss: 0.8536 - val_gender_output_acc: 0.7698 - val_image_quality_output_acc: 0.5154 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6349 - val_bag_output_acc: 0.6042 - val_pose_output_acc: 0.7173 - val_footwear_output_acc: 0.5650 - val_emotion_output_acc: 0.7297\n",
            "Epoch 23/100\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0031117057.\n",
            "360/360 [==============================] - 110s 307ms/step - loss: 7.0822 - gender_output_loss: 0.5042 - image_quality_output_loss: 0.9303 - age_output_loss: 1.3748 - weight_output_loss: 0.9573 - bag_output_loss: 0.8461 - pose_output_loss: 0.7011 - footwear_output_loss: 0.8778 - emotion_output_loss: 0.8907 - gender_output_acc: 0.7484 - image_quality_output_acc: 0.5617 - age_output_acc: 0.4048 - weight_output_acc: 0.6366 - bag_output_acc: 0.6175 - pose_output_acc: 0.6988 - footwear_output_acc: 0.5978 - emotion_output_acc: 0.7078 - val_loss: 7.3931 - val_gender_output_loss: 0.4644 - val_image_quality_output_loss: 0.9849 - val_age_output_loss: 1.3989 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.8399 - val_pose_output_loss: 0.6664 - val_footwear_output_loss: 1.2149 - val_emotion_output_loss: 0.8447 - val_gender_output_acc: 0.7738 - val_image_quality_output_acc: 0.5144 - val_age_output_acc: 0.3725 - val_weight_output_acc: 0.6166 - val_bag_output_acc: 0.6285 - val_pose_output_acc: 0.7059 - val_footwear_output_acc: 0.4306 - val_emotion_output_acc: 0.7326\n",
            "Epoch 24/100\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0030417455.\n",
            "Epoch 23/100\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0031117057.\n",
            "360/360 [==============================] - 110s 306ms/step - loss: 7.0856 - gender_output_loss: 0.5040 - image_quality_output_loss: 0.9292 - age_output_loss: 1.3767 - weight_output_loss: 0.9593 - bag_output_loss: 0.8457 - pose_output_loss: 0.7007 - footwear_output_loss: 0.8801 - emotion_output_loss: 0.8901 - gender_output_acc: 0.7460 - image_quality_output_acc: 0.5631 - age_output_acc: 0.4024 - weight_output_acc: 0.6346 - bag_output_acc: 0.6161 - pose_output_acc: 0.7003 - footwear_output_acc: 0.5943 - emotion_output_acc: 0.7080 - val_loss: 7.0888 - val_gender_output_loss: 0.4803 - val_image_quality_output_loss: 0.9832 - val_age_output_loss: 1.3985 - val_weight_output_loss: 0.9439 - val_bag_output_loss: 0.8474 - val_pose_output_loss: 0.6450 - val_footwear_output_loss: 0.9488 - val_emotion_output_loss: 0.8415 - val_gender_output_acc: 0.7684 - val_image_quality_output_acc: 0.5174 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6419 - val_bag_output_acc: 0.6176 - val_pose_output_acc: 0.7188 - val_footwear_output_acc: 0.5437 - val_emotion_output_acc: 0.7326\n",
            "Epoch 25/100\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0029704545.\n",
            "\n",
            "360/360 [==============================] - 110s 305ms/step - loss: 7.0501 - gender_output_loss: 0.4884 - image_quality_output_loss: 0.9314 - age_output_loss: 1.3768 - weight_output_loss: 0.9563 - bag_output_loss: 0.8450 - pose_output_loss: 0.6918 - footwear_output_loss: 0.8701 - emotion_output_loss: 0.8903 - gender_output_acc: 0.7558 - image_quality_output_acc: 0.5636 - age_output_acc: 0.4039 - weight_output_acc: 0.6365 - bag_output_acc: 0.6189 - pose_output_acc: 0.7054 - footwear_output_acc: 0.5944 - emotion_output_acc: 0.7079 - val_loss: 7.4628 - val_gender_output_loss: 0.4525 - val_image_quality_output_loss: 1.0278 - val_age_output_loss: 1.3856 - val_weight_output_loss: 0.9531 - val_bag_output_loss: 0.8468 - val_pose_output_loss: 0.6594 - val_footwear_output_loss: 1.2918 - val_emotion_output_loss: 0.8458 - val_gender_output_acc: 0.7812 - val_image_quality_output_acc: 0.5104 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6404 - val_bag_output_acc: 0.6280 - val_pose_output_acc: 0.7113 - val_footwear_output_acc: 0.4494 - val_emotion_output_acc: 0.7326\n",
            "Epoch 26/100\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0028980044.\n",
            "360/360 [==============================] - 110s 305ms/step - loss: 7.0132 - gender_output_loss: 0.4873 - image_quality_output_loss: 0.9261 - age_output_loss: 1.3741 - weight_output_loss: 0.9537 - bag_output_loss: 0.8404 - pose_output_loss: 0.6756 - footwear_output_loss: 0.8685 - emotion_output_loss: 0.8877 - gender_output_acc: 0.7610 - image_quality_output_acc: 0.5666 - age_output_acc: 0.4093 - weight_output_acc: 0.6369 - bag_output_acc: 0.6210 - pose_output_acc: 0.7133 - footwear_output_acc: 0.5993 - emotion_output_acc: 0.7079 - val_loss: 6.9866 - val_gender_output_loss: 0.4363 - val_image_quality_output_loss: 0.9588 - val_age_output_loss: 1.3821 - val_weight_output_loss: 0.9418 - val_bag_output_loss: 0.8627 - val_pose_output_loss: 0.6502 - val_footwear_output_loss: 0.9017 - val_emotion_output_loss: 0.8531 - val_gender_output_acc: 0.7966 - val_image_quality_output_acc: 0.5496 - val_age_output_acc: 0.4048 - val_weight_output_acc: 0.6448 - val_bag_output_acc: 0.6096 - val_pose_output_acc: 0.7207 - val_footwear_output_acc: 0.5923 - val_emotion_output_acc: 0.7326\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0028245657.\n",
            "360/360 [==============================] - 110s 306ms/step - loss: 7.0012 - gender_output_loss: 0.4863 - image_quality_output_loss: 0.9234 - age_output_loss: 1.3701 - weight_output_loss: 0.9542 - bag_output_loss: 0.8404 - pose_output_loss: 0.6734 - footwear_output_loss: 0.8645 - emotion_output_loss: 0.8889 - gender_output_acc: 0.7616 - image_quality_output_acc: 0.5616 - age_output_acc: 0.4078 - weight_output_acc: 0.6355 - bag_output_acc: 0.6219 - pose_output_acc: 0.7140 - footwear_output_acc: 0.5993 - emotion_output_acc: 0.7080 - val_loss: 6.9298 - val_gender_output_loss: 0.4454 - val_image_quality_output_loss: 0.9560 - val_age_output_loss: 1.3848 - val_weight_output_loss: 0.9438 - val_bag_output_loss: 0.8630 - val_pose_output_loss: 0.6143 - val_footwear_output_loss: 0.8845 - val_emotion_output_loss: 0.8379 - val_gender_output_acc: 0.7803 - val_image_quality_output_acc: 0.5417 - val_age_output_acc: 0.3859 - val_weight_output_acc: 0.6394 - val_bag_output_acc: 0.6190 - val_pose_output_acc: 0.7475 - val_footwear_output_acc: 0.5818 - val_emotion_output_acc: 0.7326\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0027503073.\n",
            "360/360 [==============================] - 110s 305ms/step - loss: 6.9882 - gender_output_loss: 0.4759 - image_quality_output_loss: 0.9258 - age_output_loss: 1.3732 - weight_output_loss: 0.9526 - bag_output_loss: 0.8356 - pose_output_loss: 0.6711 - footwear_output_loss: 0.8685 - emotion_output_loss: 0.8853 - gender_output_acc: 0.7695 - image_quality_output_acc: 0.5627 - age_output_acc: 0.4024 - weight_output_acc: 0.6366 - bag_output_acc: 0.6296 - pose_output_acc: 0.7152 - footwear_output_acc: 0.5990 - emotion_output_acc: 0.7077 - val_loss: 7.2649 - val_gender_output_loss: 0.4431 - val_image_quality_output_loss: 1.0097 - val_age_output_loss: 1.3865 - val_weight_output_loss: 0.9424 - val_bag_output_loss: 0.8452 - val_pose_output_loss: 0.6723 - val_footwear_output_loss: 1.1262 - val_emotion_output_loss: 0.8395 - val_gender_output_acc: 0.7902 - val_image_quality_output_acc: 0.5184 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6404 - val_bag_output_acc: 0.6300 - val_pose_output_acc: 0.7272 - val_footwear_output_acc: 0.5084 - val_emotion_output_acc: 0.7331\n",
            "Epoch 29/100\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0026753962.\n",
            "316/360 [=========================>....] - ETA: 12s - loss: 6.9541 - gender_output_loss: 0.4716 - image_quality_output_loss: 0.9205 - age_output_loss: 1.3679 - weight_output_loss: 0.9515 - bag_output_loss: 0.8360 - pose_output_loss: 0.6561 - footwear_output_loss: 0.8655 - emotion_output_loss: 0.8849 - gender_output_acc: 0.7675 - image_quality_output_acc: 0.5650 - age_output_acc: 0.4095 - weight_output_acc: 0.6369 - bag_output_acc: 0.6228 - pose_output_acc: 0.7160 - footwear_output_acc: 0.6046 - emotion_output_acc: 0.7080"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-348:\n",
            "Process ForkPoolWorker-337:\n",
            "Process ForkPoolWorker-343:\n",
            "Process ForkPoolWorker-339:\n",
            "Process ForkPoolWorker-346:\n",
            "Process ForkPoolWorker-344:\n",
            "Process ForkPoolWorker-347:\n",
            "Process ForkPoolWorker-341:\n",
            "Process ForkPoolWorker-340:\n",
            "Process ForkPoolWorker-338:\n",
            "Process ForkPoolWorker-345:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "Process ForkPoolWorker-342:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 963, in fit\n",
            "    self.std = np.std(x, axis=(0, self.row_axis, self.col_axis))\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"<ipython-input-5-12b238aa99ad>\", line 35, in __getitem__\n",
            "    self.augmentation.fit(image)\n",
            "  File \"<__array_function__ internals>\", line 6, in std\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\", line 3381, in std\n",
            "    **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\", line 217, in _std\n",
            "    keepdims=keepdims)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\", line 183, in _var\n",
            "    arrmean = umr_sum(arr, axis, dtype, keepdims=True)\n",
            "KeyboardInterrupt\n",
            "Process ForkPoolWorker-351:\n",
            "Process ForkPoolWorker-354:\n",
            "Process ForkPoolWorker-353:\n",
            "Process ForkPoolWorker-349:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Process ForkPoolWorker-350:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "Process ForkPoolWorker-352:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-12-ea90f90139eb>\", line 26, in <module>\n",
            "    callbacks=[checkpoint, early, LearningRateScheduler(scheduler, verbose=1)]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1658, in fit_generator\n",
            "    initial_epoch=initial_epoch)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\", line 215, in fit_generator\n",
            "    class_weight=class_weight)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1449, in train_on_batch\n",
            "    outputs = self.train_function(ins)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 2979, in __call__\n",
            "    return self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 2937, in _call\n",
            "    fetched = self._callable_fn(*array_vals)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1472, in __call__\n",
            "    run_metadata_ptr)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 733, in getmodule\n",
            "    if ismodule(module) and hasattr(module, '__file__'):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
            "    module = self._load()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 44, in _load\n",
            "    module = _importlib.import_module(self.__name__)\n",
            "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/__init__.py\", line 108, in <module>\n",
            "    from tensorflow.contrib import cloud\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/cloud/__init__.py\", line 24, in <module>\n",
            "    from tensorflow.contrib.cloud.python.ops.bigquery_reader_ops import *\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/cloud/__init__.py\", line 28, in <module>\n",
            "    from tensorflow.contrib.bigtable.python.ops.bigtable_api import BigtableClient\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/bigtable/__init__.py\", line 29, in <module>\n",
            "    from tensorflow.contrib.bigtable.python.ops.bigtable_api import BigtableClient\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/bigtable/python/ops/bigtable_api.py\", line 44, in <module>\n",
            "    resource_loader.get_path_to_datafile(\"_bigtable.so\"))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/util/loader.py\", line 56, in load_op_library\n",
            "    ret = load_library.load_op_library(path)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/load_library.py\", line 61, in load_op_library\n",
            "    lib_handle = py_tf.TF_LoadLibrary(library_filename)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9Qcz9_GQANw",
        "colab_type": "code",
        "outputId": "236f745c-ad30-4ac0-fe6b-433e5392e140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "def evaluate_model(model):\n",
        "  results = model.evaluate_generator(valid_gen, verbose=1)\n",
        "  accuracies = {}\n",
        "  losses = {}\n",
        "  for k, v in zip(model.metrics_names, results):\n",
        "    if k.endswith('acc'):\n",
        "      accuracies[k] = round(v * 100, 4)\n",
        "    else:\n",
        "      losses[k] = v\n",
        "  return accuracies\n",
        "\n",
        "evaluate_model(model)\n",
        "\n",
        "# results = model.evaluate_generator(valid_gen, verbose =1)\n",
        "# dict(zip(model.metrics_names, results))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 6s 89ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output_acc': 39.4841,\n",
              " 'bag_output_acc': 64.2361,\n",
              " 'emotion_output_acc': 73.2639,\n",
              " 'footwear_output_acc': 59.7718,\n",
              " 'gender_output_acc': 78.3234,\n",
              " 'image_quality_output_acc': 56.3988,\n",
              " 'pose_output_acc': 74.6032,\n",
              " 'weight_output_acc': 64.7321}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-nJYdSOQAGn",
        "colab_type": "code",
        "outputId": "190585c6-8b25-493f-b6bd-15bd6041816b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(1,1,figsize=(15,5))\n",
        "    # summarize history for accuracy\n",
        "    # axs[0].plot(range(1,len(model_history.history.history['acc'])+1),model_history.history['acc'])\n",
        "    # axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    # axs[0].set_title('Model Accuracy')\n",
        "    # axs[0].set_ylabel('Accuracy')\n",
        "    # axs[0].set_xlabel('Epoch')\n",
        "    # axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    # axs[0].legend(['train', 'val'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs.plot(range(1,len(model_history.history.history['loss'])+1),model_history.history.history['loss'])\n",
        "    axs.plot(range(1,len(model_history.history.history['val_loss'])+1),model_history.history.history['val_loss'])\n",
        "    axs.set_title('Model Loss')\n",
        "    axs.set_ylabel('Loss')\n",
        "    axs.set_xlabel('Epoch')\n",
        "    axs.set_xticks(np.arange(1,len(model_history.history.history['loss'])+1),len(model_history.history.history['loss'])/10)\n",
        "    axs.legend(['train', 'val'], loc='best')\n",
        "    plt.show()\n",
        "# plot model history\n",
        "plot_model_history(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFNCAYAAABSRs15AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUIInIiIiIiISIJTgiYiIiIiIBIj/B1wTyAPaGoFlAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA7Vki4-QAQx",
        "colab_type": "code",
        "outputId": "b16aa630-0663-4ab6-d45d-1923d80a7e62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "  return round(lr * 1/(1 + decay_factor * epoch), 10)\n",
        "\n",
        "lr = 0.004\n",
        "decay_factor = 0.001\n",
        "epoch = 100\n",
        "for i in range(epoch):\n",
        "  lr_new = scheduler(i, lr)\n",
        "  lr = lr_new\n",
        "  if i%5 == 0:\n",
        "    print(\"the epoch number is: \" + str(i) + \" and LR is: \" + str(round(lr,10)))\n",
        "  i = i+1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the epoch number is: 0 and LR is: 0.004\n",
            "the epoch number is: 5 and LR is: 0.0039405558\n",
            "the epoch number is: 10 and LR is: 0.0037866656\n",
            "the epoch number is: 15 and LR is: 0.0035498651\n",
            "the epoch number is: 20 and LR is: 0.0032469459\n",
            "the epoch number is: 25 and LR is: 0.0028980039\n",
            "the epoch number is: 30 and LR is: 0.0025242679\n",
            "the epoch number is: 35 and LR is: 0.0021460304\n",
            "the epoch number is: 40 and LR is: 0.0017809475\n",
            "the epoch number is: 45 and LR is: 0.0014428845\n",
            "the epoch number is: 50 and LR is: 0.0011413717\n",
            "the epoch number is: 55 and LR is: 0.0008816318\n",
            "the epoch number is: 60 and LR is: 0.00066506\n",
            "the epoch number is: 65 and LR is: 0.0004900005\n",
            "the epoch number is: 70 and LR is: 0.0003526487\n",
            "the epoch number is: 75 and LR is: 0.0002479393\n",
            "the epoch number is: 80 and LR is: 0.0001703153\n",
            "the epoch number is: 85 and LR is: 0.0001143177\n",
            "the epoch number is: 90 and LR is: 7.49844e-05\n",
            "the epoch number is: 95 and LR is: 4.80699e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1WB81PaN2ug",
        "colab_type": "code",
        "outputId": "48e431ff-9dc6-48ca-b5db-60ce4a3cf7da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.17.4)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.2.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.3.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=2a502271871d15a28a4911679cc2be4a7a950b473d199d51b8e52516e0a749fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-0.0.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgYCMe1chjKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.callbacks import Callback\n",
        "from seqeval.metrics import f1_score, classification_report\n",
        "\n",
        "\n",
        "class F1Metrics(Callback):\n",
        "\n",
        "    def __init__(self, id2label, pad_value=0, validation_data=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            id2label (dict): id to label mapping.\n",
        "            (e.g. {1: 'B-LOC', 2: 'I-LOC'})\n",
        "            pad_value (int): padding value.\n",
        "        \"\"\"\n",
        "        super(F1Metrics, self).__init__()\n",
        "        self.id2label = id2label\n",
        "        self.pad_value = pad_value\n",
        "        self.validation_data = validation_data\n",
        "        self.is_fit = validation_data is None\n",
        "\n",
        "    def find_pad_index(self, array):\n",
        "        \"\"\"Find padding index.\n",
        "        Args:\n",
        "            array (list): integer list.\n",
        "        Returns:\n",
        "            idx: padding index.\n",
        "        Examples:\n",
        "             >>> array = [1, 2, 0]\n",
        "             >>> self.find_pad_index(array)\n",
        "             2\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return list(array).index(self.pad_value)\n",
        "        except ValueError:\n",
        "            return len(array)\n",
        "\n",
        "    def get_length(self, y):\n",
        "        \"\"\"Get true length of y.\n",
        "        Args:\n",
        "            y (list): padded list.\n",
        "        Returns:\n",
        "            lens: true length of y.\n",
        "        Examples:\n",
        "            >>> y = [[1, 0, 0], [1, 1, 0], [1, 1, 1]]\n",
        "            >>> self.get_length(y)\n",
        "            [1, 2, 3]\n",
        "        \"\"\"\n",
        "        lens = [self.find_pad_index(row) for row in y]\n",
        "        return lens\n",
        "\n",
        "    def convert_idx_to_name(self, y, lens):\n",
        "        \"\"\"Convert label index to name.\n",
        "        Args:\n",
        "            y (list): label index list.\n",
        "            lens (list): true length of y.\n",
        "        Returns:\n",
        "            y: label name list.\n",
        "        Examples:\n",
        "            >>> # assumes that id2label = {1: 'B-LOC', 2: 'I-LOC'}\n",
        "            >>> y = [[1, 0, 0], [1, 2, 0], [1, 1, 1]]\n",
        "            >>> lens = [1, 2, 3]\n",
        "            >>> self.convert_idx_to_name(y, lens)\n",
        "            [['B-LOC'], ['B-LOC', 'I-LOC'], ['B-LOC', 'B-LOC', 'B-LOC']]\n",
        "        \"\"\"\n",
        "        y = [[self.id2label[idx] for idx in row[:l]]\n",
        "             for row, l in zip(y, lens)]\n",
        "        return y\n",
        "\n",
        "    def predict(self, X, y):\n",
        "        \"\"\"Predict sequences.\n",
        "        Args:\n",
        "            X (list): input data.\n",
        "            y (list): tags.\n",
        "        Returns:\n",
        "            y_true: true sequences.\n",
        "            y_pred: predicted sequences.\n",
        "        \"\"\"\n",
        "        y_pred = self.model.predict_on_batch(X)\n",
        "\n",
        "        # reduce dimension.\n",
        "        y_true = np.argmax(y, -1)\n",
        "        y_pred = np.argmax(y_pred, -1)\n",
        "\n",
        "        lens = self.get_length(y_true)\n",
        "\n",
        "        y_true = self.convert_idx_to_name(y_true, lens)\n",
        "        y_pred = self.convert_idx_to_name(y_pred, lens)\n",
        "\n",
        "        return y_true, y_pred\n",
        "\n",
        "    def score(self, y_true, y_pred):\n",
        "        \"\"\"Calculate f1 score.\n",
        "        Args:\n",
        "            y_true (list): true sequences.\n",
        "            y_pred (list): predicted sequences.\n",
        "        Returns:\n",
        "            score: f1 score.\n",
        "        \"\"\"\n",
        "        score = f1_score(y_true, y_pred)\n",
        "        print(' - f1: {:04.2f}'.format(score * 100))\n",
        "        print(classification_report(y_true, y_pred, digits=4))\n",
        "        return score\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if self.is_fit:\n",
        "            self.on_epoch_end_fit(epoch, logs)\n",
        "        else:\n",
        "            self.on_epoch_end_fit_generator(epoch, logs)\n",
        "\n",
        "    def on_epoch_end_fit(self, epoch, logs={}):\n",
        "        X = self.validation_data[0]\n",
        "        y = self.validation_data[1]\n",
        "        y_true, y_pred = self.predict(X, y)\n",
        "        score = self.score(y_true, y_pred)\n",
        "        logs['f1'] = score\n",
        "\n",
        "    def on_epoch_end_fit_generator(self, epoch, logs={}):\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        for X, y in self.validation_data:\n",
        "            y_true_batch, y_pred_batch = self.predict(X, y)\n",
        "            y_true.extend(y_true_batch)\n",
        "            y_pred.extend(y_pred_batch)\n",
        "        score = self.score(y_true, y_pred)\n",
        "        logs['f1'] = score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cieTvEJbe3x",
        "colab_type": "code",
        "outputId": "6e54922b-ea0f-4476-b543-2d31c5303a97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "36*360"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12960"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}