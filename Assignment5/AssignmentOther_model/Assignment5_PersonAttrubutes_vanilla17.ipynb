{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5_PersonAttrubutes_vanilla17.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ckgpeace/EIP4/blob/master/Assignment5/AssignmentOther_model/Assignment5_PersonAttrubutes_vanilla17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El-Dopn8pLpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "2dc7ef1a-5cf1-4d97-b65c-ed6a2a30fde3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "from keras.layers import Activation, Flatten, Dense, Dropout, GlobalAveragePooling2D, AveragePooling2D, Add\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, SeparableConv2D\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input, BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "1a040586-16e5-413f-d0c8-d7ac39c0f511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "763691d2-fb54-4a4d-abee-1383a508e783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=64, shuffle=True, augmentation = None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.resize(cv2.imread(item[\"image_path\"]), (112,112)) for _, item in items.iterrows()])\n",
        "        #Image Normalization\n",
        "        if self.augmentation is not None:\n",
        "          self.augmentation.fit(image)\n",
        "          image = self.augmentation.flow(image,shuffle=False, batch_size = 64 ).next()\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "e5bb68c5-d0cb-4dee-ce24-3f252b05ad45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15, random_state = 404)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "9887054e-65b6-4c98-f23b-10c79b19b1b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10980</th>\n",
              "      <td>resized/10982.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11135</th>\n",
              "      <td>resized/11137.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1784</th>\n",
              "      <td>resized/1785.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7758</th>\n",
              "      <td>resized/7759.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4692</th>\n",
              "      <td>resized/4693.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "10980  resized/10982.jpg              0  ...                        1              0\n",
              "11135  resized/11137.jpg              0  ...                        0              1\n",
              "1784    resized/1785.jpg              0  ...                        1              0\n",
              "7758    resized/7759.jpg              0  ...                        0              1\n",
              "4692    resized/4693.jpg              0  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=64, shuffle=True, \n",
        "                                augmentation = ImageDataGenerator(horizontal_flip=True, \n",
        "                                                                  width_shift_range=0.2,\n",
        "                                                                  height_shift_range=0.2,\n",
        "                                                                  rotation_range=0,\n",
        "                                                                  zoom_range=0.2,\n",
        "                                                                  featurewise_center=True, featurewise_std_normalization=True))\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False, augmentation= ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "24d86c01-b899-4124-fd12-2cfdb0a4c9c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "outputId": "6bdc0cab-86a6-4512-ad98-97375d15437d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "inp = Input(shape = (112,112,3))\n",
        "x = inp\n",
        "\n",
        "### block\n",
        "x = SeparableConv2D(filters=32, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=32, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=64, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=64, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# Pooling \n",
        "x = AveragePooling2D()(x)\n",
        "\n",
        "# ================================================================\n",
        "\n",
        "### block\n",
        "x = SeparableConv2D(filters=64, kernel_size=(3, 3), padding='valid')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=64, kernel_size=(3, 3), padding='valid')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=128, kernel_size=(3, 3), padding='valid')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=128, kernel_size=(3, 3), padding='valid')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# Pooling \n",
        "x = AveragePooling2D()(x)\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "\n",
        "### block\n",
        "x = SeparableConv2D(filters=256, kernel_size=(3, 3), padding='valid')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=256, kernel_size=(3, 3), padding='valid')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# Pooling \n",
        "x = AveragePooling2D()(x)\n",
        "\n",
        "# ================================================================\n",
        "\n",
        "# block  - Last CNN\n",
        "\n",
        "x = SeparableConv2D(filters=512, kernel_size=(3, 3), padding='valid')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=512, kernel_size=(3, 3), padding='valid')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=512, kernel_size=(3, 3), padding='valid')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# Adding Dense layer\n",
        "def final(in_layer, num_units, class_name, output_name):\n",
        "  # Conv with class size\n",
        "  x = SeparableConv2D(filters=num_units[class_name], kernel_size=(1, 1), padding='valid')(in_layer)\n",
        "  # GAP\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  x = Activation('softmax', name = output_name)(x)\n",
        "  return x\n",
        "\n",
        "gender = final(in_layer = x, num_units = num_units,  class_name = \"gender\", output_name = \"gender_output\")\n",
        "image_quality = final(in_layer = x, num_units = num_units,  class_name = \"image_quality\", output_name = \"image_quality_output\")\n",
        "age = final(in_layer = x, num_units = num_units,  class_name = \"age\", output_name = \"age_output\")\n",
        "weight = final(in_layer = x, num_units = num_units,  class_name = \"weight\", output_name = \"weight_output\")\n",
        "bag = final(in_layer = x, num_units = num_units,  class_name = \"bag\", output_name = \"bag_output\")\n",
        "footwear = final(in_layer = x, num_units = num_units,  class_name = \"footwear\", output_name = \"footwear_output\")\n",
        "emotion = final(in_layer = x, num_units = num_units,  class_name = \"emotion\", output_name = \"emotion_output\")\n",
        "pose = final(in_layer = x, num_units = num_units,  class_name = \"pose\", output_name = \"pose_output\")\n",
        "\n",
        "model = Model(inputs = inp,outputs=[gender, image_quality, age, weight, bag, pose, footwear, emotion])\n",
        "model.summary()\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_12 (InputLayer)           (None, 112, 112, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_228 (Separable (None, 110, 110, 32) 155         input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_154 (Activation)     (None, 110, 110, 32) 0           separable_conv2d_228[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_154 (BatchN (None, 110, 110, 32) 128         activation_154[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_229 (Separable (None, 108, 108, 32) 1344        batch_normalization_154[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, 108, 108, 32) 0           separable_conv2d_229[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_155 (BatchN (None, 108, 108, 32) 128         activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_230 (Separable (None, 106, 106, 64) 2400        batch_normalization_155[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, 106, 106, 64) 0           separable_conv2d_230[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_156 (BatchN (None, 106, 106, 64) 256         activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_231 (Separable (None, 104, 104, 64) 4736        batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_157 (Activation)     (None, 104, 104, 64) 0           separable_conv2d_231[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_157 (BatchN (None, 104, 104, 64) 256         activation_157[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_42 (AveragePo (None, 52, 52, 64)   0           batch_normalization_157[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_232 (Separable (None, 50, 50, 128)  8896        average_pooling2d_42[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_158 (Activation)     (None, 50, 50, 128)  0           separable_conv2d_232[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_158 (BatchN (None, 50, 50, 128)  512         activation_158[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_233 (Separable (None, 48, 48, 128)  17664       batch_normalization_158[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, 48, 48, 128)  0           separable_conv2d_233[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 48, 48, 128)  0           activation_159[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_159 (BatchN (None, 48, 48, 128)  512         dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_43 (AveragePo (None, 24, 24, 128)  0           batch_normalization_159[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_234 (Separable (None, 22, 22, 256)  34176       average_pooling2d_43[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, 22, 22, 256)  0           separable_conv2d_234[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_160 (BatchN (None, 22, 22, 256)  1024        activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_235 (Separable (None, 20, 20, 256)  68096       batch_normalization_160[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, 20, 20, 256)  0           separable_conv2d_235[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 20, 20, 256)  0           activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_161 (BatchN (None, 20, 20, 256)  1024        dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_44 (AveragePo (None, 10, 10, 256)  0           batch_normalization_161[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_236 (Separable (None, 8, 8, 512)    133888      average_pooling2d_44[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_162 (Activation)     (None, 8, 8, 512)    0           separable_conv2d_236[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_162 (BatchN (None, 8, 8, 512)    2048        activation_162[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_237 (Separable (None, 6, 6, 512)    267264      batch_normalization_162[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_163 (Activation)     (None, 6, 6, 512)    0           separable_conv2d_237[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_163 (BatchN (None, 6, 6, 512)    2048        activation_163[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_238 (Separable (None, 6, 6, 2)      1538        batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_239 (Separable (None, 6, 6, 3)      2051        batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_240 (Separable (None, 6, 6, 5)      3077        batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_241 (Separable (None, 6, 6, 4)      2564        batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_242 (Separable (None, 6, 6, 3)      2051        batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_245 (Separable (None, 6, 6, 3)      2051        batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_243 (Separable (None, 6, 6, 3)      2051        batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_244 (Separable (None, 6, 6, 4)      2564        batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_73 (Gl (None, 2)            0           separable_conv2d_238[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_74 (Gl (None, 3)            0           separable_conv2d_239[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_75 (Gl (None, 5)            0           separable_conv2d_240[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_76 (Gl (None, 4)            0           separable_conv2d_241[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_77 (Gl (None, 3)            0           separable_conv2d_242[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_80 (Gl (None, 3)            0           separable_conv2d_245[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_78 (Gl (None, 3)            0           separable_conv2d_243[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_79 (Gl (None, 4)            0           separable_conv2d_244[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Activation)      (None, 2)            0           global_average_pooling2d_73[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "image_quality_output (Activatio (None, 3)            0           global_average_pooling2d_74[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "age_output (Activation)         (None, 5)            0           global_average_pooling2d_75[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "weight_output (Activation)      (None, 4)            0           global_average_pooling2d_76[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "bag_output (Activation)         (None, 3)            0           global_average_pooling2d_77[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "pose_output (Activation)        (None, 3)            0           global_average_pooling2d_80[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "footwear_output (Activation)    (None, 3)            0           global_average_pooling2d_78[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "emotion_output (Activation)     (None, 4)            0           global_average_pooling2d_79[0][0]\n",
            "==================================================================================================\n",
            "Total params: 564,502\n",
            "Trainable params: 560,534\n",
            "Non-trainable params: 3,968\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkqMc7GkN2sl",
        "colab_type": "code",
        "outputId": "f1842749-e3ff-4868-c002-a77c848603cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# from keras.utils import plot_model\n",
        "# plot_model(model)\n",
        "import time, psutil\n",
        "uptime = time.time() - psutil.boot_time()\n",
        "remain = 12*60*60 - uptime\n",
        "remain/(60*60)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11.732399597697787"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1dcoduDN2pd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses = {\n",
        "\t\"gender_output\": \"categorical_crossentropy\",\n",
        "\t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "\t\"age_output\": \"categorical_crossentropy\",\n",
        "\t\"weight_output\": \"categorical_crossentropy\",\n",
        "  \"bag_output\":  \"categorical_crossentropy\",\n",
        "  \"pose_output\": \"categorical_crossentropy\",\n",
        "  \"footwear_output\": \"categorical_crossentropy\",\n",
        "  \"emotion_output\": \"categorical_crossentropy\"\n",
        "}\n",
        "\n",
        "loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0, \"weight_output\" :1.0,  \"bag_output\": 1.0, \"pose_output\": 1.0,  \"footwear_output\": 1.0, \"emotion_output\": 1.0 }\n",
        "\n",
        "\n",
        "\n",
        "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping\n",
        "\n",
        "decay_factor =  0.007\n",
        "def scheduler(epoch, lr):\n",
        "  return round(lr * 1/(1 + decay_factor * epoch), 10)\n",
        "\n",
        "opt = SGD(lr = 0.01, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile( optimizer=opt, loss = losses, loss_weights=loss_weights, metrics=[\"accuracy\"])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyOyW5EOQAEJ",
        "colab_type": "code",
        "outputId": "b6907db2-bd67-4073-9aa6-d5f0655b7c85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# model_path = '/content/gdrive/VGG16_vanila_1.h5'\n",
        "# checkpoint = ModelCheckpoint(model_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='max')\n",
        "\n",
        "import os\n",
        "# Checkpoint saving\n",
        "save_dir = os.path.join(os.getcwd(), \"/gdrive/My\\\\Drive/saved_models/\")\n",
        "model_name = \"model.{epoch:03d}.h5\"\n",
        "if not os.path.isdir(save_dir):\n",
        "  os.makedirs(save_dir)\n",
        "\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "\n",
        "checkpoint = ModelCheckpoint(model_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False)\n",
        "\n",
        "early = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=11, verbose=1, mode='min')\n",
        "\n",
        "# from seqeval.metrics import f1_score, classification_report\n",
        "# id2label = {1: 'B-LOC', 2: 'I-LOC'}\n",
        "# f1score = F1Metrics(id2label)\n",
        "\n",
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=100,\n",
        "    verbose=1,\n",
        "    callbacks=[checkpoint, early, LearningRateScheduler(scheduler, verbose=1)]\n",
        "    # callbacks=[checkpoint, early, LearningRateScheduler(scheduler, verbose=1), f1score ]\n",
        "    # callbacks=[checkpoint, early, LearningRateScheduler(scheduler, verbose=1),TensorBoardColabCallback(tbc)]\n",
        ")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.0099999998.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 8.0001 - gender_output_loss: 0.6688 - image_quality_output_loss: 0.9924 - age_output_loss: 1.4511 - weight_output_loss: 1.0377 - bag_output_loss: 0.9328 - pose_output_loss: 0.9381 - footwear_output_loss: 1.0112 - emotion_output_loss: 0.9681 - gender_output_acc: 0.5821 - image_quality_output_acc: 0.5468 - age_output_acc: 0.3948 - weight_output_acc: 0.6280 - bag_output_acc: 0.5602 - pose_output_acc: 0.6175 - footwear_output_acc: 0.4977 - emotion_output_acc: 0.7010Epoch 1/100\n",
            "180/180 [==============================] - 161s 896ms/step - loss: 7.9974 - gender_output_loss: 0.6688 - image_quality_output_loss: 0.9919 - age_output_loss: 1.4506 - weight_output_loss: 1.0366 - bag_output_loss: 0.9320 - pose_output_loss: 0.9380 - footwear_output_loss: 1.0109 - emotion_output_loss: 0.9685 - gender_output_acc: 0.5822 - image_quality_output_acc: 0.5473 - age_output_acc: 0.3951 - weight_output_acc: 0.6284 - bag_output_acc: 0.5609 - pose_output_acc: 0.6174 - footwear_output_acc: 0.4978 - emotion_output_acc: 0.7007 - val_loss: 9.2191 - val_gender_output_loss: 0.8370 - val_image_quality_output_loss: 1.0974 - val_age_output_loss: 1.6062 - val_weight_output_loss: 0.9948 - val_bag_output_loss: 1.1086 - val_pose_output_loss: 1.1870 - val_footwear_output_loss: 1.4189 - val_emotion_output_loss: 0.9692 - val_gender_output_acc: 0.5459 - val_image_quality_output_acc: 0.4506 - val_age_output_acc: 0.2974 - val_weight_output_acc: 0.6371 - val_bag_output_acc: 0.5444 - val_pose_output_acc: 0.4798 - val_footwear_output_acc: 0.4234 - val_emotion_output_acc: 0.7329\n",
            "Epoch 1/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/100\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0099304864.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 7.7174 - gender_output_loss: 0.6558 - image_quality_output_loss: 0.9758 - age_output_loss: 1.4097 - weight_output_loss: 0.9765 - bag_output_loss: 0.9002 - pose_output_loss: 0.9177 - footwear_output_loss: 0.9715 - emotion_output_loss: 0.9101 - gender_output_acc: 0.6061 - image_quality_output_acc: 0.5536 - age_output_acc: 0.3965 - weight_output_acc: 0.6345 - bag_output_acc: 0.5654 - pose_output_acc: 0.6219 - footwear_output_acc: 0.5297 - emotion_output_acc: 0.7081\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0099304864.\n",
            "180/180 [==============================] - 144s 797ms/step - loss: 7.7166 - gender_output_loss: 0.6556 - image_quality_output_loss: 0.9762 - age_output_loss: 1.4094 - weight_output_loss: 0.9759 - bag_output_loss: 0.8997 - pose_output_loss: 0.9174 - footwear_output_loss: 0.9717 - emotion_output_loss: 0.9106 - gender_output_acc: 0.6063 - image_quality_output_acc: 0.5530 - age_output_acc: 0.3965 - weight_output_acc: 0.6347 - bag_output_acc: 0.5661 - pose_output_acc: 0.6220 - footwear_output_acc: 0.5293 - emotion_output_acc: 0.7078 - val_loss: 9.6705 - val_gender_output_loss: 1.0096 - val_image_quality_output_loss: 1.0061 - val_age_output_loss: 1.5962 - val_weight_output_loss: 1.0423 - val_bag_output_loss: 1.0031 - val_pose_output_loss: 1.0435 - val_footwear_output_loss: 2.0445 - val_emotion_output_loss: 0.9252 - val_gender_output_acc: 0.4758 - val_image_quality_output_acc: 0.5459 - val_age_output_acc: 0.2671 - val_weight_output_acc: 0.6174 - val_bag_output_acc: 0.5383 - val_pose_output_acc: 0.5449 - val_footwear_output_acc: 0.3569 - val_emotion_output_acc: 0.7329\n",
            "Epoch 2/100Epoch 3/100\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0097933795.\n",
            "180/180 [==============================] - 143s 796ms/step - loss: 7.6596 - gender_output_loss: 0.6398 - image_quality_output_loss: 0.9737 - age_output_loss: 1.4042 - weight_output_loss: 0.9737 - bag_output_loss: 0.8934 - pose_output_loss: 0.9091 - footwear_output_loss: 0.9562 - emotion_output_loss: 0.9095 - gender_output_acc: 0.6301 - image_quality_output_acc: 0.5531 - age_output_acc: 0.3981 - weight_output_acc: 0.6346 - bag_output_acc: 0.5725 - pose_output_acc: 0.6218 - footwear_output_acc: 0.5430 - emotion_output_acc: 0.7078 - val_loss: 7.7830 - val_gender_output_loss: 0.7278 - val_image_quality_output_loss: 0.9807 - val_age_output_loss: 1.4482 - val_weight_output_loss: 0.9680 - val_bag_output_loss: 0.9141 - val_pose_output_loss: 0.9444 - val_footwear_output_loss: 0.9300 - val_emotion_output_loss: 0.8697 - val_gender_output_acc: 0.5323 - val_image_quality_output_acc: 0.5519 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.5691 - val_pose_output_acc: 0.5958 - val_footwear_output_acc: 0.5736 - val_emotion_output_acc: 0.7329\n",
            "Epoch 3/100\n",
            "Epoch 4/100\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0095919484.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 7.5998 - gender_output_loss: 0.6220 - image_quality_output_loss: 0.9721 - age_output_loss: 1.4021 - weight_output_loss: 0.9719 - bag_output_loss: 0.8847 - pose_output_loss: 0.8997 - footwear_output_loss: 0.9397 - emotion_output_loss: 0.9075 - gender_output_acc: 0.6500 - image_quality_output_acc: 0.5532 - age_output_acc: 0.3994 - weight_output_acc: 0.6347 - bag_output_acc: 0.5816 - pose_output_acc: 0.6218 - footwear_output_acc: 0.5566 - emotion_output_acc: 0.7081Epoch 4/100\n",
            "180/180 [==============================] - 143s 794ms/step - loss: 7.5982 - gender_output_loss: 0.6215 - image_quality_output_loss: 0.9724 - age_output_loss: 1.4017 - weight_output_loss: 0.9709 - bag_output_loss: 0.8846 - pose_output_loss: 0.8997 - footwear_output_loss: 0.9395 - emotion_output_loss: 0.9079 - gender_output_acc: 0.6506 - image_quality_output_acc: 0.5529 - age_output_acc: 0.3995 - weight_output_acc: 0.6350 - bag_output_acc: 0.5819 - pose_output_acc: 0.6219 - footwear_output_acc: 0.5569 - emotion_output_acc: 0.7079 - val_loss: 7.9580 - val_gender_output_loss: 0.7730 - val_image_quality_output_loss: 1.0343 - val_age_output_loss: 1.4561 - val_weight_output_loss: 0.9661 - val_bag_output_loss: 0.9250 - val_pose_output_loss: 0.9896 - val_footwear_output_loss: 0.9474 - val_emotion_output_loss: 0.8665 - val_gender_output_acc: 0.5035 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.3957 - val_weight_output_acc: 0.6416 - val_bag_output_acc: 0.5519 - val_pose_output_acc: 0.5958 - val_footwear_output_acc: 0.5570 - val_emotion_output_acc: 0.7329\n",
            "Epoch 5/100\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.009330689.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 7.5621 - gender_output_loss: 0.6113 - image_quality_output_loss: 0.9717 - age_output_loss: 1.4005 - weight_output_loss: 0.9713 - bag_output_loss: 0.8804 - pose_output_loss: 0.8918 - footwear_output_loss: 0.9273 - emotion_output_loss: 0.9078 - gender_output_acc: 0.6618 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4028 - weight_output_acc: 0.6345 - bag_output_acc: 0.5815 - pose_output_acc: 0.6226 - footwear_output_acc: 0.5618 - emotion_output_acc: 0.7073Epoch 5/100\n",
            "180/180 [==============================] - 143s 795ms/step - loss: 7.5603 - gender_output_loss: 0.6111 - image_quality_output_loss: 0.9712 - age_output_loss: 1.4007 - weight_output_loss: 0.9708 - bag_output_loss: 0.8808 - pose_output_loss: 0.8919 - footwear_output_loss: 0.9270 - emotion_output_loss: 0.9067 - gender_output_acc: 0.6618 - image_quality_output_acc: 0.5535 - age_output_acc: 0.4029 - weight_output_acc: 0.6348 - bag_output_acc: 0.5817 - pose_output_acc: 0.6221 - footwear_output_acc: 0.5623 - emotion_output_acc: 0.7078 - val_loss: 8.3168 - val_gender_output_loss: 0.9464 - val_image_quality_output_loss: 0.9861 - val_age_output_loss: 1.4470 - val_weight_output_loss: 0.9666 - val_bag_output_loss: 0.9372 - val_pose_output_loss: 0.9796 - val_footwear_output_loss: 1.1671 - val_emotion_output_loss: 0.8869 - val_gender_output_acc: 0.4587 - val_image_quality_output_acc: 0.5454 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.5393 - val_pose_output_acc: 0.5958 - val_footwear_output_acc: 0.3931 - val_emotion_output_acc: 0.7329\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.009330689.\n",
            "Epoch 6/100\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0090151584.\n",
            "180/180 [==============================] - 142s 791ms/step - loss: 7.4988 - gender_output_loss: 0.5922 - image_quality_output_loss: 0.9651 - age_output_loss: 1.3955 - weight_output_loss: 0.9678 - bag_output_loss: 0.8738 - pose_output_loss: 0.8806 - footwear_output_loss: 0.9189 - emotion_output_loss: 0.9050 - gender_output_acc: 0.6810 - image_quality_output_acc: 0.5538 - age_output_acc: 0.4000 - weight_output_acc: 0.6345 - bag_output_acc: 0.5933 - pose_output_acc: 0.6227 - footwear_output_acc: 0.5653 - emotion_output_acc: 0.7080 - val_loss: 7.5144 - val_gender_output_loss: 0.5579 - val_image_quality_output_loss: 0.9858 - val_age_output_loss: 1.4187 - val_weight_output_loss: 0.9630 - val_bag_output_loss: 0.8824 - val_pose_output_loss: 0.9284 - val_footwear_output_loss: 0.9151 - val_emotion_output_loss: 0.8631 - val_gender_output_acc: 0.7127 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.3881 - val_weight_output_acc: 0.6361 - val_bag_output_acc: 0.5978 - val_pose_output_acc: 0.5973 - val_footwear_output_acc: 0.5685 - val_emotion_output_acc: 0.7329\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0090151584.Epoch 7/100\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0086517838.\n",
            "180/180 [==============================] - 143s 796ms/step - loss: 7.4770 - gender_output_loss: 0.5847 - image_quality_output_loss: 0.9645 - age_output_loss: 1.3951 - weight_output_loss: 0.9670 - bag_output_loss: 0.8734 - pose_output_loss: 0.8703 - footwear_output_loss: 0.9183 - emotion_output_loss: 0.9038 - gender_output_acc: 0.6878 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4018 - weight_output_acc: 0.6346 - bag_output_acc: 0.5896 - pose_output_acc: 0.6229 - footwear_output_acc: 0.5693 - emotion_output_acc: 0.7078 - val_loss: 7.8173 - val_gender_output_loss: 0.7377 - val_image_quality_output_loss: 0.9675 - val_age_output_loss: 1.4386 - val_weight_output_loss: 0.9619 - val_bag_output_loss: 0.9354 - val_pose_output_loss: 0.9475 - val_footwear_output_loss: 0.9515 - val_emotion_output_loss: 0.8774 - val_gender_output_acc: 0.5781 - val_image_quality_output_acc: 0.5570 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6416 - val_bag_output_acc: 0.5338 - val_pose_output_acc: 0.5968 - val_footwear_output_acc: 0.5262 - val_emotion_output_acc: 0.7308\n",
            "Epoch 8/100\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0082476489.\n",
            "180/180 [==============================] - 143s 796ms/step - loss: 7.4330 - gender_output_loss: 0.5767 - image_quality_output_loss: 0.9616 - age_output_loss: 1.3929 - weight_output_loss: 0.9648 - bag_output_loss: 0.8662 - pose_output_loss: 0.8588 - footwear_output_loss: 0.9104 - emotion_output_loss: 0.9015 - gender_output_acc: 0.6946 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4021 - weight_output_acc: 0.6340 - bag_output_acc: 0.6008 - pose_output_acc: 0.6260 - footwear_output_acc: 0.5770 - emotion_output_acc: 0.7080 - val_loss: 7.6424 - val_gender_output_loss: 0.5526 - val_image_quality_output_loss: 0.9892 - val_age_output_loss: 1.4057 - val_weight_output_loss: 0.9584 - val_bag_output_loss: 0.8852 - val_pose_output_loss: 0.8875 - val_footwear_output_loss: 1.1022 - val_emotion_output_loss: 0.8615 - val_gender_output_acc: 0.7248 - val_image_quality_output_acc: 0.5519 - val_age_output_acc: 0.3997 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.5953 - val_pose_output_acc: 0.6003 - val_footwear_output_acc: 0.4496 - val_emotion_output_acc: 0.7329\n",
            "Epoch 9/100\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.007810274.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 7.3857 - gender_output_loss: 0.5639 - image_quality_output_loss: 0.9578 - age_output_loss: 1.3910 - weight_output_loss: 0.9628 - bag_output_loss: 0.8688 - pose_output_loss: 0.8397 - footwear_output_loss: 0.9010 - emotion_output_loss: 0.9007 - gender_output_acc: 0.7030 - image_quality_output_acc: 0.5542 - age_output_acc: 0.4025 - weight_output_acc: 0.6350 - bag_output_acc: 0.5959 - pose_output_acc: 0.6308 - footwear_output_acc: 0.5761 - emotion_output_acc: 0.7077Epoch 9/100\n",
            "180/180 [==============================] - 143s 795ms/step - loss: 7.3868 - gender_output_loss: 0.5640 - image_quality_output_loss: 0.9582 - age_output_loss: 1.3909 - weight_output_loss: 0.9632 - bag_output_loss: 0.8689 - pose_output_loss: 0.8403 - footwear_output_loss: 0.9006 - emotion_output_loss: 0.9006 - gender_output_acc: 0.7030 - image_quality_output_acc: 0.5540 - age_output_acc: 0.4026 - weight_output_acc: 0.6346 - bag_output_acc: 0.5959 - pose_output_acc: 0.6306 - footwear_output_acc: 0.5765 - emotion_output_acc: 0.7077 - val_loss: 7.3926 - val_gender_output_loss: 0.5438 - val_image_quality_output_loss: 0.9631 - val_age_output_loss: 1.4203 - val_weight_output_loss: 0.9543 - val_bag_output_loss: 0.8842 - val_pose_output_loss: 0.8517 - val_footwear_output_loss: 0.9207 - val_emotion_output_loss: 0.8545 - val_gender_output_acc: 0.7324 - val_image_quality_output_acc: 0.5549 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.6018 - val_pose_output_acc: 0.6169 - val_footwear_output_acc: 0.5706 - val_emotion_output_acc: 0.7329\n",
            "Epoch 10/100\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0073473887.\n",
            "180/180 [==============================] - 143s 792ms/step - loss: 7.3191 - gender_output_loss: 0.5510 - image_quality_output_loss: 0.9536 - age_output_loss: 1.3887 - weight_output_loss: 0.9597 - bag_output_loss: 0.8649 - pose_output_loss: 0.8083 - footwear_output_loss: 0.8948 - emotion_output_loss: 0.8981 - gender_output_acc: 0.7117 - image_quality_output_acc: 0.5547 - age_output_acc: 0.4052 - weight_output_acc: 0.6352 - bag_output_acc: 0.6018 - pose_output_acc: 0.6431 - footwear_output_acc: 0.5805 - emotion_output_acc: 0.7082 - val_loss: 7.4111 - val_gender_output_loss: 0.5147 - val_image_quality_output_loss: 0.9871 - val_age_output_loss: 1.4013 - val_weight_output_loss: 0.9476 - val_bag_output_loss: 0.8809 - val_pose_output_loss: 0.9286 - val_footwear_output_loss: 0.8935 - val_emotion_output_loss: 0.8572 - val_gender_output_acc: 0.7364 - val_image_quality_output_acc: 0.5514 - val_age_output_acc: 0.3931 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5912 - val_pose_output_acc: 0.6084 - val_footwear_output_acc: 0.5882 - val_emotion_output_acc: 0.7329\n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0068667184.\n",
            "180/180 [==============================] - 142s 790ms/step - loss: 7.2696 - gender_output_loss: 0.5455 - image_quality_output_loss: 0.9515 - age_output_loss: 1.3848 - weight_output_loss: 0.9617 - bag_output_loss: 0.8602 - pose_output_loss: 0.7792 - footwear_output_loss: 0.8913 - emotion_output_loss: 0.8956 - gender_output_acc: 0.7208 - image_quality_output_acc: 0.5561 - age_output_acc: 0.4039 - weight_output_acc: 0.6359 - bag_output_acc: 0.5991 - pose_output_acc: 0.6539 - footwear_output_acc: 0.5826 - emotion_output_acc: 0.7079 - val_loss: 7.2714 - val_gender_output_loss: 0.5343 - val_image_quality_output_loss: 0.9608 - val_age_output_loss: 1.4008 - val_weight_output_loss: 0.9473 - val_bag_output_loss: 0.8776 - val_pose_output_loss: 0.7828 - val_footwear_output_loss: 0.9192 - val_emotion_output_loss: 0.8486 - val_gender_output_acc: 0.7203 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3936 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.6220 - val_pose_output_acc: 0.6492 - val_footwear_output_acc: 0.5756 - val_emotion_output_acc: 0.7329\n",
            "Epoch 11/100\n",
            "Epoch 12/100\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0063757829.\n",
            "180/180 [==============================] - 142s 791ms/step - loss: 7.1964 - gender_output_loss: 0.5246 - image_quality_output_loss: 0.9462 - age_output_loss: 1.3838 - weight_output_loss: 0.9620 - bag_output_loss: 0.8513 - pose_output_loss: 0.7573 - footwear_output_loss: 0.8814 - emotion_output_loss: 0.8897 - gender_output_acc: 0.7340 - image_quality_output_acc: 0.5563 - age_output_acc: 0.4035 - weight_output_acc: 0.6355 - bag_output_acc: 0.6120 - pose_output_acc: 0.6667 - footwear_output_acc: 0.5906 - emotion_output_acc: 0.7079 - val_loss: 7.3242 - val_gender_output_loss: 0.4829 - val_image_quality_output_loss: 0.9586 - val_age_output_loss: 1.4019 - val_weight_output_loss: 0.9480 - val_bag_output_loss: 0.8606 - val_pose_output_loss: 0.8839 - val_footwear_output_loss: 0.9370 - val_emotion_output_loss: 0.8512 - val_gender_output_acc: 0.7616 - val_image_quality_output_acc: 0.5580 - val_age_output_acc: 0.3911 - val_weight_output_acc: 0.6436 - val_bag_output_acc: 0.6114 - val_pose_output_acc: 0.6381 - val_footwear_output_acc: 0.5791 - val_emotion_output_acc: 0.7334\n",
            "Epoch 13/100\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0058817187.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 7.1457 - gender_output_loss: 0.5091 - image_quality_output_loss: 0.9438 - age_output_loss: 1.3826 - weight_output_loss: 0.9582 - bag_output_loss: 0.8485 - pose_output_loss: 0.7303 - footwear_output_loss: 0.8816 - emotion_output_loss: 0.8918 - gender_output_acc: 0.7490 - image_quality_output_acc: 0.5558 - age_output_acc: 0.4035 - weight_output_acc: 0.6350 - bag_output_acc: 0.6119 - pose_output_acc: 0.6828 - footwear_output_acc: 0.5859 - emotion_output_acc: 0.7079\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0058817187.\n",
            "180/180 [==============================] - 143s 792ms/step - loss: 7.1452 - gender_output_loss: 0.5096 - image_quality_output_loss: 0.9434 - age_output_loss: 1.3829 - weight_output_loss: 0.9580 - bag_output_loss: 0.8491 - pose_output_loss: 0.7296 - footwear_output_loss: 0.8812 - emotion_output_loss: 0.8915 - gender_output_acc: 0.7486 - image_quality_output_acc: 0.5563 - age_output_acc: 0.4032 - weight_output_acc: 0.6352 - bag_output_acc: 0.6114 - pose_output_acc: 0.6830 - footwear_output_acc: 0.5861 - emotion_output_acc: 0.7079 - val_loss: 7.3654 - val_gender_output_loss: 0.4871 - val_image_quality_output_loss: 0.9714 - val_age_output_loss: 1.3995 - val_weight_output_loss: 0.9555 - val_bag_output_loss: 0.8753 - val_pose_output_loss: 0.8281 - val_footwear_output_loss: 0.9861 - val_emotion_output_loss: 0.8624 - val_gender_output_acc: 0.7606 - val_image_quality_output_acc: 0.5449 - val_age_output_acc: 0.3931 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.5988 - val_pose_output_acc: 0.6487 - val_footwear_output_acc: 0.5378 - val_emotion_output_acc: 0.7319\n",
            "Epoch 14/100\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0053911263.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 7.1019 - gender_output_loss: 0.5079 - image_quality_output_loss: 0.9395 - age_output_loss: 1.3782 - weight_output_loss: 0.9563 - bag_output_loss: 0.8455 - pose_output_loss: 0.7019 - footwear_output_loss: 0.8811 - emotion_output_loss: 0.8915 - gender_output_acc: 0.7448 - image_quality_output_acc: 0.5617 - age_output_acc: 0.4080 - weight_output_acc: 0.6352 - bag_output_acc: 0.6128 - pose_output_acc: 0.6988 - footwear_output_acc: 0.5863 - emotion_output_acc: 0.7071\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "180/180 [==============================] - 142s 789ms/step - loss: 7.1031 - gender_output_loss: 0.5085 - image_quality_output_loss: 0.9398 - age_output_loss: 1.3784 - weight_output_loss: 0.9565 - bag_output_loss: 0.8458 - pose_output_loss: 0.7016 - footwear_output_loss: 0.8821 - emotion_output_loss: 0.8903 - gender_output_acc: 0.7444 - image_quality_output_acc: 0.5616 - age_output_acc: 0.4081 - weight_output_acc: 0.6353 - bag_output_acc: 0.6126 - pose_output_acc: 0.6989 - footwear_output_acc: 0.5859 - emotion_output_acc: 0.7078 - val_loss: 7.8073 - val_gender_output_loss: 0.5196 - val_image_quality_output_loss: 1.0647 - val_age_output_loss: 1.4034 - val_weight_output_loss: 0.9637 - val_bag_output_loss: 0.8685 - val_pose_output_loss: 0.7676 - val_footwear_output_loss: 1.3537 - val_emotion_output_loss: 0.8662 - val_gender_output_acc: 0.7359 - val_image_quality_output_acc: 0.4597 - val_age_output_acc: 0.3846 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.6089 - val_pose_output_acc: 0.6689 - val_footwear_output_acc: 0.4158 - val_emotion_output_acc: 0.7283\n",
            "Epoch 15/100\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0049099513.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 7.0267 - gender_output_loss: 0.4886 - image_quality_output_loss: 0.9354 - age_output_loss: 1.3760 - weight_output_loss: 0.9542 - bag_output_loss: 0.8360 - pose_output_loss: 0.6810 - footwear_output_loss: 0.8706 - emotion_output_loss: 0.8849 - gender_output_acc: 0.7593 - image_quality_output_acc: 0.5611 - age_output_acc: 0.4059 - weight_output_acc: 0.6356 - bag_output_acc: 0.6237 - pose_output_acc: 0.7072 - footwear_output_acc: 0.5990 - emotion_output_acc: 0.7080\n",
            "180/180 [==============================] - 143s 792ms/step - loss: 7.0263 - gender_output_loss: 0.4885 - image_quality_output_loss: 0.9354 - age_output_loss: 1.3754 - weight_output_loss: 0.9541 - bag_output_loss: 0.8365 - pose_output_loss: 0.6806 - footwear_output_loss: 0.8710 - emotion_output_loss: 0.8849 - gender_output_acc: 0.7593 - image_quality_output_acc: 0.5610 - age_output_acc: 0.4061 - weight_output_acc: 0.6357 - bag_output_acc: 0.6236 - pose_output_acc: 0.7075 - footwear_output_acc: 0.5991 - emotion_output_acc: 0.7081 - val_loss: 7.0533 - val_gender_output_loss: 0.4760 - val_image_quality_output_loss: 1.0017 - val_age_output_loss: 1.3927 - val_weight_output_loss: 0.9519 - val_bag_output_loss: 0.8432 - val_pose_output_loss: 0.6629 - val_footwear_output_loss: 0.8789 - val_emotion_output_loss: 0.8460 - val_gender_output_acc: 0.7777 - val_image_quality_output_acc: 0.5433 - val_age_output_acc: 0.3866 - val_weight_output_acc: 0.6316 - val_bag_output_acc: 0.6280 - val_pose_output_acc: 0.7021 - val_footwear_output_acc: 0.5892 - val_emotion_output_acc: 0.7334\n",
            "Epoch 16/100\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0044433948.\n",
            "180/180 [==============================] - 142s 791ms/step - loss: 6.9933 - gender_output_loss: 0.4821 - image_quality_output_loss: 0.9319 - age_output_loss: 1.3735 - weight_output_loss: 0.9522 - bag_output_loss: 0.8335 - pose_output_loss: 0.6708 - footwear_output_loss: 0.8628 - emotion_output_loss: 0.8866 - gender_output_acc: 0.7601 - image_quality_output_acc: 0.5625 - age_output_acc: 0.4042 - weight_output_acc: 0.6367 - bag_output_acc: 0.6275 - pose_output_acc: 0.7140 - footwear_output_acc: 0.6042 - emotion_output_acc: 0.7076 - val_loss: 7.2533 - val_gender_output_loss: 0.5071 - val_image_quality_output_loss: 0.9439 - val_age_output_loss: 1.3913 - val_weight_output_loss: 0.9507 - val_bag_output_loss: 0.8873 - val_pose_output_loss: 0.6680 - val_footwear_output_loss: 1.0648 - val_emotion_output_loss: 0.8403 - val_gender_output_acc: 0.7374 - val_image_quality_output_acc: 0.5620 - val_age_output_acc: 0.3972 - val_weight_output_acc: 0.6457 - val_bag_output_acc: 0.5922 - val_pose_output_acc: 0.7087 - val_footwear_output_acc: 0.5010 - val_emotion_output_acc: 0.7334\n",
            "Epoch 17/100\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.0039958588.\n",
            "180/180 [==============================] - 142s 791ms/step - loss: 6.9467 - gender_output_loss: 0.4712 - image_quality_output_loss: 0.9314 - age_output_loss: 1.3698 - weight_output_loss: 0.9512 - bag_output_loss: 0.8295 - pose_output_loss: 0.6479 - footwear_output_loss: 0.8658 - emotion_output_loss: 0.8800 - gender_output_acc: 0.7694 - image_quality_output_acc: 0.5602 - age_output_acc: 0.4099 - weight_output_acc: 0.6376 - bag_output_acc: 0.6287 - pose_output_acc: 0.7289 - footwear_output_acc: 0.5998 - emotion_output_acc: 0.7080 - val_loss: 7.1247 - val_gender_output_loss: 0.4607 - val_image_quality_output_loss: 0.9938 - val_age_output_loss: 1.3907 - val_weight_output_loss: 0.9443 - val_bag_output_loss: 0.8447 - val_pose_output_loss: 0.6542 - val_footwear_output_loss: 0.9982 - val_emotion_output_loss: 0.8382 - val_gender_output_acc: 0.7782 - val_image_quality_output_acc: 0.5534 - val_age_output_acc: 0.4057 - val_weight_output_acc: 0.6447 - val_bag_output_acc: 0.6356 - val_pose_output_acc: 0.7152 - val_footwear_output_acc: 0.5096 - val_emotion_output_acc: 0.7329\n",
            "Epoch 18/100\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0035709192.\n",
            "180/180 [==============================] - 142s 791ms/step - loss: 6.9114 - gender_output_loss: 0.4624 - image_quality_output_loss: 0.9284 - age_output_loss: 1.3690 - weight_output_loss: 0.9495 - bag_output_loss: 0.8286 - pose_output_loss: 0.6384 - footwear_output_loss: 0.8538 - emotion_output_loss: 0.8812 - gender_output_acc: 0.7735 - image_quality_output_acc: 0.5620 - age_output_acc: 0.4120 - weight_output_acc: 0.6357 - bag_output_acc: 0.6285 - pose_output_acc: 0.7273 - footwear_output_acc: 0.6043 - emotion_output_acc: 0.7078 - val_loss: 7.0205 - val_gender_output_loss: 0.5086 - val_image_quality_output_loss: 0.9456 - val_age_output_loss: 1.3838 - val_weight_output_loss: 0.9424 - val_bag_output_loss: 0.9192 - val_pose_output_loss: 0.6113 - val_footwear_output_loss: 0.8634 - val_emotion_output_loss: 0.8462 - val_gender_output_acc: 0.7742 - val_image_quality_output_acc: 0.5655 - val_age_output_acc: 0.3921 - val_weight_output_acc: 0.6431 - val_bag_output_acc: 0.5998 - val_pose_output_acc: 0.7480 - val_footwear_output_acc: 0.6023 - val_emotion_output_acc: 0.7293\n",
            "\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0031713314.\n",
            "180/180 [==============================] - 142s 791ms/step - loss: 6.8709 - gender_output_loss: 0.4529 - image_quality_output_loss: 0.9227 - age_output_loss: 1.3662 - weight_output_loss: 0.9479 - bag_output_loss: 0.8226 - pose_output_loss: 0.6241 - footwear_output_loss: 0.8544 - emotion_output_loss: 0.8801 - gender_output_acc: 0.7845 - image_quality_output_acc: 0.5630 - age_output_acc: 0.4108 - weight_output_acc: 0.6369 - bag_output_acc: 0.6378 - pose_output_acc: 0.7385 - footwear_output_acc: 0.6068 - emotion_output_acc: 0.7080 - val_loss: 6.9791 - val_gender_output_loss: 0.4341 - val_image_quality_output_loss: 1.0221 - val_age_output_loss: 1.3806 - val_weight_output_loss: 0.9420 - val_bag_output_loss: 0.8326 - val_pose_output_loss: 0.6201 - val_footwear_output_loss: 0.9089 - val_emotion_output_loss: 0.8388 - val_gender_output_acc: 0.8044 - val_image_quality_output_acc: 0.5333 - val_age_output_acc: 0.4007 - val_weight_output_acc: 0.6442 - val_bag_output_acc: 0.6401 - val_pose_output_acc: 0.7359 - val_footwear_output_acc: 0.5645 - val_emotion_output_acc: 0.7324\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0031713314.\n",
            "Epoch 20/100\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0027990569.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 6.8394 - gender_output_loss: 0.4463 - image_quality_output_loss: 0.9235 - age_output_loss: 1.3623 - weight_output_loss: 0.9456 - bag_output_loss: 0.8173 - pose_output_loss: 0.6146 - footwear_output_loss: 0.8501 - emotion_output_loss: 0.8796 - gender_output_acc: 0.7832 - image_quality_output_acc: 0.5671 - age_output_acc: 0.4076 - weight_output_acc: 0.6366 - bag_output_acc: 0.6415 - pose_output_acc: 0.7421 - footwear_output_acc: 0.6082 - emotion_output_acc: 0.7073Epoch 20/100\n",
            "180/180 [==============================] - 142s 791ms/step - loss: 6.8373 - gender_output_loss: 0.4465 - image_quality_output_loss: 0.9232 - age_output_loss: 1.3624 - weight_output_loss: 0.9455 - bag_output_loss: 0.8170 - pose_output_loss: 0.6142 - footwear_output_loss: 0.8499 - emotion_output_loss: 0.8786 - gender_output_acc: 0.7831 - image_quality_output_acc: 0.5671 - age_output_acc: 0.4079 - weight_output_acc: 0.6369 - bag_output_acc: 0.6418 - pose_output_acc: 0.7423 - footwear_output_acc: 0.6084 - emotion_output_acc: 0.7078 - val_loss: 7.3599 - val_gender_output_loss: 0.4859 - val_image_quality_output_loss: 0.9970 - val_age_output_loss: 1.3846 - val_weight_output_loss: 0.9458 - val_bag_output_loss: 0.8333 - val_pose_output_loss: 0.6316 - val_footwear_output_loss: 1.2421 - val_emotion_output_loss: 0.8395 - val_gender_output_acc: 0.7485 - val_image_quality_output_acc: 0.5383 - val_age_output_acc: 0.3982 - val_weight_output_acc: 0.6447 - val_bag_output_acc: 0.6310 - val_pose_output_acc: 0.7303 - val_footwear_output_acc: 0.4481 - val_emotion_output_acc: 0.7329\n",
            "Epoch 21/100\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0024553131.\n",
            "180/180 [==============================] - 142s 790ms/step - loss: 6.8071 - gender_output_loss: 0.4350 - image_quality_output_loss: 0.9200 - age_output_loss: 1.3631 - weight_output_loss: 0.9429 - bag_output_loss: 0.8171 - pose_output_loss: 0.6052 - footwear_output_loss: 0.8464 - emotion_output_loss: 0.8774 - gender_output_acc: 0.7932 - image_quality_output_acc: 0.5678 - age_output_acc: 0.4071 - weight_output_acc: 0.6395 - bag_output_acc: 0.6389 - pose_output_acc: 0.7472 - footwear_output_acc: 0.6062 - emotion_output_acc: 0.7078 - val_loss: 6.9445 - val_gender_output_loss: 0.5370 - val_image_quality_output_loss: 0.9536 - val_age_output_loss: 1.3812 - val_weight_output_loss: 0.9299 - val_bag_output_loss: 0.8476 - val_pose_output_loss: 0.5689 - val_footwear_output_loss: 0.8883 - val_emotion_output_loss: 0.8380 - val_gender_output_acc: 0.7581 - val_image_quality_output_acc: 0.5610 - val_age_output_acc: 0.3921 - val_weight_output_acc: 0.6477 - val_bag_output_acc: 0.6295 - val_pose_output_acc: 0.7666 - val_footwear_output_acc: 0.5998 - val_emotion_output_acc: 0.7308\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0024553131.Epoch 21/100\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0021406391.\n",
            "180/180 [==============================] - 143s 794ms/step - loss: 6.7643 - gender_output_loss: 0.4293 - image_quality_output_loss: 0.9172 - age_output_loss: 1.3599 - weight_output_loss: 0.9422 - bag_output_loss: 0.8128 - pose_output_loss: 0.5873 - footwear_output_loss: 0.8405 - emotion_output_loss: 0.8751 - gender_output_acc: 0.7968 - image_quality_output_acc: 0.5696 - age_output_acc: 0.4111 - weight_output_acc: 0.6402 - bag_output_acc: 0.6411 - pose_output_acc: 0.7590 - footwear_output_acc: 0.6150 - emotion_output_acc: 0.7080 - val_loss: 7.2034 - val_gender_output_loss: 0.4080 - val_image_quality_output_loss: 1.0378 - val_age_output_loss: 1.3860 - val_weight_output_loss: 0.9444 - val_bag_output_loss: 0.8200 - val_pose_output_loss: 0.5834 - val_footwear_output_loss: 1.1855 - val_emotion_output_loss: 0.8383 - val_gender_output_acc: 0.8075 - val_image_quality_output_acc: 0.5050 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6457 - val_bag_output_acc: 0.6396 - val_pose_output_acc: 0.7545 - val_footwear_output_acc: 0.4551 - val_emotion_output_acc: 0.7324\n",
            "Epoch 23/100\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0018549732.\n",
            "180/180 [==============================] - 142s 787ms/step - loss: 6.7374 - gender_output_loss: 0.4141 - image_quality_output_loss: 0.9157 - age_output_loss: 1.3523 - weight_output_loss: 0.9390 - bag_output_loss: 0.8112 - pose_output_loss: 0.5861 - footwear_output_loss: 0.8434 - emotion_output_loss: 0.8757 - gender_output_acc: 0.8091 - image_quality_output_acc: 0.5661 - age_output_acc: 0.4133 - weight_output_acc: 0.6370 - bag_output_acc: 0.6440 - pose_output_acc: 0.7576 - footwear_output_acc: 0.6154 - emotion_output_acc: 0.7080 - val_loss: 6.8774 - val_gender_output_loss: 0.4607 - val_image_quality_output_loss: 0.9671 - val_age_output_loss: 1.3771 - val_weight_output_loss: 0.9308 - val_bag_output_loss: 0.8355 - val_pose_output_loss: 0.5582 - val_footwear_output_loss: 0.9110 - val_emotion_output_loss: 0.8369 - val_gender_output_acc: 0.7928 - val_image_quality_output_acc: 0.5519 - val_age_output_acc: 0.3957 - val_weight_output_acc: 0.6452 - val_bag_output_acc: 0.6280 - val_pose_output_acc: 0.7722 - val_footwear_output_acc: 0.5811 - val_emotion_output_acc: 0.7329\n",
            "Epoch 24/100\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0015977374.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 6.6969 - gender_output_loss: 0.4137 - image_quality_output_loss: 0.9139 - age_output_loss: 1.3538 - weight_output_loss: 0.9402 - bag_output_loss: 0.8027 - pose_output_loss: 0.5647 - footwear_output_loss: 0.8335 - emotion_output_loss: 0.8743 - gender_output_acc: 0.8128 - image_quality_output_acc: 0.5649 - age_output_acc: 0.4149 - weight_output_acc: 0.6374 - bag_output_acc: 0.6536 - pose_output_acc: 0.7675 - footwear_output_acc: 0.6228 - emotion_output_acc: 0.7075Epoch 24/100\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0015977374.\n",
            "180/180 [==============================] - 142s 791ms/step - loss: 6.6961 - gender_output_loss: 0.4146 - image_quality_output_loss: 0.9137 - age_output_loss: 1.3537 - weight_output_loss: 0.9395 - bag_output_loss: 0.8032 - pose_output_loss: 0.5646 - footwear_output_loss: 0.8337 - emotion_output_loss: 0.8730 - gender_output_acc: 0.8119 - image_quality_output_acc: 0.5649 - age_output_acc: 0.4150 - weight_output_acc: 0.6378 - bag_output_acc: 0.6534 - pose_output_acc: 0.7679 - footwear_output_acc: 0.6229 - emotion_output_acc: 0.7082 - val_loss: 7.0641 - val_gender_output_loss: 0.4261 - val_image_quality_output_loss: 1.0544 - val_age_output_loss: 1.3840 - val_weight_output_loss: 0.9654 - val_bag_output_loss: 0.8198 - val_pose_output_loss: 0.5753 - val_footwear_output_loss: 0.9967 - val_emotion_output_loss: 0.8423 - val_gender_output_acc: 0.8054 - val_image_quality_output_acc: 0.5121 - val_age_output_acc: 0.3891 - val_weight_output_acc: 0.6300 - val_bag_output_acc: 0.6366 - val_pose_output_acc: 0.7697 - val_footwear_output_acc: 0.5413 - val_emotion_output_acc: 0.7329\n",
            "Epoch 25/100\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0013679259.\n",
            "180/180 [==============================] - 143s 793ms/step - loss: 6.6869 - gender_output_loss: 0.4121 - image_quality_output_loss: 0.9106 - age_output_loss: 1.3507 - weight_output_loss: 0.9388 - bag_output_loss: 0.8023 - pose_output_loss: 0.5627 - footwear_output_loss: 0.8377 - emotion_output_loss: 0.8720 - gender_output_acc: 0.8074 - image_quality_output_acc: 0.5674 - age_output_acc: 0.4119 - weight_output_acc: 0.6399 - bag_output_acc: 0.6493 - pose_output_acc: 0.7675 - footwear_output_acc: 0.6150 - emotion_output_acc: 0.7081 - val_loss: 6.6994 - val_gender_output_loss: 0.3966 - val_image_quality_output_loss: 0.9751 - val_age_output_loss: 1.3743 - val_weight_output_loss: 0.9225 - val_bag_output_loss: 0.8102 - val_pose_output_loss: 0.5349 - val_footwear_output_loss: 0.8531 - val_emotion_output_loss: 0.8326 - val_gender_output_acc: 0.8236 - val_image_quality_output_acc: 0.5499 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6512 - val_bag_output_acc: 0.6447 - val_pose_output_acc: 0.7777 - val_footwear_output_acc: 0.6064 - val_emotion_output_acc: 0.7324\n",
            "Epoch 26/100\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0011641923.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 6.6649 - gender_output_loss: 0.4038 - image_quality_output_loss: 0.9106 - age_output_loss: 1.3534 - weight_output_loss: 0.9369 - bag_output_loss: 0.7966 - pose_output_loss: 0.5597 - footwear_output_loss: 0.8316 - emotion_output_loss: 0.8723 - gender_output_acc: 0.8139 - image_quality_output_acc: 0.5686 - age_output_acc: 0.4084 - weight_output_acc: 0.6380 - bag_output_acc: 0.6542 - pose_output_acc: 0.7723 - footwear_output_acc: 0.6210 - emotion_output_acc: 0.7078\n",
            "Epoch 26/100\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0011641923.\n",
            "180/180 [==============================] - 143s 793ms/step - loss: 6.6631 - gender_output_loss: 0.4034 - image_quality_output_loss: 0.9105 - age_output_loss: 1.3530 - weight_output_loss: 0.9370 - bag_output_loss: 0.7965 - pose_output_loss: 0.5592 - footwear_output_loss: 0.8314 - emotion_output_loss: 0.8722 - gender_output_acc: 0.8142 - image_quality_output_acc: 0.5687 - age_output_acc: 0.4087 - weight_output_acc: 0.6378 - bag_output_acc: 0.6543 - pose_output_acc: 0.7724 - footwear_output_acc: 0.6213 - emotion_output_acc: 0.7080 - val_loss: 6.7782 - val_gender_output_loss: 0.3977 - val_image_quality_output_loss: 0.9966 - val_age_output_loss: 1.3794 - val_weight_output_loss: 0.9421 - val_bag_output_loss: 0.8252 - val_pose_output_loss: 0.5352 - val_footwear_output_loss: 0.8615 - val_emotion_output_loss: 0.8406 - val_gender_output_acc: 0.8266 - val_image_quality_output_acc: 0.5368 - val_age_output_acc: 0.3831 - val_weight_output_acc: 0.6381 - val_bag_output_acc: 0.6477 - val_pose_output_acc: 0.7818 - val_footwear_output_acc: 0.5872 - val_emotion_output_acc: 0.7324\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0009849343.\n",
            "180/180 [==============================] - 142s 789ms/step - loss: 6.6332 - gender_output_loss: 0.4015 - image_quality_output_loss: 0.9071 - age_output_loss: 1.3477 - weight_output_loss: 0.9332 - bag_output_loss: 0.8000 - pose_output_loss: 0.5426 - footwear_output_loss: 0.8311 - emotion_output_loss: 0.8699 - gender_output_acc: 0.8144 - image_quality_output_acc: 0.5676 - age_output_acc: 0.4108 - weight_output_acc: 0.6423 - bag_output_acc: 0.6517 - pose_output_acc: 0.7790 - footwear_output_acc: 0.6212 - emotion_output_acc: 0.7083 - val_loss: 6.7266 - val_gender_output_loss: 0.3871 - val_image_quality_output_loss: 0.9624 - val_age_output_loss: 1.3634 - val_weight_output_loss: 0.9226 - val_bag_output_loss: 0.8144 - val_pose_output_loss: 0.5428 - val_footwear_output_loss: 0.9020 - val_emotion_output_loss: 0.8320 - val_gender_output_acc: 0.8221 - val_image_quality_output_acc: 0.5459 - val_age_output_acc: 0.3992 - val_weight_output_acc: 0.6447 - val_bag_output_acc: 0.6502 - val_pose_output_acc: 0.7782 - val_footwear_output_acc: 0.5746 - val_emotion_output_acc: 0.7329\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.000828372.\n",
            "180/180 [==============================] - 142s 790ms/step - loss: 6.6392 - gender_output_loss: 0.3976 - image_quality_output_loss: 0.9066 - age_output_loss: 1.3479 - weight_output_loss: 0.9337 - bag_output_loss: 0.8007 - pose_output_loss: 0.5503 - footwear_output_loss: 0.8302 - emotion_output_loss: 0.8722 - gender_output_acc: 0.8214 - image_quality_output_acc: 0.5706 - age_output_acc: 0.4117 - weight_output_acc: 0.6418 - bag_output_acc: 0.6468 - pose_output_acc: 0.7741 - footwear_output_acc: 0.6176 - emotion_output_acc: 0.7081 - val_loss: 6.7347 - val_gender_output_loss: 0.3883 - val_image_quality_output_loss: 0.9931 - val_age_output_loss: 1.3715 - val_weight_output_loss: 0.9361 - val_bag_output_loss: 0.8056 - val_pose_output_loss: 0.5303 - val_footwear_output_loss: 0.8720 - val_emotion_output_loss: 0.8378 - val_gender_output_acc: 0.8327 - val_image_quality_output_acc: 0.5323 - val_age_output_acc: 0.3891 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.6467 - val_pose_output_acc: 0.7848 - val_footwear_output_acc: 0.5922 - val_emotion_output_acc: 0.7324\n",
            "Epoch 29/100\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0006926187.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 6.6070 - gender_output_loss: 0.3927 - image_quality_output_loss: 0.9049 - age_output_loss: 1.3454 - weight_output_loss: 0.9314 - bag_output_loss: 0.7904 - pose_output_loss: 0.5512 - footwear_output_loss: 0.8199 - emotion_output_loss: 0.8713 - gender_output_acc: 0.8225 - image_quality_output_acc: 0.5690 - age_output_acc: 0.4174 - weight_output_acc: 0.6439 - bag_output_acc: 0.6593 - pose_output_acc: 0.7774 - footwear_output_acc: 0.6240 - emotion_output_acc: 0.7076\n",
            "180/180 [==============================] - 142s 791ms/step - loss: 6.6063 - gender_output_loss: 0.3929 - image_quality_output_loss: 0.9050 - age_output_loss: 1.3450 - weight_output_loss: 0.9312 - bag_output_loss: 0.7913 - pose_output_loss: 0.5508 - footwear_output_loss: 0.8197 - emotion_output_loss: 0.8703 - gender_output_acc: 0.8223 - image_quality_output_acc: 0.5689 - age_output_acc: 0.4174 - weight_output_acc: 0.6438 - bag_output_acc: 0.6591 - pose_output_acc: 0.7776 - footwear_output_acc: 0.6241 - emotion_output_acc: 0.7081 - val_loss: 6.6897 - val_gender_output_loss: 0.3954 - val_image_quality_output_loss: 0.9475 - val_age_output_loss: 1.3647 - val_weight_output_loss: 0.9179 - val_bag_output_loss: 0.8143 - val_pose_output_loss: 0.5281 - val_footwear_output_loss: 0.8870 - val_emotion_output_loss: 0.8348 - val_gender_output_acc: 0.8306 - val_image_quality_output_acc: 0.5514 - val_age_output_acc: 0.4032 - val_weight_output_acc: 0.6462 - val_bag_output_acc: 0.6416 - val_pose_output_acc: 0.7823 - val_footwear_output_acc: 0.5927 - val_emotion_output_acc: 0.7324\n",
            "Epoch 30/100\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0005757429.\n",
            "180/180 [==============================] - 142s 790ms/step - loss: 6.5858 - gender_output_loss: 0.3852 - image_quality_output_loss: 0.9072 - age_output_loss: 1.3433 - weight_output_loss: 0.9291 - bag_output_loss: 0.7893 - pose_output_loss: 0.5336 - footwear_output_loss: 0.8285 - emotion_output_loss: 0.8697 - gender_output_acc: 0.8242 - image_quality_output_acc: 0.5686 - age_output_acc: 0.4173 - weight_output_acc: 0.6429 - bag_output_acc: 0.6595 - pose_output_acc: 0.7875 - footwear_output_acc: 0.6215 - emotion_output_acc: 0.7079 - val_loss: 6.6605 - val_gender_output_loss: 0.3768 - val_image_quality_output_loss: 0.9785 - val_age_output_loss: 1.3646 - val_weight_output_loss: 0.9307 - val_bag_output_loss: 0.8031 - val_pose_output_loss: 0.5302 - val_footwear_output_loss: 0.8391 - val_emotion_output_loss: 0.8374 - val_gender_output_acc: 0.8306 - val_image_quality_output_acc: 0.5418 - val_age_output_acc: 0.4017 - val_weight_output_acc: 0.6406 - val_bag_output_acc: 0.6552 - val_pose_output_acc: 0.7863 - val_footwear_output_acc: 0.6094 - val_emotion_output_acc: 0.7319\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0004758206.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 6.5710 - gender_output_loss: 0.3833 - image_quality_output_loss: 0.9047 - age_output_loss: 1.3410 - weight_output_loss: 0.9263 - bag_output_loss: 0.7912 - pose_output_loss: 0.5352 - footwear_output_loss: 0.8226 - emotion_output_loss: 0.8668 - gender_output_acc: 0.8273 - image_quality_output_acc: 0.5718 - age_output_acc: 0.4140 - weight_output_acc: 0.6408 - bag_output_acc: 0.6592 - pose_output_acc: 0.7840 - footwear_output_acc: 0.6207 - emotion_output_acc: 0.7084\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0004758206.\n",
            "180/180 [==============================] - 142s 791ms/step - loss: 6.5728 - gender_output_loss: 0.3827 - image_quality_output_loss: 0.9045 - age_output_loss: 1.3417 - weight_output_loss: 0.9268 - bag_output_loss: 0.7908 - pose_output_loss: 0.5361 - footwear_output_loss: 0.8228 - emotion_output_loss: 0.8673 - gender_output_acc: 0.8277 - image_quality_output_acc: 0.5723 - age_output_acc: 0.4138 - weight_output_acc: 0.6404 - bag_output_acc: 0.6592 - pose_output_acc: 0.7839 - footwear_output_acc: 0.6207 - emotion_output_acc: 0.7082 - val_loss: 6.6409 - val_gender_output_loss: 0.3772 - val_image_quality_output_loss: 0.9795 - val_age_output_loss: 1.3661 - val_weight_output_loss: 0.9131 - val_bag_output_loss: 0.7997 - val_pose_output_loss: 0.5296 - val_footwear_output_loss: 0.8439 - val_emotion_output_loss: 0.8317 - val_gender_output_acc: 0.8377 - val_image_quality_output_acc: 0.5358 - val_age_output_acc: 0.3987 - val_weight_output_acc: 0.6462 - val_bag_output_acc: 0.6547 - val_pose_output_acc: 0.7908 - val_footwear_output_acc: 0.6094 - val_emotion_output_acc: 0.7324\n",
            "Epoch 32/100\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0003909783.\n",
            "180/180 [==============================] - 142s 791ms/step - loss: 6.5579 - gender_output_loss: 0.3815 - image_quality_output_loss: 0.9043 - age_output_loss: 1.3443 - weight_output_loss: 0.9254 - bag_output_loss: 0.7856 - pose_output_loss: 0.5269 - footwear_output_loss: 0.8212 - emotion_output_loss: 0.8687 - gender_output_acc: 0.8266 - image_quality_output_acc: 0.5708 - age_output_acc: 0.4181 - weight_output_acc: 0.6419 - bag_output_acc: 0.6594 - pose_output_acc: 0.7862 - footwear_output_acc: 0.6282 - emotion_output_acc: 0.7077 - val_loss: 6.6770 - val_gender_output_loss: 0.3806 - val_image_quality_output_loss: 0.9675 - val_age_output_loss: 1.3630 - val_weight_output_loss: 0.9203 - val_bag_output_loss: 0.8019 - val_pose_output_loss: 0.5314 - val_footwear_output_loss: 0.8803 - val_emotion_output_loss: 0.8319 - val_gender_output_acc: 0.8382 - val_image_quality_output_acc: 0.5449 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6462 - val_bag_output_acc: 0.6527 - val_pose_output_acc: 0.7868 - val_footwear_output_acc: 0.5948 - val_emotion_output_acc: 0.7324\n",
            "Epoch 33/100\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.0003194267.\n",
            "180/180 [==============================] - 143s 792ms/step - loss: 6.5593 - gender_output_loss: 0.3854 - image_quality_output_loss: 0.9017 - age_output_loss: 1.3399 - weight_output_loss: 0.9254 - bag_output_loss: 0.7890 - pose_output_loss: 0.5308 - footwear_output_loss: 0.8183 - emotion_output_loss: 0.8687 - gender_output_acc: 0.8274 - image_quality_output_acc: 0.5731 - age_output_acc: 0.4191 - weight_output_acc: 0.6414 - bag_output_acc: 0.6585 - pose_output_acc: 0.7849 - footwear_output_acc: 0.6292 - emotion_output_acc: 0.7078 - val_loss: 6.8809 - val_gender_output_loss: 0.3904 - val_image_quality_output_loss: 1.0104 - val_age_output_loss: 1.3681 - val_weight_output_loss: 0.9304 - val_bag_output_loss: 0.8035 - val_pose_output_loss: 0.5524 - val_footwear_output_loss: 0.9921 - val_emotion_output_loss: 0.8337 - val_gender_output_acc: 0.8211 - val_image_quality_output_acc: 0.5186 - val_age_output_acc: 0.4017 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.6472 - val_pose_output_acc: 0.7702 - val_footwear_output_acc: 0.5418 - val_emotion_output_acc: 0.7324\n",
            "Epoch 34/100\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002594855.\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.0003194267.\n",
            "180/180 [==============================] - 143s 794ms/step - loss: 6.5500 - gender_output_loss: 0.3808 - image_quality_output_loss: 0.9026 - age_output_loss: 1.3402 - weight_output_loss: 0.9242 - bag_output_loss: 0.7869 - pose_output_loss: 0.5282 - footwear_output_loss: 0.8204 - emotion_output_loss: 0.8667 - gender_output_acc: 0.8291 - image_quality_output_acc: 0.5714 - age_output_acc: 0.4174 - weight_output_acc: 0.6422 - bag_output_acc: 0.6592 - pose_output_acc: 0.7911 - footwear_output_acc: 0.6275 - emotion_output_acc: 0.7078 - val_loss: 6.6250 - val_gender_output_loss: 0.3727 - val_image_quality_output_loss: 0.9624 - val_age_output_loss: 1.3604 - val_weight_output_loss: 0.9183 - val_bag_output_loss: 0.8041 - val_pose_output_loss: 0.5251 - val_footwear_output_loss: 0.8506 - val_emotion_output_loss: 0.8316 - val_gender_output_acc: 0.8397 - val_image_quality_output_acc: 0.5494 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.6537 - val_pose_output_acc: 0.7863 - val_footwear_output_acc: 0.6053 - val_emotion_output_acc: 0.7324\n",
            "Epoch 34/100\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002594855.\n",
            "Epoch 35/100\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.0002096006.\n",
            "180/180 [==============================] - 143s 792ms/step - loss: 6.5431 - gender_output_loss: 0.3753 - image_quality_output_loss: 0.8994 - age_output_loss: 1.3393 - weight_output_loss: 0.9286 - bag_output_loss: 0.7877 - pose_output_loss: 0.5205 - footwear_output_loss: 0.8251 - emotion_output_loss: 0.8671 - gender_output_acc: 0.8326 - image_quality_output_acc: 0.5748 - age_output_acc: 0.4134 - weight_output_acc: 0.6395 - bag_output_acc: 0.6620 - pose_output_acc: 0.7891 - footwear_output_acc: 0.6241 - emotion_output_acc: 0.7076 - val_loss: 6.7528 - val_gender_output_loss: 0.3804 - val_image_quality_output_loss: 0.9857 - val_age_output_loss: 1.3639 - val_weight_output_loss: 0.9217 - val_bag_output_loss: 0.8019 - val_pose_output_loss: 0.5375 - val_footwear_output_loss: 0.9287 - val_emotion_output_loss: 0.8330 - val_gender_output_acc: 0.8281 - val_image_quality_output_acc: 0.5353 - val_age_output_acc: 0.4032 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.6527 - val_pose_output_acc: 0.7802 - val_footwear_output_acc: 0.5726 - val_emotion_output_acc: 0.7324\n",
            "\n",
            "Epoch 36/100\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0001683539.\n",
            "180/180 [==============================] - 143s 794ms/step - loss: 6.5147 - gender_output_loss: 0.3738 - image_quality_output_loss: 0.9027 - age_output_loss: 1.3372 - weight_output_loss: 0.9242 - bag_output_loss: 0.7761 - pose_output_loss: 0.5157 - footwear_output_loss: 0.8169 - emotion_output_loss: 0.8681 - gender_output_acc: 0.8323 - image_quality_output_acc: 0.5711 - age_output_acc: 0.4193 - weight_output_acc: 0.6419 - bag_output_acc: 0.6712 - pose_output_acc: 0.7935 - footwear_output_acc: 0.6297 - emotion_output_acc: 0.7077 - val_loss: 6.6355 - val_gender_output_loss: 0.3785 - val_image_quality_output_loss: 0.9767 - val_age_output_loss: 1.3621 - val_weight_output_loss: 0.9171 - val_bag_output_loss: 0.8016 - val_pose_output_loss: 0.5231 - val_footwear_output_loss: 0.8418 - val_emotion_output_loss: 0.8347 - val_gender_output_acc: 0.8332 - val_image_quality_output_acc: 0.5393 - val_age_output_acc: 0.4022 - val_weight_output_acc: 0.6457 - val_bag_output_acc: 0.6537 - val_pose_output_acc: 0.7873 - val_footwear_output_acc: 0.6053 - val_emotion_output_acc: 0.7324\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.000134468.\n",
            "180/180 [==============================] - 143s 794ms/step - loss: 6.5147 - gender_output_loss: 0.3738 - image_quality_output_loss: 0.9027 - age_output_loss: 1.3372 - weight_output_loss: 0.9242 - bag_output_loss: 0.7761 - pose_output_loss: 0.5157 - footwear_output_loss: 0.8169 - emotion_output_loss: 0.8681 - gender_output_acc: 0.8323 - image_quality_output_acc: 0.5711 - age_output_acc: 0.4193 - weight_output_acc: 0.6419 - bag_output_acc: 0.6712 - pose_output_acc: 0.7935 - footwear_output_acc: 0.6297 - emotion_output_acc: 0.7077 - val_loss: 6.6355 - val_gender_output_loss: 0.3785 - val_image_quality_output_loss: 0.9767 - val_age_output_loss: 1.3621 - val_weight_output_loss: 0.9171 - val_bag_output_loss: 0.8016 - val_pose_output_loss: 0.5231 - val_footwear_output_loss: 0.8418 - val_emotion_output_loss: 0.8347 - val_gender_output_acc: 0.8332 - val_image_quality_output_acc: 0.5393 - val_age_output_acc: 0.4022 - val_weight_output_acc: 0.6457 - val_bag_output_acc: 0.6537 - val_pose_output_acc: 0.7873 - val_footwear_output_acc: 0.6053 - val_emotion_output_acc: 0.7324\n",
            "180/180 [==============================] - 143s 795ms/step - loss: 6.5451 - gender_output_loss: 0.3749 - image_quality_output_loss: 0.9037 - age_output_loss: 1.3400 - weight_output_loss: 0.9244 - bag_output_loss: 0.7846 - pose_output_loss: 0.5188 - footwear_output_loss: 0.8294 - emotion_output_loss: 0.8693 - gender_output_acc: 0.8317 - image_quality_output_acc: 0.5698 - age_output_acc: 0.4174 - weight_output_acc: 0.6431 - bag_output_acc: 0.6628 - pose_output_acc: 0.7931 - footwear_output_acc: 0.6207 - emotion_output_acc: 0.7081 - val_loss: 6.6694 - val_gender_output_loss: 0.3756 - val_image_quality_output_loss: 0.9892 - val_age_output_loss: 1.3620 - val_weight_output_loss: 0.9193 - val_bag_output_loss: 0.7995 - val_pose_output_loss: 0.5229 - val_footwear_output_loss: 0.8664 - val_emotion_output_loss: 0.8344 - val_gender_output_acc: 0.8322 - val_image_quality_output_acc: 0.5323 - val_age_output_acc: 0.4088 - val_weight_output_acc: 0.6436 - val_bag_output_acc: 0.6583 - val_pose_output_acc: 0.7848 - val_footwear_output_acc: 0.5988 - val_emotion_output_acc: 0.7324\n",
            "Epoch 38/100\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0001068054.\n",
            "180/180 [==============================] - 143s 792ms/step - loss: 6.5245 - gender_output_loss: 0.3745 - image_quality_output_loss: 0.8973 - age_output_loss: 1.3391 - weight_output_loss: 0.9239 - bag_output_loss: 0.7868 - pose_output_loss: 0.5150 - footwear_output_loss: 0.8207 - emotion_output_loss: 0.8672 - gender_output_acc: 0.8313 - image_quality_output_acc: 0.5707 - age_output_acc: 0.4141 - weight_output_acc: 0.6439 - bag_output_acc: 0.6630 - pose_output_acc: 0.7977 - footwear_output_acc: 0.6247 - emotion_output_acc: 0.7078 - val_loss: 6.6879 - val_gender_output_loss: 0.3807 - val_image_quality_output_loss: 0.9853 - val_age_output_loss: 1.3619 - val_weight_output_loss: 0.9166 - val_bag_output_loss: 0.8003 - val_pose_output_loss: 0.5256 - val_footwear_output_loss: 0.8829 - val_emotion_output_loss: 0.8346 - val_gender_output_acc: 0.8317 - val_image_quality_output_acc: 0.5363 - val_age_output_acc: 0.4068 - val_weight_output_acc: 0.6472 - val_bag_output_acc: 0.6552 - val_pose_output_acc: 0.7853 - val_footwear_output_acc: 0.5953 - val_emotion_output_acc: 0.7324\n",
            "Epoch 39/100\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 8.43645e-05.\n",
            "180/180 [==============================] - 143s 794ms/step - loss: 6.5251 - gender_output_loss: 0.3769 - image_quality_output_loss: 0.9014 - age_output_loss: 1.3381 - weight_output_loss: 0.9243 - bag_output_loss: 0.7837 - pose_output_loss: 0.5175 - footwear_output_loss: 0.8163 - emotion_output_loss: 0.8669 - gender_output_acc: 0.8283 - image_quality_output_acc: 0.5752 - age_output_acc: 0.4155 - weight_output_acc: 0.6429 - bag_output_acc: 0.6654 - pose_output_acc: 0.7928 - footwear_output_acc: 0.6327 - emotion_output_acc: 0.7084 - val_loss: 6.6596 - val_gender_output_loss: 0.3748 - val_image_quality_output_loss: 0.9783 - val_age_output_loss: 1.3617 - val_weight_output_loss: 0.9170 - val_bag_output_loss: 0.8017 - val_pose_output_loss: 0.5233 - val_footwear_output_loss: 0.8699 - val_emotion_output_loss: 0.8328 - val_gender_output_acc: 0.8367 - val_image_quality_output_acc: 0.5418 - val_age_output_acc: 0.4057 - val_weight_output_acc: 0.6442 - val_bag_output_acc: 0.6492 - val_pose_output_acc: 0.7868 - val_footwear_output_acc: 0.5993 - val_emotion_output_acc: 0.7324\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 6.62722e-05.\n",
            "180/180 [==============================] - 143s 795ms/step - loss: 6.5266 - gender_output_loss: 0.3723 - image_quality_output_loss: 0.9005 - age_output_loss: 1.3390 - weight_output_loss: 0.9224 - bag_output_loss: 0.7793 - pose_output_loss: 0.5250 - footwear_output_loss: 0.8194 - emotion_output_loss: 0.8687 - gender_output_acc: 0.8324 - image_quality_output_acc: 0.5727 - age_output_acc: 0.4207 - weight_output_acc: 0.6448 - bag_output_acc: 0.6675 - pose_output_acc: 0.7898 - footwear_output_acc: 0.6235 - emotion_output_acc: 0.7078 - val_loss: 6.6698 - val_gender_output_loss: 0.3743 - val_image_quality_output_loss: 0.9847 - val_age_output_loss: 1.3616 - val_weight_output_loss: 0.9196 - val_bag_output_loss: 0.8002 - val_pose_output_loss: 0.5225 - val_footwear_output_loss: 0.8720 - val_emotion_output_loss: 0.8348 - val_gender_output_acc: 0.8397 - val_image_quality_output_acc: 0.5348 - val_age_output_acc: 0.4068 - val_weight_output_acc: 0.6436 - val_bag_output_acc: 0.6547 - val_pose_output_acc: 0.7853 - val_footwear_output_acc: 0.5978 - val_emotion_output_acc: 0.7324\n",
            "Epoch 41/100\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 5.17752e-05.\n",
            "180/180 [==============================] - 143s 793ms/step - loss: 6.5269 - gender_output_loss: 0.3761 - image_quality_output_loss: 0.8978 - age_output_loss: 1.3365 - weight_output_loss: 0.9240 - bag_output_loss: 0.7861 - pose_output_loss: 0.5225 - footwear_output_loss: 0.8149 - emotion_output_loss: 0.8689 - gender_output_acc: 0.8288 - image_quality_output_acc: 0.5766 - age_output_acc: 0.4199 - weight_output_acc: 0.6452 - bag_output_acc: 0.6632 - pose_output_acc: 0.7898 - footwear_output_acc: 0.6289 - emotion_output_acc: 0.7080 - val_loss: 6.6643 - val_gender_output_loss: 0.3757 - val_image_quality_output_loss: 0.9776 - val_age_output_loss: 1.3620 - val_weight_output_loss: 0.9162 - val_bag_output_loss: 0.8005 - val_pose_output_loss: 0.5231 - val_footwear_output_loss: 0.8764 - val_emotion_output_loss: 0.8327 - val_gender_output_acc: 0.8382 - val_image_quality_output_acc: 0.5439 - val_age_output_acc: 0.4073 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.6537 - val_pose_output_acc: 0.7858 - val_footwear_output_acc: 0.6003 - val_emotion_output_acc: 0.7324\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 5.17752e-05.\n",
            "Epoch 42/100\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 4.02294e-05.\n",
            "180/180 [==============================] - 143s 793ms/step - loss: 6.5118 - gender_output_loss: 0.3710 - image_quality_output_loss: 0.8986 - age_output_loss: 1.3367 - weight_output_loss: 0.9235 - bag_output_loss: 0.7831 - pose_output_loss: 0.5171 - footwear_output_loss: 0.8169 - emotion_output_loss: 0.8649 - gender_output_acc: 0.8335 - image_quality_output_acc: 0.5714 - age_output_acc: 0.4176 - weight_output_acc: 0.6462 - bag_output_acc: 0.6641 - pose_output_acc: 0.7930 - footwear_output_acc: 0.6333 - emotion_output_acc: 0.7080 - val_loss: 6.6457 - val_gender_output_loss: 0.3740 - val_image_quality_output_loss: 0.9754 - val_age_output_loss: 1.3612 - val_weight_output_loss: 0.9166 - val_bag_output_loss: 0.7997 - val_pose_output_loss: 0.5207 - val_footwear_output_loss: 0.8651 - val_emotion_output_loss: 0.8329 - val_gender_output_acc: 0.8352 - val_image_quality_output_acc: 0.5449 - val_age_output_acc: 0.4088 - val_weight_output_acc: 0.6431 - val_bag_output_acc: 0.6537 - val_pose_output_acc: 0.7888 - val_footwear_output_acc: 0.6013 - val_emotion_output_acc: 0.7324\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 3.10892e-05.\n",
            "180/180 [==============================] - 143s 792ms/step - loss: 6.5054 - gender_output_loss: 0.3721 - image_quality_output_loss: 0.8984 - age_output_loss: 1.3368 - weight_output_loss: 0.9216 - bag_output_loss: 0.7783 - pose_output_loss: 0.5137 - footwear_output_loss: 0.8184 - emotion_output_loss: 0.8661 - gender_output_acc: 0.8354 - image_quality_output_acc: 0.5758 - age_output_acc: 0.4203 - weight_output_acc: 0.6439 - bag_output_acc: 0.6693 - pose_output_acc: 0.7942 - footwear_output_acc: 0.6310 - emotion_output_acc: 0.7081 - val_loss: 6.6541 - val_gender_output_loss: 0.3749 - val_image_quality_output_loss: 0.9772 - val_age_output_loss: 1.3614 - val_weight_output_loss: 0.9179 - val_bag_output_loss: 0.8000 - val_pose_output_loss: 0.5211 - val_footwear_output_loss: 0.8681 - val_emotion_output_loss: 0.8335 - val_gender_output_acc: 0.8377 - val_image_quality_output_acc: 0.5408 - val_age_output_acc: 0.4083 - val_weight_output_acc: 0.6442 - val_bag_output_acc: 0.6547 - val_pose_output_acc: 0.7863 - val_footwear_output_acc: 0.6008 - val_emotion_output_acc: 0.7324\n",
            "Epoch 44/100\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 2.38964e-05.\n",
            "180/180 [==============================] - 143s 793ms/step - loss: 6.5210 - gender_output_loss: 0.3748 - image_quality_output_loss: 0.9004 - age_output_loss: 1.3372 - weight_output_loss: 0.9227 - bag_output_loss: 0.7830 - pose_output_loss: 0.5174 - footwear_output_loss: 0.8204 - emotion_output_loss: 0.8652 - gender_output_acc: 0.8321 - image_quality_output_acc: 0.5722 - age_output_acc: 0.4176 - weight_output_acc: 0.6431 - bag_output_acc: 0.6629 - pose_output_acc: 0.7944 - footwear_output_acc: 0.6248 - emotion_output_acc: 0.7081 - val_loss: 6.6546 - val_gender_output_loss: 0.3758 - val_image_quality_output_loss: 0.9786 - val_age_output_loss: 1.3616 - val_weight_output_loss: 0.9179 - val_bag_output_loss: 0.8001 - val_pose_output_loss: 0.5200 - val_footwear_output_loss: 0.8668 - val_emotion_output_loss: 0.8338 - val_gender_output_acc: 0.8387 - val_image_quality_output_acc: 0.5403 - val_age_output_acc: 0.4093 - val_weight_output_acc: 0.6457 - val_bag_output_acc: 0.6527 - val_pose_output_acc: 0.7878 - val_footwear_output_acc: 0.5993 - val_emotion_output_acc: 0.7324\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 45/100\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 1.82694e-05.\n",
            "180/180 [==============================] - 143s 793ms/step - loss: 6.5016 - gender_output_loss: 0.3701 - image_quality_output_loss: 0.8957 - age_output_loss: 1.3394 - weight_output_loss: 0.9222 - bag_output_loss: 0.7814 - pose_output_loss: 0.5083 - footwear_output_loss: 0.8165 - emotion_output_loss: 0.8680 - gender_output_acc: 0.8324 - image_quality_output_acc: 0.5764 - age_output_acc: 0.4167 - weight_output_acc: 0.6434 - bag_output_acc: 0.6683 - pose_output_acc: 0.7970 - footwear_output_acc: 0.6305 - emotion_output_acc: 0.7078 - val_loss: 6.6460 - val_gender_output_loss: 0.3740 - val_image_quality_output_loss: 0.9785 - val_age_output_loss: 1.3614 - val_weight_output_loss: 0.9178 - val_bag_output_loss: 0.7997 - val_pose_output_loss: 0.5204 - val_footwear_output_loss: 0.8607 - val_emotion_output_loss: 0.8335 - val_gender_output_acc: 0.8372 - val_image_quality_output_acc: 0.5408 - val_age_output_acc: 0.4068 - val_weight_output_acc: 0.6457 - val_bag_output_acc: 0.6512 - val_pose_output_acc: 0.7863 - val_footwear_output_acc: 0.6018 - val_emotion_output_acc: 0.7324\n",
            "Epoch 00045: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd8ac676208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9Qcz9_GQANw",
        "colab_type": "code",
        "outputId": "1717b733-abd4-438c-a536-258c8aa3bf27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "def evaluate_model(model):\n",
        "  results = model.evaluate_generator(valid_gen, verbose=1)\n",
        "  accuracies = {}\n",
        "  losses = {}\n",
        "  for k, v in zip(model.metrics_names, results):\n",
        "    if k.endswith('acc'):\n",
        "      accuracies[k] = round(v * 100, 4)\n",
        "    else:\n",
        "      losses[k] = v\n",
        "  return accuracies\n",
        "\n",
        "evaluate_model(model)\n",
        "\n",
        "# results = model.evaluate_generator(valid_gen, verbose =1)\n",
        "# dict(zip(model.metrics_names, results))\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31/31 [==============================] - 6s 183ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output_acc': 40.6754,\n",
              " 'bag_output_acc': 65.121,\n",
              " 'emotion_output_acc': 73.2359,\n",
              " 'footwear_output_acc': 60.1815,\n",
              " 'gender_output_acc': 83.7198,\n",
              " 'image_quality_output_acc': 54.0827,\n",
              " 'pose_output_acc': 78.629,\n",
              " 'weight_output_acc': 64.5665}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-nJYdSOQAGn",
        "colab_type": "code",
        "outputId": "c93daf41-af44-4bbc-f39e-fcffabacbdc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(1,1,figsize=(15,5))\n",
        "    # summarize history for accuracy\n",
        "    # axs[0].plot(range(1,len(model_history.history.history['acc'])+1),model_history.history['acc'])\n",
        "    # axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    # axs[0].set_title('Model Accuracy')\n",
        "    # axs[0].set_ylabel('Accuracy')\n",
        "    # axs[0].set_xlabel('Epoch')\n",
        "    # axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    # axs[0].legend(['train', 'val'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs.plot(range(1,len(model_history.history.history['loss'])+1),model_history.history.history['loss'])\n",
        "    axs.plot(range(1,len(model_history.history.history['val_loss'])+1),model_history.history.history['val_loss'])\n",
        "    axs.set_title('Model Loss')\n",
        "    axs.set_ylabel('Loss')\n",
        "    axs.set_xlabel('Epoch')\n",
        "    axs.set_xticks(np.arange(1,len(model_history.history.history['loss'])+1),len(model_history.history.history['loss'])/10)\n",
        "    axs.legend(['train', 'val'], loc='best')\n",
        "    plt.show()\n",
        "# plot model history\n",
        "plot_model_history(model)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFNCAYAAABSRs15AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCvHf7+xB+PCHm6wAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA7Vki4-QAQx",
        "colab_type": "code",
        "outputId": "b16aa630-0663-4ab6-d45d-1923d80a7e62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "  return round(lr * 1/(1 + decay_factor * epoch), 10)\n",
        "\n",
        "lr = 0.004\n",
        "decay_factor = 0.001\n",
        "epoch = 100\n",
        "for i in range(epoch):\n",
        "  lr_new = scheduler(i, lr)\n",
        "  lr = lr_new\n",
        "  if i%5 == 0:\n",
        "    print(\"the epoch number is: \" + str(i) + \" and LR is: \" + str(round(lr,10)))\n",
        "  i = i+1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the epoch number is: 0 and LR is: 0.004\n",
            "the epoch number is: 5 and LR is: 0.0039405558\n",
            "the epoch number is: 10 and LR is: 0.0037866656\n",
            "the epoch number is: 15 and LR is: 0.0035498651\n",
            "the epoch number is: 20 and LR is: 0.0032469459\n",
            "the epoch number is: 25 and LR is: 0.0028980039\n",
            "the epoch number is: 30 and LR is: 0.0025242679\n",
            "the epoch number is: 35 and LR is: 0.0021460304\n",
            "the epoch number is: 40 and LR is: 0.0017809475\n",
            "the epoch number is: 45 and LR is: 0.0014428845\n",
            "the epoch number is: 50 and LR is: 0.0011413717\n",
            "the epoch number is: 55 and LR is: 0.0008816318\n",
            "the epoch number is: 60 and LR is: 0.00066506\n",
            "the epoch number is: 65 and LR is: 0.0004900005\n",
            "the epoch number is: 70 and LR is: 0.0003526487\n",
            "the epoch number is: 75 and LR is: 0.0002479393\n",
            "the epoch number is: 80 and LR is: 0.0001703153\n",
            "the epoch number is: 85 and LR is: 0.0001143177\n",
            "the epoch number is: 90 and LR is: 7.49844e-05\n",
            "the epoch number is: 95 and LR is: 4.80699e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1WB81PaN2ug",
        "colab_type": "code",
        "outputId": "48e431ff-9dc6-48ca-b5db-60ce4a3cf7da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.17.4)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.2.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.3.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=2a502271871d15a28a4911679cc2be4a7a950b473d199d51b8e52516e0a749fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-0.0.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgYCMe1chjKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.callbacks import Callback\n",
        "from seqeval.metrics import f1_score, classification_report\n",
        "\n",
        "\n",
        "class F1Metrics(Callback):\n",
        "\n",
        "    def __init__(self, id2label, pad_value=0, validation_data=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            id2label (dict): id to label mapping.\n",
        "            (e.g. {1: 'B-LOC', 2: 'I-LOC'})\n",
        "            pad_value (int): padding value.\n",
        "        \"\"\"\n",
        "        super(F1Metrics, self).__init__()\n",
        "        self.id2label = id2label\n",
        "        self.pad_value = pad_value\n",
        "        self.validation_data = validation_data\n",
        "        self.is_fit = validation_data is None\n",
        "\n",
        "    def find_pad_index(self, array):\n",
        "        \"\"\"Find padding index.\n",
        "        Args:\n",
        "            array (list): integer list.\n",
        "        Returns:\n",
        "            idx: padding index.\n",
        "        Examples:\n",
        "             >>> array = [1, 2, 0]\n",
        "             >>> self.find_pad_index(array)\n",
        "             2\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return list(array).index(self.pad_value)\n",
        "        except ValueError:\n",
        "            return len(array)\n",
        "\n",
        "    def get_length(self, y):\n",
        "        \"\"\"Get true length of y.\n",
        "        Args:\n",
        "            y (list): padded list.\n",
        "        Returns:\n",
        "            lens: true length of y.\n",
        "        Examples:\n",
        "            >>> y = [[1, 0, 0], [1, 1, 0], [1, 1, 1]]\n",
        "            >>> self.get_length(y)\n",
        "            [1, 2, 3]\n",
        "        \"\"\"\n",
        "        lens = [self.find_pad_index(row) for row in y]\n",
        "        return lens\n",
        "\n",
        "    def convert_idx_to_name(self, y, lens):\n",
        "        \"\"\"Convert label index to name.\n",
        "        Args:\n",
        "            y (list): label index list.\n",
        "            lens (list): true length of y.\n",
        "        Returns:\n",
        "            y: label name list.\n",
        "        Examples:\n",
        "            >>> # assumes that id2label = {1: 'B-LOC', 2: 'I-LOC'}\n",
        "            >>> y = [[1, 0, 0], [1, 2, 0], [1, 1, 1]]\n",
        "            >>> lens = [1, 2, 3]\n",
        "            >>> self.convert_idx_to_name(y, lens)\n",
        "            [['B-LOC'], ['B-LOC', 'I-LOC'], ['B-LOC', 'B-LOC', 'B-LOC']]\n",
        "        \"\"\"\n",
        "        y = [[self.id2label[idx] for idx in row[:l]]\n",
        "             for row, l in zip(y, lens)]\n",
        "        return y\n",
        "\n",
        "    def predict(self, X, y):\n",
        "        \"\"\"Predict sequences.\n",
        "        Args:\n",
        "            X (list): input data.\n",
        "            y (list): tags.\n",
        "        Returns:\n",
        "            y_true: true sequences.\n",
        "            y_pred: predicted sequences.\n",
        "        \"\"\"\n",
        "        y_pred = self.model.predict_on_batch(X)\n",
        "\n",
        "        # reduce dimension.\n",
        "        y_true = np.argmax(y, -1)\n",
        "        y_pred = np.argmax(y_pred, -1)\n",
        "\n",
        "        lens = self.get_length(y_true)\n",
        "\n",
        "        y_true = self.convert_idx_to_name(y_true, lens)\n",
        "        y_pred = self.convert_idx_to_name(y_pred, lens)\n",
        "\n",
        "        return y_true, y_pred\n",
        "\n",
        "    def score(self, y_true, y_pred):\n",
        "        \"\"\"Calculate f1 score.\n",
        "        Args:\n",
        "            y_true (list): true sequences.\n",
        "            y_pred (list): predicted sequences.\n",
        "        Returns:\n",
        "            score: f1 score.\n",
        "        \"\"\"\n",
        "        score = f1_score(y_true, y_pred)\n",
        "        print(' - f1: {:04.2f}'.format(score * 100))\n",
        "        print(classification_report(y_true, y_pred, digits=4))\n",
        "        return score\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if self.is_fit:\n",
        "            self.on_epoch_end_fit(epoch, logs)\n",
        "        else:\n",
        "            self.on_epoch_end_fit_generator(epoch, logs)\n",
        "\n",
        "    def on_epoch_end_fit(self, epoch, logs={}):\n",
        "        X = self.validation_data[0]\n",
        "        y = self.validation_data[1]\n",
        "        y_true, y_pred = self.predict(X, y)\n",
        "        score = self.score(y_true, y_pred)\n",
        "        logs['f1'] = score\n",
        "\n",
        "    def on_epoch_end_fit_generator(self, epoch, logs={}):\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        for X, y in self.validation_data:\n",
        "            y_true_batch, y_pred_batch = self.predict(X, y)\n",
        "            y_true.extend(y_true_batch)\n",
        "            y_pred.extend(y_pred_batch)\n",
        "        score = self.score(y_true, y_pred)\n",
        "        logs['f1'] = score"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}