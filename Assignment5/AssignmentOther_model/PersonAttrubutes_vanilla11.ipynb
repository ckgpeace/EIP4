{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PersonAttrubutes_vanilla11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ckgpeace/EIP4/blob/master/Assignment5/AssignmentOther_model/PersonAttrubutes_vanilla11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El-Dopn8pLpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "63fe8c77-c0e3-48d0-bf6a-e0fe4fabc40a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "outputId": "1fd2f339-dbe3-4d08-df5e-0f963f1263bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "from keras.layers import Activation, Flatten, Dense, Dropout, GlobalAveragePooling2D, AveragePooling2D\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, SeparableConv2D\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input, BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "08dec34b-9adf-4a98-871e-082865609e9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "98ae3ab5-2595-4a0d-ca52-df1d06577316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=64, shuffle=True, augmentation = None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.resize(cv2.imread(item[\"image_path\"]), (112,112)) for _, item in items.iterrows()])\n",
        "        #Image Normalization\n",
        "        if self.augmentation is not None:\n",
        "          self.augmentation.fit(image)\n",
        "          image = self.augmentation.flow(image,shuffle=False, batch_size = 64 ).next()\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "15c17d8f-984d-4f65-f2dd-d79db266c279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15, random_state = 404)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "0906263a-501f-4ba7-db91-271bd3a71935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10980</th>\n",
              "      <td>resized/10982.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11135</th>\n",
              "      <td>resized/11137.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1784</th>\n",
              "      <td>resized/1785.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7758</th>\n",
              "      <td>resized/7759.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4692</th>\n",
              "      <td>resized/4693.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "10980  resized/10982.jpg              0  ...                        1              0\n",
              "11135  resized/11137.jpg              0  ...                        0              1\n",
              "1784    resized/1785.jpg              0  ...                        1              0\n",
              "7758    resized/7759.jpg              0  ...                        0              1\n",
              "4692    resized/4693.jpg              0  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=64, shuffle=True, \n",
        "                                augmentation = ImageDataGenerator(horizontal_flip=True, \n",
        "                                                                  width_shift_range=0.2,\n",
        "                                                                  height_shift_range=0.2,\n",
        "                                                                  rotation_range=0,\n",
        "                                                                  zoom_range=0.2,\n",
        "                                                                  featurewise_center=True, featurewise_std_normalization=True))\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False, augmentation= ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "bc42c16b-b9bd-47c3-fe82-876a812ba0f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "outputId": "7bfd89e0-cb54-4c23-c3a0-92ab78f2939d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "inp = Input(shape = (112,112,3))\n",
        "x = inp\n",
        "### block 1\n",
        "\n",
        "x = SeparableConv2D(filters=32, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=32, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=64, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=64, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# Pooling \n",
        "x = MaxPooling2D()(x)\n",
        "\n",
        "# ================================================================\n",
        "\n",
        "### block\n",
        "\n",
        "x = SeparableConv2D(filters=128, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=128, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=128, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=128, kernel_size=(3, 3), padding='valid')(x) \n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# Pooling \n",
        "x = MaxPooling2D()(x)\n",
        "\n",
        "# ================================================================\n",
        "\n",
        "### block 2 \n",
        "x = SeparableConv2D(filters=256, kernel_size=(3, 3), padding='valid')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=256, kernel_size=(3, 3), padding='valid')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.05)(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=256, kernel_size=(3, 3), padding='valid')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.05)(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# Pooling \n",
        "x = MaxPooling2D()(x)\n",
        "\n",
        "# ================================================================\n",
        "# block  - Last CNN\n",
        "\n",
        "x = SeparableConv2D(filters=512, kernel_size=(3, 3), padding='valid')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = SeparableConv2D(filters=512, kernel_size=(3, 3), padding='valid')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "\n",
        "\n",
        "# Adding Dense layer\n",
        "def final(in_layer, num_units, class_name, output_name):\n",
        "  # Conv with class size\n",
        "  x = SeparableConv2D(filters=num_units[class_name], kernel_size=(1, 1), padding='valid')(in_layer)\n",
        "  # GAP\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  x = Activation('softmax', name = output_name)(x)\n",
        "  return x\n",
        "\n",
        "gender = final(in_layer = x, num_units = num_units,  class_name = \"gender\", output_name = \"gender_output\")\n",
        "image_quality = final(in_layer = x, num_units = num_units,  class_name = \"image_quality\", output_name = \"image_quality_output\")\n",
        "age = final(in_layer = x, num_units = num_units,  class_name = \"age\", output_name = \"age_output\")\n",
        "weight = final(in_layer = x, num_units = num_units,  class_name = \"weight\", output_name = \"weight_output\")\n",
        "bag = final(in_layer = x, num_units = num_units,  class_name = \"bag\", output_name = \"bag_output\")\n",
        "footwear = final(in_layer = x, num_units = num_units,  class_name = \"footwear\", output_name = \"footwear_output\")\n",
        "emotion = final(in_layer = x, num_units = num_units,  class_name = \"emotion\", output_name = \"emotion_output\")\n",
        "pose = final(in_layer = x, num_units = num_units,  class_name = \"pose\", output_name = \"pose_output\")\n",
        "\n",
        "model = Model(inputs = inp,outputs=[gender, image_quality, age, weight, bag, pose, footwear, emotion])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_13 (InputLayer)           (None, 112, 112, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_240 (Separable (None, 110, 110, 32) 155         input_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_144 (Activation)     (None, 110, 110, 32) 0           separable_conv2d_240[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_144 (BatchN (None, 110, 110, 32) 128         activation_144[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_241 (Separable (None, 108, 108, 32) 1344        batch_normalization_144[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_145 (Activation)     (None, 108, 108, 32) 0           separable_conv2d_241[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_145 (BatchN (None, 108, 108, 32) 128         activation_145[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_242 (Separable (None, 106, 106, 64) 2400        batch_normalization_145[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_146 (Activation)     (None, 106, 106, 64) 0           separable_conv2d_242[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_146 (BatchN (None, 106, 106, 64) 256         activation_146[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_243 (Separable (None, 104, 104, 64) 4736        batch_normalization_146[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_147 (Activation)     (None, 104, 104, 64) 0           separable_conv2d_243[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_147 (BatchN (None, 104, 104, 64) 256         activation_147[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling2D) (None, 52, 52, 64)   0           batch_normalization_147[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_244 (Separable (None, 50, 50, 128)  8896        max_pooling2d_16[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_148 (Activation)     (None, 50, 50, 128)  0           separable_conv2d_244[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_148 (BatchN (None, 50, 50, 128)  512         activation_148[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_245 (Separable (None, 48, 48, 128)  17664       batch_normalization_148[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_149 (Activation)     (None, 48, 48, 128)  0           separable_conv2d_245[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_149 (BatchN (None, 48, 48, 128)  512         activation_149[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_246 (Separable (None, 46, 46, 128)  17664       batch_normalization_149[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_150 (Activation)     (None, 46, 46, 128)  0           separable_conv2d_246[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_150 (BatchN (None, 46, 46, 128)  512         activation_150[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_247 (Separable (None, 44, 44, 128)  17664       batch_normalization_150[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_151 (Activation)     (None, 44, 44, 128)  0           separable_conv2d_247[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_151 (BatchN (None, 44, 44, 128)  512         activation_151[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling2D) (None, 22, 22, 128)  0           batch_normalization_151[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_248 (Separable (None, 20, 20, 256)  34176       max_pooling2d_17[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_152 (Activation)     (None, 20, 20, 256)  0           separable_conv2d_248[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_152 (BatchN (None, 20, 20, 256)  1024        activation_152[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_249 (Separable (None, 18, 18, 256)  68096       batch_normalization_152[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_153 (Activation)     (None, 18, 18, 256)  0           separable_conv2d_249[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 18, 18, 256)  0           activation_153[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_153 (BatchN (None, 18, 18, 256)  1024        dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_250 (Separable (None, 16, 16, 256)  68096       batch_normalization_153[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_154 (Activation)     (None, 16, 16, 256)  0           separable_conv2d_250[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 16, 16, 256)  0           activation_154[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_154 (BatchN (None, 16, 16, 256)  1024        dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling2D) (None, 8, 8, 256)    0           batch_normalization_154[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_251 (Separable (None, 6, 6, 512)    133888      max_pooling2d_18[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, 6, 6, 512)    0           separable_conv2d_251[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_155 (BatchN (None, 6, 6, 512)    2048        activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_252 (Separable (None, 4, 4, 512)    267264      batch_normalization_155[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, 4, 4, 512)    0           separable_conv2d_252[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_156 (BatchN (None, 4, 4, 512)    2048        activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_253 (Separable (None, 4, 4, 2)      1538        batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_254 (Separable (None, 4, 4, 3)      2051        batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_255 (Separable (None, 4, 4, 5)      3077        batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_256 (Separable (None, 4, 4, 4)      2564        batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_257 (Separable (None, 4, 4, 3)      2051        batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_260 (Separable (None, 4, 4, 3)      2051        batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_258 (Separable (None, 4, 4, 3)      2051        batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_259 (Separable (None, 4, 4, 4)      2564        batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_97 (Gl (None, 2)            0           separable_conv2d_253[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_98 (Gl (None, 3)            0           separable_conv2d_254[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_99 (Gl (None, 5)            0           separable_conv2d_255[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_100 (G (None, 4)            0           separable_conv2d_256[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_101 (G (None, 3)            0           separable_conv2d_257[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_104 (G (None, 3)            0           separable_conv2d_260[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_102 (G (None, 3)            0           separable_conv2d_258[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_103 (G (None, 4)            0           separable_conv2d_259[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Activation)      (None, 2)            0           global_average_pooling2d_97[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "image_quality_output (Activatio (None, 3)            0           global_average_pooling2d_98[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "age_output (Activation)         (None, 5)            0           global_average_pooling2d_99[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "weight_output (Activation)      (None, 4)            0           global_average_pooling2d_100[0][0\n",
            "__________________________________________________________________________________________________\n",
            "bag_output (Activation)         (None, 3)            0           global_average_pooling2d_101[0][0\n",
            "__________________________________________________________________________________________________\n",
            "pose_output (Activation)        (None, 3)            0           global_average_pooling2d_104[0][0\n",
            "__________________________________________________________________________________________________\n",
            "footwear_output (Activation)    (None, 3)            0           global_average_pooling2d_102[0][0\n",
            "__________________________________________________________________________________________________\n",
            "emotion_output (Activation)     (None, 4)            0           global_average_pooling2d_103[0][0\n",
            "==================================================================================================\n",
            "Total params: 669,974\n",
            "Trainable params: 664,982\n",
            "Non-trainable params: 4,992\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkqMc7GkN2sl",
        "colab_type": "code",
        "outputId": "96856b19-2fa9-42e3-81c5-49a77959bf82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# from keras.utils import plot_model\n",
        "# plot_model(model)\n",
        "import time, psutil\n",
        "uptime = time.time() - psutil.boot_time()\n",
        "remain = 12*60*60 - uptime\n",
        "remain/(60*60)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.497417849898338"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1dcoduDN2pd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses = {\n",
        "\t\"gender_output\": \"categorical_crossentropy\",\n",
        "\t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "\t\"age_output\": \"categorical_crossentropy\",\n",
        "\t\"weight_output\": \"categorical_crossentropy\",\n",
        "  \"bag_output\":  \"categorical_crossentropy\",\n",
        "  \"pose_output\": \"categorical_crossentropy\",\n",
        "  \"footwear_output\": \"categorical_crossentropy\",\n",
        "  \"emotion_output\": \"categorical_crossentropy\"\n",
        "}\n",
        "\n",
        "loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0, \"weight_output\" :1.0,  \"bag_output\": 1.0, \"pose_output\": 1.0,  \"footwear_output\": 1.0, \"emotion_output\": 1.0 }\n",
        "\n",
        "\n",
        "\n",
        "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping\n",
        "\n",
        "decay_factor =  0.001075\n",
        "def scheduler(epoch, lr):\n",
        "  return round(lr * 1/(1 + decay_factor * epoch), 10)\n",
        "\n",
        "# sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "opt = SGD(lr = 0.01, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile( optimizer=opt, loss = losses, loss_weights=loss_weights, metrics=[\"accuracy\"])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyOyW5EOQAEJ",
        "colab_type": "code",
        "outputId": "310e4e99-a2cc-4791-98ff-702604faa3dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# model_path = '/content/gdrive/VGG16_vanila_1.h5'\n",
        "# checkpoint = ModelCheckpoint(model_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='max')\n",
        "\n",
        "import os\n",
        "# Checkpoint saving\n",
        "save_dir = os.path.join(os.getcwd(), \"/gdrive/My\\\\Drive/saved_models/\")\n",
        "model_name = \"model.{epoch:03d}.h5\"\n",
        "if not os.path.isdir(save_dir):\n",
        "  os.makedirs(save_dir)\n",
        "\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "\n",
        "checkpoint = ModelCheckpoint(model_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False)\n",
        "\n",
        "early = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=11, verbose=1, mode='min')\n",
        "\n",
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=100,\n",
        "    verbose=1,\n",
        "    callbacks=[checkpoint, early, LearningRateScheduler(scheduler, verbose=1)]\n",
        "    # callbacks=[checkpoint, early, LearningRateScheduler(scheduler, verbose=1),TensorBoardColabCallback(tbc)]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.0099999998.\n",
            "180/180 [==============================] - 124s 687ms/step - loss: 8.0005 - gender_output_loss: 0.6729 - image_quality_output_loss: 0.9893 - age_output_loss: 1.4512 - weight_output_loss: 1.0343 - bag_output_loss: 0.9324 - pose_output_loss: 0.9390 - footwear_output_loss: 1.0124 - emotion_output_loss: 0.9691 - gender_output_acc: 0.5773 - image_quality_output_acc: 0.5488 - age_output_acc: 0.3932 - weight_output_acc: 0.6286 - bag_output_acc: 0.5624 - pose_output_acc: 0.6175 - footwear_output_acc: 0.5003 - emotion_output_acc: 0.7022 - val_loss: 8.3836 - val_gender_output_loss: 0.9957 - val_image_quality_output_loss: 1.0893 - val_age_output_loss: 1.4954 - val_weight_output_loss: 0.9972 - val_bag_output_loss: 0.9570 - val_pose_output_loss: 0.9751 - val_footwear_output_loss: 0.9959 - val_emotion_output_loss: 0.8780 - val_gender_output_acc: 0.5514 - val_image_quality_output_acc: 0.5433 - val_age_output_acc: 0.3700 - val_weight_output_acc: 0.6210 - val_bag_output_acc: 0.5418 - val_pose_output_acc: 0.5958 - val_footwear_output_acc: 0.5464 - val_emotion_output_acc: 0.7329\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/100\n",
            "Epoch 1/100\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0099892613.\n",
            "180/180 [==============================] - 105s 583ms/step - loss: 7.6905 - gender_output_loss: 0.6516 - image_quality_output_loss: 0.9659 - age_output_loss: 1.4095 - weight_output_loss: 0.9766 - bag_output_loss: 0.8972 - pose_output_loss: 0.9138 - footwear_output_loss: 0.9656 - emotion_output_loss: 0.9103 - gender_output_acc: 0.6079 - image_quality_output_acc: 0.5532 - age_output_acc: 0.3994 - weight_output_acc: 0.6345 - bag_output_acc: 0.5692 - pose_output_acc: 0.6218 - footwear_output_acc: 0.5387 - emotion_output_acc: 0.7077 - val_loss: 9.7040 - val_gender_output_loss: 1.3749 - val_image_quality_output_loss: 1.4332 - val_age_output_loss: 1.5208 - val_weight_output_loss: 0.9741 - val_bag_output_loss: 0.9606 - val_pose_output_loss: 1.1637 - val_footwear_output_loss: 1.3728 - val_emotion_output_loss: 0.9039 - val_gender_output_acc: 0.5504 - val_image_quality_output_acc: 0.3165 - val_age_output_acc: 0.3684 - val_weight_output_acc: 0.6416 - val_bag_output_acc: 0.5665 - val_pose_output_acc: 0.5958 - val_footwear_output_acc: 0.4526 - val_emotion_output_acc: 0.7314\n",
            "Epoch 3/100\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0099678308.\n",
            "180/180 [==============================] - 104s 580ms/step - loss: 7.6213 - gender_output_loss: 0.6348 - image_quality_output_loss: 0.9586 - age_output_loss: 1.4056 - weight_output_loss: 0.9739 - bag_output_loss: 0.8911 - pose_output_loss: 0.9036 - footwear_output_loss: 0.9465 - emotion_output_loss: 0.9073 - gender_output_acc: 0.6274 - image_quality_output_acc: 0.5538 - age_output_acc: 0.3985 - weight_output_acc: 0.6344 - bag_output_acc: 0.5767 - pose_output_acc: 0.6221 - footwear_output_acc: 0.5534 - emotion_output_acc: 0.7079 - val_loss: 10.4098 - val_gender_output_loss: 1.6596 - val_image_quality_output_loss: 1.4565 - val_age_output_loss: 1.6038 - val_weight_output_loss: 1.0183 - val_bag_output_loss: 1.1966 - val_pose_output_loss: 1.2866 - val_footwear_output_loss: 1.2222 - val_emotion_output_loss: 0.9662 - val_gender_output_acc: 0.5534 - val_image_quality_output_acc: 0.4481 - val_age_output_acc: 0.2903 - val_weight_output_acc: 0.6190 - val_bag_output_acc: 0.5454 - val_pose_output_acc: 0.5917 - val_footwear_output_acc: 0.5287 - val_emotion_output_acc: 0.7258\n",
            "Epoch 4/100\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.009935788.\n",
            "Epoch 3/100\n",
            "180/180 [==============================] - 104s 580ms/step - loss: 7.5366 - gender_output_loss: 0.6086 - image_quality_output_loss: 0.9453 - age_output_loss: 1.4003 - weight_output_loss: 0.9687 - bag_output_loss: 0.8831 - pose_output_loss: 0.8912 - footwear_output_loss: 0.9335 - emotion_output_loss: 0.9059 - gender_output_acc: 0.6621 - image_quality_output_acc: 0.5518 - age_output_acc: 0.3989 - weight_output_acc: 0.6347 - bag_output_acc: 0.5841 - pose_output_acc: 0.6220 - footwear_output_acc: 0.5598 - emotion_output_acc: 0.7081 - val_loss: 8.6735 - val_gender_output_loss: 0.9034 - val_image_quality_output_loss: 1.2157 - val_age_output_loss: 1.4912 - val_weight_output_loss: 0.9669 - val_bag_output_loss: 0.9074 - val_pose_output_loss: 1.0422 - val_footwear_output_loss: 1.2601 - val_emotion_output_loss: 0.8867 - val_gender_output_acc: 0.5398 - val_image_quality_output_acc: 0.3684 - val_age_output_acc: 0.3891 - val_weight_output_acc: 0.6346 - val_bag_output_acc: 0.5660 - val_pose_output_acc: 0.5963 - val_footwear_output_acc: 0.4209 - val_emotion_output_acc: 0.7324\n",
            "Epoch 5/100\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0098932469.\n",
            "180/180 [==============================] - 105s 583ms/step - loss: 7.4731 - gender_output_loss: 0.5909 - image_quality_output_loss: 0.9377 - age_output_loss: 1.3975 - weight_output_loss: 0.9694 - bag_output_loss: 0.8777 - pose_output_loss: 0.8755 - footwear_output_loss: 0.9218 - emotion_output_loss: 0.9027 - gender_output_acc: 0.6727 - image_quality_output_acc: 0.5566 - age_output_acc: 0.4031 - weight_output_acc: 0.6347 - bag_output_acc: 0.5840 - pose_output_acc: 0.6224 - footwear_output_acc: 0.5640 - emotion_output_acc: 0.7078 - val_loss: 8.2741 - val_gender_output_loss: 0.8073 - val_image_quality_output_loss: 1.1521 - val_age_output_loss: 1.4360 - val_weight_output_loss: 0.9806 - val_bag_output_loss: 0.9016 - val_pose_output_loss: 1.0531 - val_footwear_output_loss: 1.0557 - val_emotion_output_loss: 0.8878 - val_gender_output_acc: 0.6129 - val_image_quality_output_acc: 0.4320 - val_age_output_acc: 0.3690 - val_weight_output_acc: 0.6250 - val_bag_output_acc: 0.5766 - val_pose_output_acc: 0.6013 - val_footwear_output_acc: 0.5232 - val_emotion_output_acc: 0.7283\n",
            "Epoch 6/100\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.009840355.\n",
            "180/180 [==============================] - 105s 583ms/step - loss: 7.4040 - gender_output_loss: 0.5709 - image_quality_output_loss: 0.9285 - age_output_loss: 1.3944 - weight_output_loss: 0.9693 - bag_output_loss: 0.8719 - pose_output_loss: 0.8510 - footwear_output_loss: 0.9165 - emotion_output_loss: 0.9015 - gender_output_acc: 0.6964 - image_quality_output_acc: 0.5590 - age_output_acc: 0.4013 - weight_output_acc: 0.6343 - bag_output_acc: 0.5883 - pose_output_acc: 0.6241 - footwear_output_acc: 0.5672 - emotion_output_acc: 0.7079 - val_loss: 8.0132 - val_gender_output_loss: 0.7279 - val_image_quality_output_loss: 1.1500 - val_age_output_loss: 1.4212 - val_weight_output_loss: 0.9891 - val_bag_output_loss: 0.9327 - val_pose_output_loss: 0.9425 - val_footwear_output_loss: 0.9702 - val_emotion_output_loss: 0.8797 - val_gender_output_acc: 0.6507 - val_image_quality_output_acc: 0.4526 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6154 - val_bag_output_acc: 0.5736 - val_pose_output_acc: 0.6109 - val_footwear_output_acc: 0.5842 - val_emotion_output_acc: 0.7314\n",
            "Epoch 7/100\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0097772917.\n",
            "180/180 [==============================] - 105s 581ms/step - loss: 7.3368 - gender_output_loss: 0.5555 - image_quality_output_loss: 0.9288 - age_output_loss: 1.3921 - weight_output_loss: 0.9656 - bag_output_loss: 0.8720 - pose_output_loss: 0.8161 - footwear_output_loss: 0.9084 - emotion_output_loss: 0.8982 - gender_output_acc: 0.7052 - image_quality_output_acc: 0.5565 - age_output_acc: 0.4014 - weight_output_acc: 0.6352 - bag_output_acc: 0.5886 - pose_output_acc: 0.6376 - footwear_output_acc: 0.5727 - emotion_output_acc: 0.7081 - val_loss: 9.3199 - val_gender_output_loss: 0.8434 - val_image_quality_output_loss: 1.5396 - val_age_output_loss: 1.5105 - val_weight_output_loss: 0.9815 - val_bag_output_loss: 1.0222 - val_pose_output_loss: 1.3402 - val_footwear_output_loss: 1.1758 - val_emotion_output_loss: 0.9066 - val_gender_output_acc: 0.6048 - val_image_quality_output_acc: 0.2949 - val_age_output_acc: 0.3936 - val_weight_output_acc: 0.6406 - val_bag_output_acc: 0.4728 - val_pose_output_acc: 0.6205 - val_footwear_output_acc: 0.4602 - val_emotion_output_acc: 0.7283\n",
            "Epoch 8/100\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0097042671.\n",
            "\n",
            "\n",
            "180/180 [==============================] - 105s 584ms/step - loss: 7.2461 - gender_output_loss: 0.5374 - image_quality_output_loss: 0.9214 - age_output_loss: 1.3882 - weight_output_loss: 0.9644 - bag_output_loss: 0.8638 - pose_output_loss: 0.7787 - footwear_output_loss: 0.8980 - emotion_output_loss: 0.8942 - gender_output_acc: 0.7230 - image_quality_output_acc: 0.5618 - age_output_acc: 0.4036 - weight_output_acc: 0.6345 - bag_output_acc: 0.5989 - pose_output_acc: 0.6583 - footwear_output_acc: 0.5844 - emotion_output_acc: 0.7079 - val_loss: 7.9400 - val_gender_output_loss: 0.5605 - val_image_quality_output_loss: 1.3424 - val_age_output_loss: 1.4124 - val_weight_output_loss: 0.9765 - val_bag_output_loss: 0.8637 - val_pose_output_loss: 0.9378 - val_footwear_output_loss: 0.9602 - val_emotion_output_loss: 0.8866 - val_gender_output_acc: 0.7379 - val_image_quality_output_acc: 0.3528 - val_age_output_acc: 0.3710 - val_weight_output_acc: 0.6139 - val_bag_output_acc: 0.6134 - val_pose_output_acc: 0.6547 - val_footwear_output_acc: 0.5685 - val_emotion_output_acc: 0.7303\n",
            "Epoch 9/100\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0096215216.\n",
            "180/180 [==============================] - 105s 582ms/step - loss: 7.1943 - gender_output_loss: 0.5299 - image_quality_output_loss: 0.9197 - age_output_loss: 1.3864 - weight_output_loss: 0.9633 - bag_output_loss: 0.8620 - pose_output_loss: 0.7488 - footwear_output_loss: 0.8896 - emotion_output_loss: 0.8946 - gender_output_acc: 0.7286 - image_quality_output_acc: 0.5621 - age_output_acc: 0.4017 - weight_output_acc: 0.6340 - bag_output_acc: 0.5983 - pose_output_acc: 0.6729 - footwear_output_acc: 0.5816 - emotion_output_acc: 0.7079 - val_loss: 8.4219 - val_gender_output_loss: 0.8618 - val_image_quality_output_loss: 1.0846 - val_age_output_loss: 1.4423 - val_weight_output_loss: 1.0746 - val_bag_output_loss: 0.9863 - val_pose_output_loss: 1.1130 - val_footwear_output_loss: 0.9251 - val_emotion_output_loss: 0.9342 - val_gender_output_acc: 0.6578 - val_image_quality_output_acc: 0.5212 - val_age_output_acc: 0.3579 - val_weight_output_acc: 0.5212 - val_bag_output_acc: 0.5766 - val_pose_output_acc: 0.6492 - val_footwear_output_acc: 0.5811 - val_emotion_output_acc: 0.6935\n",
            "Epoch 10/100\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0095293252.\n",
            "180/180 [==============================] - 105s 583ms/step - loss: 7.1195 - gender_output_loss: 0.5048 - image_quality_output_loss: 0.9176 - age_output_loss: 1.3866 - weight_output_loss: 0.9636 - bag_output_loss: 0.8509 - pose_output_loss: 0.7214 - footwear_output_loss: 0.8867 - emotion_output_loss: 0.8878 - gender_output_acc: 0.7451 - image_quality_output_acc: 0.5617 - age_output_acc: 0.4036 - weight_output_acc: 0.6351 - bag_output_acc: 0.6087 - pose_output_acc: 0.6839 - footwear_output_acc: 0.5887 - emotion_output_acc: 0.7081 - val_loss: 7.4882 - val_gender_output_loss: 0.6336 - val_image_quality_output_loss: 1.0695 - val_age_output_loss: 1.4103 - val_weight_output_loss: 0.9583 - val_bag_output_loss: 0.8634 - val_pose_output_loss: 0.7094 - val_footwear_output_loss: 0.9906 - val_emotion_output_loss: 0.8530 - val_gender_output_acc: 0.7097 - val_image_quality_output_acc: 0.4592 - val_age_output_acc: 0.3881 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.6169 - val_pose_output_acc: 0.7077 - val_footwear_output_acc: 0.5635 - val_emotion_output_acc: 0.7324\n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0094279745.\n",
            "180/180 [==============================] - 105s 584ms/step - loss: 7.0731 - gender_output_loss: 0.4992 - image_quality_output_loss: 0.9143 - age_output_loss: 1.3826 - weight_output_loss: 0.9606 - bag_output_loss: 0.8471 - pose_output_loss: 0.6982 - footwear_output_loss: 0.8825 - emotion_output_loss: 0.8887 - gender_output_acc: 0.7546 - image_quality_output_acc: 0.5666 - age_output_acc: 0.4043 - weight_output_acc: 0.6358 - bag_output_acc: 0.6143 - pose_output_acc: 0.6990 - footwear_output_acc: 0.5901 - emotion_output_acc: 0.7078 - val_loss: 7.3986 - val_gender_output_loss: 0.4818 - val_image_quality_output_loss: 1.1329 - val_age_output_loss: 1.4350 - val_weight_output_loss: 0.9587 - val_bag_output_loss: 0.8385 - val_pose_output_loss: 0.7779 - val_footwear_output_loss: 0.9066 - val_emotion_output_loss: 0.8671 - val_gender_output_acc: 0.7722 - val_image_quality_output_acc: 0.4914 - val_age_output_acc: 0.3327 - val_weight_output_acc: 0.6371 - val_bag_output_acc: 0.6401 - val_pose_output_acc: 0.7077 - val_footwear_output_acc: 0.5791 - val_emotion_output_acc: 0.7303\n",
            "Epoch 12/100\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.009317792.\n",
            "180/180 [==============================] - 105s 581ms/step - loss: 7.0202 - gender_output_loss: 0.4878 - image_quality_output_loss: 0.9128 - age_output_loss: 1.3793 - weight_output_loss: 0.9593 - bag_output_loss: 0.8426 - pose_output_loss: 0.6782 - footwear_output_loss: 0.8746 - emotion_output_loss: 0.8856 - gender_output_acc: 0.7620 - image_quality_output_acc: 0.5661 - age_output_acc: 0.4023 - weight_output_acc: 0.6352 - bag_output_acc: 0.6168 - pose_output_acc: 0.7105 - footwear_output_acc: 0.5889 - emotion_output_acc: 0.7081 - val_loss: 7.6915 - val_gender_output_loss: 0.7359 - val_image_quality_output_loss: 1.1394 - val_age_output_loss: 1.4029 - val_weight_output_loss: 0.9564 - val_bag_output_loss: 0.8613 - val_pose_output_loss: 0.8435 - val_footwear_output_loss: 0.9042 - val_emotion_output_loss: 0.8479 - val_gender_output_acc: 0.6714 - val_image_quality_output_acc: 0.4446 - val_age_output_acc: 0.3866 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.6104 - val_pose_output_acc: 0.7036 - val_footwear_output_acc: 0.5827 - val_emotion_output_acc: 0.7329\n",
            "Epoch 12/100\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.009317792.\n",
            "Epoch 13/100\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0091991233.\n",
            "180/180 [==============================] - 105s 582ms/step - loss: 6.9698 - gender_output_loss: 0.4709 - image_quality_output_loss: 0.9062 - age_output_loss: 1.3783 - weight_output_loss: 0.9625 - bag_output_loss: 0.8396 - pose_output_loss: 0.6574 - footwear_output_loss: 0.8720 - emotion_output_loss: 0.8829 - gender_output_acc: 0.7780 - image_quality_output_acc: 0.5672 - age_output_acc: 0.4049 - weight_output_acc: 0.6342 - bag_output_acc: 0.6196 - pose_output_acc: 0.7219 - footwear_output_acc: 0.5990 - emotion_output_acc: 0.7076 - val_loss: 7.4924 - val_gender_output_loss: 0.7704 - val_image_quality_output_loss: 0.9621 - val_age_output_loss: 1.3981 - val_weight_output_loss: 0.9532 - val_bag_output_loss: 1.0340 - val_pose_output_loss: 0.6208 - val_footwear_output_loss: 0.8941 - val_emotion_output_loss: 0.8598 - val_gender_output_acc: 0.6850 - val_image_quality_output_acc: 0.5494 - val_age_output_acc: 0.3871 - val_weight_output_acc: 0.6406 - val_bag_output_acc: 0.5665 - val_pose_output_acc: 0.7440 - val_footwear_output_acc: 0.6119 - val_emotion_output_acc: 0.7319\n",
            "Epoch 14/100\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.009072337.\n",
            "180/180 [==============================] - 105s 581ms/step - loss: 6.9153 - gender_output_loss: 0.4608 - image_quality_output_loss: 0.9066 - age_output_loss: 1.3747 - weight_output_loss: 0.9573 - bag_output_loss: 0.8337 - pose_output_loss: 0.6349 - footwear_output_loss: 0.8626 - emotion_output_loss: 0.8847 - gender_output_acc: 0.7785 - image_quality_output_acc: 0.5681 - age_output_acc: 0.4057 - weight_output_acc: 0.6365 - bag_output_acc: 0.6265 - pose_output_acc: 0.7365 - footwear_output_acc: 0.5990 - emotion_output_acc: 0.7076 - val_loss: 7.7137 - val_gender_output_loss: 0.7489 - val_image_quality_output_loss: 1.0115 - val_age_output_loss: 1.4181 - val_weight_output_loss: 0.9604 - val_bag_output_loss: 0.9087 - val_pose_output_loss: 0.7448 - val_footwear_output_loss: 1.0360 - val_emotion_output_loss: 0.8853 - val_gender_output_acc: 0.7253 - val_image_quality_output_acc: 0.5247 - val_age_output_acc: 0.3579 - val_weight_output_acc: 0.6326 - val_bag_output_acc: 0.6043 - val_pose_output_acc: 0.7324 - val_footwear_output_acc: 0.5554 - val_emotion_output_acc: 0.7233\n",
            "Epoch 15/100\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0089378231.\n",
            "180/180 [==============================] - 105s 583ms/step - loss: 6.8784 - gender_output_loss: 0.4550 - image_quality_output_loss: 0.9044 - age_output_loss: 1.3731 - weight_output_loss: 0.9524 - bag_output_loss: 0.8292 - pose_output_loss: 0.6255 - footwear_output_loss: 0.8577 - emotion_output_loss: 0.8811 - gender_output_acc: 0.7856 - image_quality_output_acc: 0.5681 - age_output_acc: 0.4062 - weight_output_acc: 0.6368 - bag_output_acc: 0.6326 - pose_output_acc: 0.7404 - footwear_output_acc: 0.6002 - emotion_output_acc: 0.7077 - val_loss: 7.8846 - val_gender_output_loss: 0.8569 - val_image_quality_output_loss: 1.0796 - val_age_output_loss: 1.3872 - val_weight_output_loss: 0.9482 - val_bag_output_loss: 1.1087 - val_pose_output_loss: 0.7443 - val_footwear_output_loss: 0.8965 - val_emotion_output_loss: 0.8633 - val_gender_output_acc: 0.6598 - val_image_quality_output_acc: 0.5176 - val_age_output_acc: 0.3926 - val_weight_output_acc: 0.6467 - val_bag_output_acc: 0.5590 - val_pose_output_acc: 0.6961 - val_footwear_output_acc: 0.6114 - val_emotion_output_acc: 0.7324\n",
            "Epoch 16/100\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0087959874.\n",
            "180/180 [==============================] - 105s 582ms/step - loss: 6.8398 - gender_output_loss: 0.4464 - image_quality_output_loss: 0.9019 - age_output_loss: 1.3710 - weight_output_loss: 0.9535 - bag_output_loss: 0.8178 - pose_output_loss: 0.6119 - footwear_output_loss: 0.8554 - emotion_output_loss: 0.8819 - gender_output_acc: 0.7856 - image_quality_output_acc: 0.5671 - age_output_acc: 0.4094 - weight_output_acc: 0.6370 - bag_output_acc: 0.6385 - pose_output_acc: 0.7496 - footwear_output_acc: 0.6072 - emotion_output_acc: 0.7076 - val_loss: 6.9633 - val_gender_output_loss: 0.4160 - val_image_quality_output_loss: 1.0431 - val_age_output_loss: 1.3979 - val_weight_output_loss: 0.9358 - val_bag_output_loss: 0.8436 - val_pose_output_loss: 0.5765 - val_footwear_output_loss: 0.9144 - val_emotion_output_loss: 0.8360 - val_gender_output_acc: 0.8095 - val_image_quality_output_acc: 0.4738 - val_age_output_acc: 0.3992 - val_weight_output_acc: 0.6436 - val_bag_output_acc: 0.6386 - val_pose_output_acc: 0.7676 - val_footwear_output_acc: 0.5726 - val_emotion_output_acc: 0.7329\n",
            "Epoch 17/100\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.008647255.\n",
            "180/180 [==============================] - 105s 581ms/step - loss: 6.8044 - gender_output_loss: 0.4299 - image_quality_output_loss: 0.9014 - age_output_loss: 1.3681 - weight_output_loss: 0.9500 - bag_output_loss: 0.8209 - pose_output_loss: 0.5997 - footwear_output_loss: 0.8579 - emotion_output_loss: 0.8766 - gender_output_acc: 0.7969 - image_quality_output_acc: 0.5687 - age_output_acc: 0.4097 - weight_output_acc: 0.6380 - bag_output_acc: 0.6387 - pose_output_acc: 0.7519 - footwear_output_acc: 0.6047 - emotion_output_acc: 0.7081 - val_loss: 7.2423 - val_gender_output_loss: 0.5778 - val_image_quality_output_loss: 1.0389 - val_age_output_loss: 1.3969 - val_weight_output_loss: 0.9433 - val_bag_output_loss: 0.8723 - val_pose_output_loss: 0.6567 - val_footwear_output_loss: 0.8952 - val_emotion_output_loss: 0.8614 - val_gender_output_acc: 0.7530 - val_image_quality_output_acc: 0.5292 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6280 - val_bag_output_acc: 0.6114 - val_pose_output_acc: 0.7399 - val_footwear_output_acc: 0.6008 - val_emotion_output_acc: 0.7268\n",
            "180/180 [==============================] - 105s 582ms/step - loss: 6.8398 - gender_output_loss: 0.4464 - image_quality_output_loss: 0.9019 - age_output_loss: 1.3710 - weight_output_loss: 0.9535 - bag_output_loss: 0.8178 - pose_output_loss: 0.6119 - footwear_output_loss: 0.8554 - emotion_output_loss: 0.8819 - gender_output_acc: 0.7856 - image_quality_output_acc: 0.5671 - age_output_acc: 0.4094 - weight_output_acc: 0.6370 - bag_output_acc: 0.6385 - pose_output_acc: 0.7496 - footwear_output_acc: 0.6072 - emotion_output_acc: 0.7076 - val_loss: 6.9633 - val_gender_output_loss: 0.4160 - val_image_quality_output_loss: 1.0431 - val_age_output_loss: 1.3979 - val_weight_output_loss: 0.9358 - val_bag_output_loss: 0.8436 - val_pose_output_loss: 0.5765 - val_footwear_output_loss: 0.9144 - val_emotion_output_loss: 0.8360 - val_gender_output_acc: 0.8095 - val_image_quality_output_acc: 0.4738 - val_age_output_acc: 0.3992 - val_weight_output_acc: 0.6436 - val_bag_output_acc: 0.6386 - val_pose_output_acc: 0.7676 - val_footwear_output_acc: 0.5726 - val_emotion_output_acc: 0.7329\n",
            "Epoch 18/100\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0084920622.\n",
            "180/180 [==============================] - 104s 580ms/step - loss: 6.7742 - gender_output_loss: 0.4239 - image_quality_output_loss: 0.9002 - age_output_loss: 1.3658 - weight_output_loss: 0.9478 - bag_output_loss: 0.8143 - pose_output_loss: 0.5926 - footwear_output_loss: 0.8534 - emotion_output_loss: 0.8761 - gender_output_acc: 0.8039 - image_quality_output_acc: 0.5681 - age_output_acc: 0.4110 - weight_output_acc: 0.6367 - bag_output_acc: 0.6394 - pose_output_acc: 0.7583 - footwear_output_acc: 0.6088 - emotion_output_acc: 0.7078 - val_loss: 7.0207 - val_gender_output_loss: 0.4311 - val_image_quality_output_loss: 1.0091 - val_age_output_loss: 1.3916 - val_weight_output_loss: 0.9324 - val_bag_output_loss: 0.9150 - val_pose_output_loss: 0.6245 - val_footwear_output_loss: 0.8693 - val_emotion_output_loss: 0.8478 - val_gender_output_acc: 0.8155 - val_image_quality_output_acc: 0.5252 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6381 - val_bag_output_acc: 0.6013 - val_pose_output_acc: 0.7666 - val_footwear_output_acc: 0.6104 - val_emotion_output_acc: 0.7288\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0083308597.\n",
            "180/180 [==============================] - 105s 585ms/step - loss: 6.7408 - gender_output_loss: 0.4233 - image_quality_output_loss: 0.9008 - age_output_loss: 1.3645 - weight_output_loss: 0.9431 - bag_output_loss: 0.8101 - pose_output_loss: 0.5805 - footwear_output_loss: 0.8427 - emotion_output_loss: 0.8758 - gender_output_acc: 0.8041 - image_quality_output_acc: 0.5676 - age_output_acc: 0.4093 - weight_output_acc: 0.6382 - bag_output_acc: 0.6469 - pose_output_acc: 0.7611 - footwear_output_acc: 0.6114 - emotion_output_acc: 0.7079 - val_loss: 6.8835 - val_gender_output_loss: 0.4069 - val_image_quality_output_loss: 1.0915 - val_age_output_loss: 1.3742 - val_weight_output_loss: 0.9456 - val_bag_output_loss: 0.8115 - val_pose_output_loss: 0.5596 - val_footwear_output_loss: 0.8454 - val_emotion_output_loss: 0.8490 - val_gender_output_acc: 0.8211 - val_image_quality_output_acc: 0.4768 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6361 - val_bag_output_acc: 0.6633 - val_pose_output_acc: 0.7868 - val_footwear_output_acc: 0.6114 - val_emotion_output_acc: 0.7319\n",
            "Epoch 20/100\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0081641074.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 6.6801 - gender_output_loss: 0.4002 - image_quality_output_loss: 0.8960 - age_output_loss: 1.3584 - weight_output_loss: 0.9404 - bag_output_loss: 0.8030 - pose_output_loss: 0.5683 - footwear_output_loss: 0.8410 - emotion_output_loss: 0.8728 - gender_output_acc: 0.8181 - image_quality_output_acc: 0.5755 - age_output_acc: 0.4120 - weight_output_acc: 0.6432 - bag_output_acc: 0.6489 - pose_output_acc: 0.7672 - footwear_output_acc: 0.6128 - emotion_output_acc: 0.7078\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0081641074.\n",
            "180/180 [==============================] - 105s 583ms/step - loss: 6.6793 - gender_output_loss: 0.3995 - image_quality_output_loss: 0.8963 - age_output_loss: 1.3582 - weight_output_loss: 0.9401 - bag_output_loss: 0.8032 - pose_output_loss: 0.5684 - footwear_output_loss: 0.8410 - emotion_output_loss: 0.8727 - gender_output_acc: 0.8186 - image_quality_output_acc: 0.5756 - age_output_acc: 0.4120 - weight_output_acc: 0.6436 - bag_output_acc: 0.6492 - pose_output_acc: 0.7670 - footwear_output_acc: 0.6132 - emotion_output_acc: 0.7079 - val_loss: 7.4580 - val_gender_output_loss: 0.6422 - val_image_quality_output_loss: 1.0994 - val_age_output_loss: 1.4095 - val_weight_output_loss: 0.9383 - val_bag_output_loss: 0.9527 - val_pose_output_loss: 0.6674 - val_footwear_output_loss: 0.9006 - val_emotion_output_loss: 0.8480 - val_gender_output_acc: 0.7424 - val_image_quality_output_acc: 0.4602 - val_age_output_acc: 0.3831 - val_weight_output_acc: 0.6477 - val_bag_output_acc: 0.5998 - val_pose_output_acc: 0.7560 - val_footwear_output_acc: 0.5912 - val_emotion_output_acc: 0.7319\n",
            "Epoch 21/100\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0079922739.\n",
            "180/180 [==============================] - 105s 581ms/step - loss: 6.6459 - gender_output_loss: 0.3991 - image_quality_output_loss: 0.8925 - age_output_loss: 1.3588 - weight_output_loss: 0.9354 - bag_output_loss: 0.7959 - pose_output_loss: 0.5558 - footwear_output_loss: 0.8367 - emotion_output_loss: 0.8718 - gender_output_acc: 0.8147 - image_quality_output_acc: 0.5781 - age_output_acc: 0.4133 - weight_output_acc: 0.6434 - bag_output_acc: 0.6562 - pose_output_acc: 0.7781 - footwear_output_acc: 0.6190 - emotion_output_acc: 0.7079 - val_loss: 6.9808 - val_gender_output_loss: 0.3879 - val_image_quality_output_loss: 1.1728 - val_age_output_loss: 1.3782 - val_weight_output_loss: 0.9329 - val_bag_output_loss: 0.8155 - val_pose_output_loss: 0.5891 - val_footwear_output_loss: 0.8670 - val_emotion_output_loss: 0.8373 - val_gender_output_acc: 0.8231 - val_image_quality_output_acc: 0.4047 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6341 - val_bag_output_acc: 0.6492 - val_pose_output_acc: 0.7777 - val_footwear_output_acc: 0.5902 - val_emotion_output_acc: 0.7329\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0078158317.\n",
            "180/180 [==============================] - 105s 585ms/step - loss: 6.6446 - gender_output_loss: 0.4015 - image_quality_output_loss: 0.8919 - age_output_loss: 1.3543 - weight_output_loss: 0.9321 - bag_output_loss: 0.8013 - pose_output_loss: 0.5536 - footwear_output_loss: 0.8379 - emotion_output_loss: 0.8719 - gender_output_acc: 0.8138 - image_quality_output_acc: 0.5732 - age_output_acc: 0.4146 - weight_output_acc: 0.6402 - bag_output_acc: 0.6523 - pose_output_acc: 0.7784 - footwear_output_acc: 0.6150 - emotion_output_acc: 0.7077 - val_loss: 7.0765 - val_gender_output_loss: 0.4620 - val_image_quality_output_loss: 1.0889 - val_age_output_loss: 1.3800 - val_weight_output_loss: 0.9123 - val_bag_output_loss: 0.9419 - val_pose_output_loss: 0.6104 - val_footwear_output_loss: 0.8269 - val_emotion_output_loss: 0.8541 - val_gender_output_acc: 0.8135 - val_image_quality_output_acc: 0.4723 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6502 - val_bag_output_acc: 0.6053 - val_pose_output_acc: 0.7596 - val_footwear_output_acc: 0.6240 - val_emotion_output_acc: 0.7319\n",
            "Epoch 23/100\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0076352575.\n",
            "179/180 [============================>.] - ETA: 0s - loss: 6.5935 - gender_output_loss: 0.3915 - image_quality_output_loss: 0.8892 - age_output_loss: 1.3496 - weight_output_loss: 0.9276 - bag_output_loss: 0.7939 - pose_output_loss: 0.5401 - footwear_output_loss: 0.8327 - emotion_output_loss: 0.8688 - gender_output_acc: 0.8228 - image_quality_output_acc: 0.5762 - age_output_acc: 0.4184 - weight_output_acc: 0.6432 - bag_output_acc: 0.6550 - pose_output_acc: 0.7808 - footwear_output_acc: 0.6176 - emotion_output_acc: 0.7085\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0076352575.\n",
            "180/180 [==============================] - 105s 585ms/step - loss: 6.6446 - gender_output_loss: 0.4015 - image_quality_output_loss: 0.8919 - age_output_loss: 1.3543 - weight_output_loss: 0.9321 - bag_output_loss: 0.8013 - pose_output_loss: 0.5536 - footwear_output_loss: 0.8379 - emotion_output_loss: 0.8719 - gender_output_acc: 0.8138 - image_quality_output_acc: 0.5732 - age_output_acc: 0.4146 - weight_output_acc: 0.6402 - bag_output_acc: 0.6523 - pose_output_acc: 0.7784 - footwear_output_acc: 0.6150 - emotion_output_acc: 0.7077 - val_loss: 7.0765 - val_gender_output_loss: 0.4620 - val_image_quality_output_loss: 1.0889 - val_age_output_loss: 1.3800 - val_weight_output_loss: 0.9123 - val_bag_output_loss: 0.9419 - val_pose_output_loss: 0.6104 - val_footwear_output_loss: 0.8269 - val_emotion_output_loss: 0.8541 - val_gender_output_acc: 0.8135 - val_image_quality_output_acc: 0.4723 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6502 - val_bag_output_acc: 0.6053 - val_pose_output_acc: 0.7596 - val_footwear_output_acc: 0.6240 - val_emotion_output_acc: 0.7319\n",
            "180/180 [==============================] - 105s 582ms/step - loss: 6.5961 - gender_output_loss: 0.3916 - image_quality_output_loss: 0.8902 - age_output_loss: 1.3499 - weight_output_loss: 0.9288 - bag_output_loss: 0.7939 - pose_output_loss: 0.5398 - footwear_output_loss: 0.8325 - emotion_output_loss: 0.8694 - gender_output_acc: 0.8227 - image_quality_output_acc: 0.5755 - age_output_acc: 0.4181 - weight_output_acc: 0.6429 - bag_output_acc: 0.6550 - pose_output_acc: 0.7810 - footwear_output_acc: 0.6179 - emotion_output_acc: 0.7082 - val_loss: 6.9454 - val_gender_output_loss: 0.4019 - val_image_quality_output_loss: 1.1881 - val_age_output_loss: 1.3901 - val_weight_output_loss: 0.9550 - val_bag_output_loss: 0.7939 - val_pose_output_loss: 0.5396 - val_footwear_output_loss: 0.8370 - val_emotion_output_loss: 0.8397 - val_gender_output_acc: 0.8286 - val_image_quality_output_acc: 0.4430 - val_age_output_acc: 0.3952 - val_weight_output_acc: 0.6366 - val_bag_output_acc: 0.6643 - val_pose_output_acc: 0.7898 - val_footwear_output_acc: 0.6255 - val_emotion_output_acc: 0.7324\n",
            "Epoch 24/100\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0074510309.\n",
            "180/180 [==============================] - 104s 580ms/step - loss: 6.5791 - gender_output_loss: 0.3873 - image_quality_output_loss: 0.8879 - age_output_loss: 1.3515 - weight_output_loss: 0.9273 - bag_output_loss: 0.7907 - pose_output_loss: 0.5340 - footwear_output_loss: 0.8314 - emotion_output_loss: 0.8690 - gender_output_acc: 0.8254 - image_quality_output_acc: 0.5741 - age_output_acc: 0.4164 - weight_output_acc: 0.6419 - bag_output_acc: 0.6567 - pose_output_acc: 0.7815 - footwear_output_acc: 0.6227 - emotion_output_acc: 0.7081 - val_loss: 6.9670 - val_gender_output_loss: 0.3692 - val_image_quality_output_loss: 1.1424 - val_age_output_loss: 1.3951 - val_weight_output_loss: 1.0175 - val_bag_output_loss: 0.8166 - val_pose_output_loss: 0.5444 - val_footwear_output_loss: 0.8472 - val_emotion_output_loss: 0.8345 - val_gender_output_acc: 0.8311 - val_image_quality_output_acc: 0.4708 - val_age_output_acc: 0.3831 - val_weight_output_acc: 0.5771 - val_bag_output_acc: 0.6552 - val_pose_output_acc: 0.7838 - val_footwear_output_acc: 0.6129 - val_emotion_output_acc: 0.7329\n",
            "Epoch 25/100\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0072636293.\n",
            "180/180 [==============================]\n",
            "180/180 [==============================] - 105s 583ms/step - loss: 6.5437 - gender_output_loss: 0.3764 - image_quality_output_loss: 0.8886 - age_output_loss: 1.3437 - weight_output_loss: 0.9267 - bag_output_loss: 0.7882 - pose_output_loss: 0.5237 - footwear_output_loss: 0.8277 - emotion_output_loss: 0.8686 - gender_output_acc: 0.8287 - image_quality_output_acc: 0.5763 - age_output_acc: 0.4159 - weight_output_acc: 0.6399 - bag_output_acc: 0.6646 - pose_output_acc: 0.7916 - footwear_output_acc: 0.6235 - emotion_output_acc: 0.7074 - val_loss: 7.3771 - val_gender_output_loss: 0.4675 - val_image_quality_output_loss: 1.2151 - val_age_output_loss: 1.5307 - val_weight_output_loss: 0.9549 - val_bag_output_loss: 0.9038 - val_pose_output_loss: 0.5914 - val_footwear_output_loss: 0.8761 - val_emotion_output_loss: 0.8376 - val_gender_output_acc: 0.7903 - val_image_quality_output_acc: 0.4017 - val_age_output_acc: 0.3438 - val_weight_output_acc: 0.6447 - val_bag_output_acc: 0.5907 - val_pose_output_acc: 0.7737 - val_footwear_output_acc: 0.6018 - val_emotion_output_acc: 0.7324\n",
            "Epoch 26/100\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0070735282.\n",
            "180/180 [==============================] - 105s 582ms/step - loss: 6.5068 - gender_output_loss: 0.3690 - image_quality_output_loss: 0.8854 - age_output_loss: 1.3422 - weight_output_loss: 0.9215 - bag_output_loss: 0.7846 - pose_output_loss: 0.5131 - footwear_output_loss: 0.8238 - emotion_output_loss: 0.8673 - gender_output_acc: 0.8346 - image_quality_output_acc: 0.5792 - age_output_acc: 0.4191 - weight_output_acc: 0.6448 - bag_output_acc: 0.6636 - pose_output_acc: 0.7973 - footwear_output_acc: 0.6289 - emotion_output_acc: 0.7080 - val_loss: 6.9372 - val_gender_output_loss: 0.3682 - val_image_quality_output_loss: 1.1626 - val_age_output_loss: 1.3918 - val_weight_output_loss: 0.9141 - val_bag_output_loss: 0.8155 - val_pose_output_loss: 0.5390 - val_footwear_output_loss: 0.9086 - val_emotion_output_loss: 0.8373 - val_gender_output_acc: 0.8432 - val_image_quality_output_acc: 0.4153 - val_age_output_acc: 0.3866 - val_weight_output_acc: 0.6492 - val_bag_output_acc: 0.6562 - val_pose_output_acc: 0.7868 - val_footwear_output_acc: 0.5701 - val_emotion_output_acc: 0.7344\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0068811986.\n",
            "180/180 [==============================] - 105s 582ms/step - loss: 6.4949 - gender_output_loss: 0.3672 - image_quality_output_loss: 0.8882 - age_output_loss: 1.3440 - weight_output_loss: 0.9169 - bag_output_loss: 0.7766 - pose_output_loss: 0.5148 - footwear_output_loss: 0.8178 - emotion_output_loss: 0.8693 - gender_output_acc: 0.8384 - image_quality_output_acc: 0.5752 - age_output_acc: 0.4181 - weight_output_acc: 0.6460 - bag_output_acc: 0.6695 - pose_output_acc: 0.7961 - footwear_output_acc: 0.6256 - emotion_output_acc: 0.7077 - val_loss: 6.8066 - val_gender_output_loss: 0.4342 - val_image_quality_output_loss: 1.0544 - val_age_output_loss: 1.3638 - val_weight_output_loss: 0.9074 - val_bag_output_loss: 0.8690 - val_pose_output_loss: 0.5273 - val_footwear_output_loss: 0.8073 - val_emotion_output_loss: 0.8431 - val_gender_output_acc: 0.8140 - val_image_quality_output_acc: 0.4859 - val_age_output_acc: 0.4093 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.6411 - val_pose_output_acc: 0.8024 - val_footwear_output_acc: 0.6386 - val_emotion_output_acc: 0.7329\n",
            "Epoch 27/100Epoch 28/100\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0066871052.\n",
            "180/180 [==============================] - 105s 586ms/step - loss: 6.4541 - gender_output_loss: 0.3597 - image_quality_output_loss: 0.8840 - age_output_loss: 1.3377 - weight_output_loss: 0.9168 - bag_output_loss: 0.7756 - pose_output_loss: 0.5012 - footwear_output_loss: 0.8154 - emotion_output_loss: 0.8637 - gender_output_acc: 0.8434 - image_quality_output_acc: 0.5810 - age_output_acc: 0.4195 - weight_output_acc: 0.6459 - bag_output_acc: 0.6696 - pose_output_acc: 0.7984 - footwear_output_acc: 0.6318 - emotion_output_acc: 0.7082 - val_loss: 6.7679 - val_gender_output_loss: 0.4290 - val_image_quality_output_loss: 0.9887 - val_age_output_loss: 1.3622 - val_weight_output_loss: 0.9032 - val_bag_output_loss: 0.8405 - val_pose_output_loss: 0.5772 - val_footwear_output_loss: 0.8197 - val_emotion_output_loss: 0.8473 - val_gender_output_acc: 0.8100 - val_image_quality_output_acc: 0.5519 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6517 - val_bag_output_acc: 0.6285 - val_pose_output_acc: 0.7792 - val_footwear_output_acc: 0.6310 - val_emotion_output_acc: 0.7329\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0066871052.\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 29/100\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0064917049.\n",
            "180/180 [==============================] - 105s 582ms/step - loss: 6.4309 - gender_output_loss: 0.3506 - image_quality_output_loss: 0.8781 - age_output_loss: 1.3291 - weight_output_loss: 0.9145 - bag_output_loss: 0.7757 - pose_output_loss: 0.4990 - footwear_output_loss: 0.8159 - emotion_output_loss: 0.8680 - gender_output_acc: 0.8452 - image_quality_output_acc: 0.5803 - age_output_acc: 0.4207 - weight_output_acc: 0.6465 - bag_output_acc: 0.6733 - pose_output_acc: 0.8035 - footwear_output_acc: 0.6333 - emotion_output_acc: 0.7080 - val_loss: 7.0691 - val_gender_output_loss: 0.4585 - val_image_quality_output_loss: 1.1227 - val_age_output_loss: 1.4210 - val_weight_output_loss: 0.9282 - val_bag_output_loss: 0.8711 - val_pose_output_loss: 0.5277 - val_footwear_output_loss: 0.8825 - val_emotion_output_loss: 0.8573 - val_gender_output_acc: 0.8135 - val_image_quality_output_acc: 0.4728 - val_age_output_acc: 0.3821 - val_weight_output_acc: 0.6552 - val_bag_output_acc: 0.6285 - val_pose_output_acc: 0.7939 - val_footwear_output_acc: 0.5867 - val_emotion_output_acc: 0.7334\n",
            "Epoch 30/100\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0062954444.\n",
            "Epoch 29/100\n",
            "180/180 [==============================] - 105s 585ms/step - loss: 6.4201 - gender_output_loss: 0.3469 - image_quality_output_loss: 0.8816 - age_output_loss: 1.3278 - weight_output_loss: 0.9116 - bag_output_loss: 0.7741 - pose_output_loss: 0.4964 - footwear_output_loss: 0.8179 - emotion_output_loss: 0.8637 - gender_output_acc: 0.8482 - image_quality_output_acc: 0.5796 - age_output_acc: 0.4244 - weight_output_acc: 0.6489 - bag_output_acc: 0.6703 - pose_output_acc: 0.8066 - footwear_output_acc: 0.6331 - emotion_output_acc: 0.7076 - val_loss: 6.8479 - val_gender_output_loss: 0.3556 - val_image_quality_output_loss: 1.0835 - val_age_output_loss: 1.3830 - val_weight_output_loss: 0.9098 - val_bag_output_loss: 0.8594 - val_pose_output_loss: 0.5348 - val_footwear_output_loss: 0.8633 - val_emotion_output_loss: 0.8584 - val_gender_output_acc: 0.8468 - val_image_quality_output_acc: 0.4682 - val_age_output_acc: 0.3967 - val_weight_output_acc: 0.6648 - val_bag_output_acc: 0.6235 - val_pose_output_acc: 0.8004 - val_footwear_output_acc: 0.6018 - val_emotion_output_acc: 0.7273\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0060987595.\n",
            "180/180 [==============================] - 104s 580ms/step - loss: 6.3920 - gender_output_loss: 0.3477 - image_quality_output_loss: 0.8822 - age_output_loss: 1.3235 - weight_output_loss: 0.9056 - bag_output_loss: 0.7662 - pose_output_loss: 0.4872 - footwear_output_loss: 0.8154 - emotion_output_loss: 0.8643 - gender_output_acc: 0.8473 - image_quality_output_acc: 0.5832 - age_output_acc: 0.4264 - weight_output_acc: 0.6494 - bag_output_acc: 0.6740 - pose_output_acc: 0.8065 - footwear_output_acc: 0.6311 - emotion_output_acc: 0.7076 - val_loss: 6.7123 - val_gender_output_loss: 0.3649 - val_image_quality_output_loss: 0.9953 - val_age_output_loss: 1.3764 - val_weight_output_loss: 0.9034 - val_bag_output_loss: 0.8696 - val_pose_output_loss: 0.5218 - val_footwear_output_loss: 0.8346 - val_emotion_output_loss: 0.8465 - val_gender_output_acc: 0.8417 - val_image_quality_output_acc: 0.5161 - val_age_output_acc: 0.3936 - val_weight_output_acc: 0.6482 - val_bag_output_acc: 0.6225 - val_pose_output_acc: 0.8029 - val_footwear_output_acc: 0.6290 - val_emotion_output_acc: 0.7308\n",
            "Epoch 32/100\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0059020728.\n",
            "180/180 [==============================] - 105s 581ms/step - loss: 6.3777 - gender_output_loss: 0.3430 - image_quality_output_loss: 0.8805 - age_output_loss: 1.3216 - weight_output_loss: 0.9055 - bag_output_loss: 0.7650 - pose_output_loss: 0.4826 - footwear_output_loss: 0.8157 - emotion_output_loss: 0.8640 - gender_output_acc: 0.8492 - image_quality_output_acc: 0.5797 - age_output_acc: 0.4253 - weight_output_acc: 0.6470 - bag_output_acc: 0.6726 - pose_output_acc: 0.8125 - footwear_output_acc: 0.6311 - emotion_output_acc: 0.7076 - val_loss: 6.6722 - val_gender_output_loss: 0.3378 - val_image_quality_output_loss: 1.0346 - val_age_output_loss: 1.3739 - val_weight_output_loss: 0.9160 - val_bag_output_loss: 0.7923 - val_pose_output_loss: 0.5672 - val_footwear_output_loss: 0.8173 - val_emotion_output_loss: 0.8332 - val_gender_output_acc: 0.8548 - val_image_quality_output_acc: 0.4899 - val_age_output_acc: 0.3906 - val_weight_output_acc: 0.6522 - val_bag_output_acc: 0.6643 - val_pose_output_acc: 0.7908 - val_footwear_output_acc: 0.6336 - val_emotion_output_acc: 0.7324\n",
            "Epoch 33/100\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.0057057936.\n",
            "180/180 [==============================] - 104s 579ms/step - loss: 6.3431 - gender_output_loss: 0.3337 - image_quality_output_loss: 0.8777 - age_output_loss: 1.3212 - weight_output_loss: 0.8998 - bag_output_loss: 0.7603 - pose_output_loss: 0.4823 - footwear_output_loss: 0.8067 - emotion_output_loss: 0.8615 - gender_output_acc: 0.8530 - image_quality_output_acc: 0.5849 - age_output_acc: 0.4192 - weight_output_acc: 0.6510 - bag_output_acc: 0.6801 - pose_output_acc: 0.8075 - footwear_output_acc: 0.6335 - emotion_output_acc: 0.7082 - val_loss: 6.5999 - val_gender_output_loss: 0.3540 - val_image_quality_output_loss: 1.0144 - val_age_output_loss: 1.3605 - val_weight_output_loss: 0.9127 - val_bag_output_loss: 0.7965 - val_pose_output_loss: 0.4994 - val_footwear_output_loss: 0.8215 - val_emotion_output_loss: 0.8410 - val_gender_output_acc: 0.8523 - val_image_quality_output_acc: 0.5202 - val_age_output_acc: 0.4057 - val_weight_output_acc: 0.6457 - val_bag_output_acc: 0.6663 - val_pose_output_acc: 0.8085 - val_footwear_output_acc: 0.6321 - val_emotion_output_acc: 0.7278\n",
            "\n",
            "Epoch 34/100\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.005510315.\n",
            "180/180 [==============================] - 105s 582ms/step - loss: 6.3151 - gender_output_loss: 0.3272 - image_quality_output_loss: 0.8781 - age_output_loss: 1.3117 - weight_output_loss: 0.9026 - bag_output_loss: 0.7573 - pose_output_loss: 0.4744 - footwear_output_loss: 0.8025 - emotion_output_loss: 0.8612 - gender_output_acc: 0.8569 - image_quality_output_acc: 0.5810 - age_output_acc: 0.4339 - weight_output_acc: 0.6480 - bag_output_acc: 0.6794 - pose_output_acc: 0.8174 - footwear_output_acc: 0.6365 - emotion_output_acc: 0.7074 - val_loss: 6.8394 - val_gender_output_loss: 0.3460 - val_image_quality_output_loss: 1.1043 - val_age_output_loss: 1.3671 - val_weight_output_loss: 0.9344 - val_bag_output_loss: 0.8116 - val_pose_output_loss: 0.5265 - val_footwear_output_loss: 0.9120 - val_emotion_output_loss: 0.8375 - val_gender_output_acc: 0.8523 - val_image_quality_output_acc: 0.4430 - val_age_output_acc: 0.3982 - val_weight_output_acc: 0.6522 - val_bag_output_acc: 0.6638 - val_pose_output_acc: 0.7964 - val_footwear_output_acc: 0.5731 - val_emotion_output_acc: 0.7314\n",
            "Epoch 35/100\n",
            "Epoch 34/100\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.0053160145.\n",
            "180/180 [==============================] - 105s 583ms/step - loss: 6.2951 - gender_output_loss: 0.3258 - image_quality_output_loss: 0.8764 - age_output_loss: 1.3107 - weight_output_loss: 0.8958 - bag_output_loss: 0.7527 - pose_output_loss: 0.4701 - footwear_output_loss: 0.8045 - emotion_output_loss: 0.8592 - gender_output_acc: 0.8580 - image_quality_output_acc: 0.5829 - age_output_acc: 0.4237 - weight_output_acc: 0.6514 - bag_output_acc: 0.6829 - pose_output_acc: 0.8192 - footwear_output_acc: 0.6339 - emotion_output_acc: 0.7082 - val_loss: 6.7150 - val_gender_output_loss: 0.3452 - val_image_quality_output_loss: 1.0517 - val_age_output_loss: 1.3545 - val_weight_output_loss: 0.9155 - val_bag_output_loss: 0.8078 - val_pose_output_loss: 0.5765 - val_footwear_output_loss: 0.8230 - val_emotion_output_loss: 0.8408 - val_gender_output_acc: 0.8543 - val_image_quality_output_acc: 0.5066 - val_age_output_acc: 0.4168 - val_weight_output_acc: 0.6442 - val_bag_output_acc: 0.6658 - val_pose_output_acc: 0.7828 - val_footwear_output_acc: 0.6376 - val_emotion_output_acc: 0.7308\n",
            "Epoch 36/100\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.005123252.\n",
            "180/180 [==============================] - 106s 587ms/step - loss: 6.2797 - gender_output_loss: 0.3227 - image_quality_output_loss: 0.8756 - age_output_loss: 1.3146 - weight_output_loss: 0.8939 - bag_output_loss: 0.7535 - pose_output_loss: 0.4550 - footwear_output_loss: 0.8021 - emotion_output_loss: 0.8622 - gender_output_acc: 0.8615 - image_quality_output_acc: 0.5827 - age_output_acc: 0.4274 - weight_output_acc: 0.6518 - bag_output_acc: 0.6790 - pose_output_acc: 0.8194 - footwear_output_acc: 0.6385 - emotion_output_acc: 0.7078 - val_loss: 6.5592 - val_gender_output_loss: 0.3767 - val_image_quality_output_loss: 0.9407 - val_age_output_loss: 1.3628 - val_weight_output_loss: 0.9211 - val_bag_output_loss: 0.8009 - val_pose_output_loss: 0.5029 - val_footwear_output_loss: 0.8120 - val_emotion_output_loss: 0.8421 - val_gender_output_acc: 0.8322 - val_image_quality_output_acc: 0.5575 - val_age_output_acc: 0.4027 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.6673 - val_pose_output_acc: 0.8125 - val_footwear_output_acc: 0.6326 - val_emotion_output_acc: 0.7298\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0049323694.\n",
            "180/180 [==============================] - 105s 581ms/step - loss: 6.2437 - gender_output_loss: 0.3169 - image_quality_output_loss: 0.8705 - age_output_loss: 1.3053 - weight_output_loss: 0.8903 - bag_output_loss: 0.7467 - pose_output_loss: 0.4605 - footwear_output_loss: 0.7963 - emotion_output_loss: 0.8572 - gender_output_acc: 0.8588 - image_quality_output_acc: 0.5869 - age_output_acc: 0.4313 - weight_output_acc: 0.6523 - bag_output_acc: 0.6865 - pose_output_acc: 0.8177 - footwear_output_acc: 0.6424 - emotion_output_acc: 0.7077 - val_loss: 6.8958 - val_gender_output_loss: 0.3314 - val_image_quality_output_loss: 1.1526 - val_age_output_loss: 1.3534 - val_weight_output_loss: 0.9083 - val_bag_output_loss: 0.8671 - val_pose_output_loss: 0.5464 - val_footwear_output_loss: 0.8856 - val_emotion_output_loss: 0.8510 - val_gender_output_acc: 0.8624 - val_image_quality_output_acc: 0.4395 - val_age_output_acc: 0.4199 - val_weight_output_acc: 0.6507 - val_bag_output_acc: 0.6321 - val_pose_output_acc: 0.7913 - val_footwear_output_acc: 0.6023 - val_emotion_output_acc: 0.7263\n",
            "Epoch 38/100\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0047436893.\n",
            "180/180 [==============================] - 105s 581ms/step - loss: 6.2437 - gender_output_loss: 0.3169 - image_quality_output_loss: 0.8705 - age_output_loss: 1.3053 - weight_output_loss: 0.8903 - bag_output_loss: 0.7467 - pose_output_loss: 0.4605 - footwear_output_loss: 0.7963 - emotion_output_loss: 0.8572 - gender_output_acc: 0.8588 - image_quality_output_acc: 0.5869 - age_output_acc: 0.4313 - weight_output_acc: 0.6523 - bag_output_acc: 0.6865 - pose_output_acc: 0.8177 - footwear_output_acc: 0.6424 - emotion_output_acc: 0.7077 - val_loss: 6.8958 - val_gender_output_loss: 0.3314 - val_image_quality_output_loss: 1.1526 - val_age_output_loss: 1.3534 - val_weight_output_loss: 0.9083 - val_bag_output_loss: 0.8671 - val_pose_output_loss: 0.5464 - val_footwear_output_loss: 0.8856 - val_emotion_output_loss: 0.8510 - val_gender_output_acc: 0.8624 - val_image_quality_output_acc: 0.4395 - val_age_output_acc: 0.4199 - val_weight_output_acc: 0.6507 - val_bag_output_acc: 0.6321 - val_pose_output_acc: 0.7913 - val_footwear_output_acc: 0.6023 - val_emotion_output_acc: 0.7263\n",
            "180/180 [==============================] - 105s 585ms/step - loss: 6.2276 - gender_output_loss: 0.3151 - image_quality_output_loss: 0.8721 - age_output_loss: 1.3033 - weight_output_loss: 0.8859 - bag_output_loss: 0.7500 - pose_output_loss: 0.4489 - footwear_output_loss: 0.7935 - emotion_output_loss: 0.8590 - gender_output_acc: 0.8626 - image_quality_output_acc: 0.5822 - age_output_acc: 0.4317 - weight_output_acc: 0.6521 - bag_output_acc: 0.6837 - pose_output_acc: 0.8240 - footwear_output_acc: 0.6423 - emotion_output_acc: 0.7076 - val_loss: 6.6420 - val_gender_output_loss: 0.3162 - val_image_quality_output_loss: 1.0010 - val_age_output_loss: 1.3354 - val_weight_output_loss: 0.9308 - val_bag_output_loss: 0.8551 - val_pose_output_loss: 0.5545 - val_footwear_output_loss: 0.8090 - val_emotion_output_loss: 0.8399 - val_gender_output_acc: 0.8679 - val_image_quality_output_acc: 0.5111 - val_age_output_acc: 0.4108 - val_weight_output_acc: 0.6366 - val_bag_output_acc: 0.6356 - val_pose_output_acc: 0.7873 - val_footwear_output_acc: 0.6401 - val_emotion_output_acc: 0.7293\n",
            "Epoch 39/100\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0045575147.\n",
            "180/180 [==============================] - 105s 583ms/step - loss: 6.2100 - gender_output_loss: 0.3101 - image_quality_output_loss: 0.8676 - age_output_loss: 1.3010 - weight_output_loss: 0.8884 - bag_output_loss: 0.7448 - pose_output_loss: 0.4496 - footwear_output_loss: 0.7914 - emotion_output_loss: 0.8572 - gender_output_acc: 0.8663 - image_quality_output_acc: 0.5846 - age_output_acc: 0.4347 - weight_output_acc: 0.6525 - bag_output_acc: 0.6866 - pose_output_acc: 0.8257 - footwear_output_acc: 0.6406 - emotion_output_acc: 0.7077 - val_loss: 6.6713 - val_gender_output_loss: 0.3641 - val_image_quality_output_loss: 1.0005 - val_age_output_loss: 1.3536 - val_weight_output_loss: 0.9565 - val_bag_output_loss: 0.8015 - val_pose_output_loss: 0.5442 - val_footwear_output_loss: 0.8162 - val_emotion_output_loss: 0.8348 - val_gender_output_acc: 0.8543 - val_image_quality_output_acc: 0.5136 - val_age_output_acc: 0.4042 - val_weight_output_acc: 0.6195 - val_bag_output_acc: 0.6764 - val_pose_output_acc: 0.7959 - val_footwear_output_acc: 0.6457 - val_emotion_output_acc: 0.7308\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0043741292.\n",
            "180/180 [==============================] - 105s 585ms/step - loss: 6.2276 - gender_output_loss: 0.3151 - image_quality_output_loss: 0.8721 - age_output_loss: 1.3033 - weight_output_loss: 0.8859 - bag_output_loss: 0.7500 - pose_output_loss: 0.4489 - footwear_output_loss: 0.7935 - emotion_output_loss: 0.8590 - gender_output_acc: 0.8626 - image_quality_output_acc: 0.5822 - age_output_acc: 0.4317 - weight_output_acc: 0.6521 - bag_output_acc: 0.6837 - pose_output_acc: 0.8240 - footwear_output_acc: 0.6423 - emotion_output_acc: 0.7076 - val_loss: 6.6420 - val_gender_output_loss: 0.3162 - val_image_quality_output_loss: 1.0010 - val_age_output_loss: 1.3354 - val_weight_output_loss: 0.9308 - val_bag_output_loss: 0.8551 - val_pose_output_loss: 0.5545 - val_footwear_output_loss: 0.8090 - val_emotion_output_loss: 0.8399 - val_gender_output_acc: 0.8679 - val_image_quality_output_acc: 0.5111 - val_age_output_acc: 0.4108 - val_weight_output_acc: 0.6366 - val_bag_output_acc: 0.6356 - val_pose_output_acc: 0.7873 - val_footwear_output_acc: 0.6401 - val_emotion_output_acc: 0.7293\n",
            "180/180 [==============================] - 105s 582ms/step - loss: 6.1778 - gender_output_loss: 0.2999 - image_quality_output_loss: 0.8663 - age_output_loss: 1.2993 - weight_output_loss: 0.8806 - bag_output_loss: 0.7405 - pose_output_loss: 0.4437 - footwear_output_loss: 0.7917 - emotion_output_loss: 0.8557 - gender_output_acc: 0.8753 - image_quality_output_acc: 0.5884 - age_output_acc: 0.4349 - weight_output_acc: 0.6553 - bag_output_acc: 0.6910 - pose_output_acc: 0.8266 - footwear_output_acc: 0.6391 - emotion_output_acc: 0.7091 - val_loss: 6.7098 - val_gender_output_loss: 0.4087 - val_image_quality_output_loss: 0.9890 - val_age_output_loss: 1.3919 - val_weight_output_loss: 0.9198 - val_bag_output_loss: 0.8186 - val_pose_output_loss: 0.5250 - val_footwear_output_loss: 0.8157 - val_emotion_output_loss: 0.8412 - val_gender_output_acc: 0.8372 - val_image_quality_output_acc: 0.5237 - val_age_output_acc: 0.3871 - val_weight_output_acc: 0.6472 - val_bag_output_acc: 0.6633 - val_pose_output_acc: 0.8185 - val_footwear_output_acc: 0.6401 - val_emotion_output_acc: 0.7319\n",
            "Epoch 41/100\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 0.004193796.\n",
            "180/180 [==============================] - 105s 581ms/step - loss: 6.1788 - gender_output_loss: 0.2990 - image_quality_output_loss: 0.8641 - age_output_loss: 1.2944 - weight_output_loss: 0.8790 - bag_output_loss: 0.7441 - pose_output_loss: 0.4420 - footwear_output_loss: 0.7988 - emotion_output_loss: 0.8574 - gender_output_acc: 0.8741 - image_quality_output_acc: 0.5873 - age_output_acc: 0.4351 - weight_output_acc: 0.6587 - bag_output_acc: 0.6904 - pose_output_acc: 0.8279 - footwear_output_acc: 0.6416 - emotion_output_acc: 0.7078 - val_loss: 6.7757 - val_gender_output_loss: 0.3541 - val_image_quality_output_loss: 1.0870 - val_age_output_loss: 1.3792 - val_weight_output_loss: 0.9272 - val_bag_output_loss: 0.8112 - val_pose_output_loss: 0.5266 - val_footwear_output_loss: 0.8433 - val_emotion_output_loss: 0.8470 - val_gender_output_acc: 0.8629 - val_image_quality_output_acc: 0.4814 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6578 - val_bag_output_acc: 0.6794 - val_pose_output_acc: 0.8100 - val_footwear_output_acc: 0.6245 - val_emotion_output_acc: 0.7263\n",
            "Epoch 42/100\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 0.0040167573.\n",
            "180/180 [==============================] - 105s 582ms/step - loss: 6.1500 - gender_output_loss: 0.3007 - image_quality_output_loss: 0.8634 - age_output_loss: 1.2913 - weight_output_loss: 0.8775 - bag_output_loss: 0.7433 - pose_output_loss: 0.4281 - footwear_output_loss: 0.7930 - emotion_output_loss: 0.8525 - gender_output_acc: 0.8714 - image_quality_output_acc: 0.5866 - age_output_acc: 0.4388 - weight_output_acc: 0.6567 - bag_output_acc: 0.6862 - pose_output_acc: 0.8337 - footwear_output_acc: 0.6388 - emotion_output_acc: 0.7080 - val_loss: 6.8487 - val_gender_output_loss: 0.4156 - val_image_quality_output_loss: 1.0853 - val_age_output_loss: 1.3639 - val_weight_output_loss: 0.9405 - val_bag_output_loss: 0.8127 - val_pose_output_loss: 0.5020 - val_footwear_output_loss: 0.8883 - val_emotion_output_loss: 0.8404 - val_gender_output_acc: 0.8286 - val_image_quality_output_acc: 0.4768 - val_age_output_acc: 0.4042 - val_weight_output_acc: 0.6270 - val_bag_output_acc: 0.6583 - val_pose_output_acc: 0.8170 - val_footwear_output_acc: 0.6023 - val_emotion_output_acc: 0.7308\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 0.0038432354.\n",
            "180/180 [==============================] - 105s 582ms/step - loss: 6.1064 - gender_output_loss: 0.2881 - image_quality_output_loss: 0.8636 - age_output_loss: 1.2853 - weight_output_loss: 0.8713 - bag_output_loss: 0.7324 - pose_output_loss: 0.4277 - footwear_output_loss: 0.7879 - emotion_output_loss: 0.8501 - gender_output_acc: 0.8773 - image_quality_output_acc: 0.5880 - age_output_acc: 0.4386 - weight_output_acc: 0.6575 - bag_output_acc: 0.6966 - pose_output_acc: 0.8351 - footwear_output_acc: 0.6457 - emotion_output_acc: 0.7079 - val_loss: 6.8076 - val_gender_output_loss: 0.3318 - val_image_quality_output_loss: 1.1976 - val_age_output_loss: 1.3513 - val_weight_output_loss: 0.9080 - val_bag_output_loss: 0.7963 - val_pose_output_loss: 0.4945 - val_footwear_output_loss: 0.8717 - val_emotion_output_loss: 0.8564 - val_gender_output_acc: 0.8553 - val_image_quality_output_acc: 0.4264 - val_age_output_acc: 0.4133 - val_weight_output_acc: 0.6482 - val_bag_output_acc: 0.6699 - val_pose_output_acc: 0.8145 - val_footwear_output_acc: 0.6064 - val_emotion_output_acc: 0.7228\n",
            "Epoch 44/100\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 0.003673431.\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 43/100\n",
            "180/180 [==============================] - 104s 580ms/step - loss: 6.0931 - gender_output_loss: 0.2949 - image_quality_output_loss: 0.8617 - age_output_loss: 1.2852 - weight_output_loss: 0.8694 - bag_output_loss: 0.7336 - pose_output_loss: 0.4183 - footwear_output_loss: 0.7790 - emotion_output_loss: 0.8510 - gender_output_acc: 0.8742 - image_quality_output_acc: 0.5916 - age_output_acc: 0.4376 - weight_output_acc: 0.6600 - bag_output_acc: 0.6933 - pose_output_acc: 0.8411 - footwear_output_acc: 0.6510 - emotion_output_acc: 0.7076 - val_loss: 6.7488 - val_gender_output_loss: 0.3241 - val_image_quality_output_loss: 1.1628 - val_age_output_loss: 1.3451 - val_weight_output_loss: 0.9098 - val_bag_output_loss: 0.8106 - val_pose_output_loss: 0.5050 - val_footwear_output_loss: 0.8339 - val_emotion_output_loss: 0.8574 - val_gender_output_acc: 0.8639 - val_image_quality_output_acc: 0.4289 - val_age_output_acc: 0.4143 - val_weight_output_acc: 0.6462 - val_bag_output_acc: 0.6527 - val_pose_output_acc: 0.8115 - val_footwear_output_acc: 0.6300 - val_emotion_output_acc: 0.7248\n",
            "Epoch 45/100\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 0.0035075251.\n",
            "180/180 [==============================] - 105s 586ms/step - loss: 6.0677 - gender_output_loss: 0.2827 - image_quality_output_loss: 0.8619 - age_output_loss: 1.2828 - weight_output_loss: 0.8662 - bag_output_loss: 0.7268 - pose_output_loss: 0.4157 - footwear_output_loss: 0.7816 - emotion_output_loss: 0.8499 - gender_output_acc: 0.8801 - image_quality_output_acc: 0.5920 - age_output_acc: 0.4451 - weight_output_acc: 0.6569 - bag_output_acc: 0.6930 - pose_output_acc: 0.8412 - footwear_output_acc: 0.6471 - emotion_output_acc: 0.7085 - val_loss: 6.6642 - val_gender_output_loss: 0.3171 - val_image_quality_output_loss: 1.0595 - val_age_output_loss: 1.3667 - val_weight_output_loss: 0.9341 - val_bag_output_loss: 0.8005 - val_pose_output_loss: 0.5385 - val_footwear_output_loss: 0.8120 - val_emotion_output_loss: 0.8359 - val_gender_output_acc: 0.8725 - val_image_quality_output_acc: 0.4970 - val_age_output_acc: 0.4047 - val_weight_output_acc: 0.6331 - val_bag_output_acc: 0.6673 - val_pose_output_acc: 0.8201 - val_footwear_output_acc: 0.6447 - val_emotion_output_acc: 0.7268\n",
            "Epoch 46/100\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 0.003345678.\n",
            "180/180 [==============================] - 105s 582ms/step - loss: 6.0608 - gender_output_loss: 0.2835 - image_quality_output_loss: 0.8617 - age_output_loss: 1.2777 - weight_output_loss: 0.8647 - bag_output_loss: 0.7240 - pose_output_loss: 0.4253 - footwear_output_loss: 0.7764 - emotion_output_loss: 0.8475 - gender_output_acc: 0.8815 - image_quality_output_acc: 0.5957 - age_output_acc: 0.4384 - weight_output_acc: 0.6615 - bag_output_acc: 0.7009 - pose_output_acc: 0.8346 - footwear_output_acc: 0.6520 - emotion_output_acc: 0.7081 - val_loss: 6.7205 - val_gender_output_loss: 0.3865 - val_image_quality_output_loss: 1.0529 - val_age_output_loss: 1.3358 - val_weight_output_loss: 0.9187 - val_bag_output_loss: 0.8126 - val_pose_output_loss: 0.5035 - val_footwear_output_loss: 0.8782 - val_emotion_output_loss: 0.8324 - val_gender_output_acc: 0.8493 - val_image_quality_output_acc: 0.4924 - val_age_output_acc: 0.4148 - val_weight_output_acc: 0.6356 - val_bag_output_acc: 0.6623 - val_pose_output_acc: 0.8216 - val_footwear_output_acc: 0.5993 - val_emotion_output_acc: 0.7293\n",
            "Epoch 47/100\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 0.00318803.\n",
            "180/180 [==============================] - 105s 586ms/step - loss: 6.0677 - gender_output_loss: 0.2827 - image_quality_output_loss: 0.8619 - age_output_loss: 1.2828 - weight_output_loss: 0.8662 - bag_output_loss: 0.7268 - pose_output_loss: 0.4157 - footwear_output_loss: 0.7816 - emotion_output_loss: 0.8499 - gender_output_acc: 0.8801 - image_quality_output_acc: 0.5920 - age_output_acc: 0.4451 - weight_output_acc: 0.6569 - bag_output_acc: 0.6930 - pose_output_acc: 0.8412 - footwear_output_acc: 0.6471 - emotion_output_acc: 0.7085 - val_loss: 6.6642 - val_gender_output_loss: 0.3171 - val_image_quality_output_loss: 1.0595 - val_age_output_loss: 1.3667 - val_weight_output_loss: 0.9341 - val_bag_output_loss: 0.8005 - val_pose_output_loss: 0.5385 - val_footwear_output_loss: 0.8120 - val_emotion_output_loss: 0.8359 - val_gender_output_acc: 0.8725 - val_image_quality_output_acc: 0.4970 - val_age_output_acc: 0.4047 - val_weight_output_acc: 0.6331 - val_bag_output_acc: 0.6673 - val_pose_output_acc: 0.8201 - val_footwear_output_acc: 0.6447 - val_emotion_output_acc: 0.7268\n",
            "180/180 [==============================] - 105s 583ms/step - loss: 6.0393 - gender_output_loss: 0.2844 - image_quality_output_loss: 0.8570 - age_output_loss: 1.2729 - weight_output_loss: 0.8672 - bag_output_loss: 0.7169 - pose_output_loss: 0.4114 - footwear_output_loss: 0.7820 - emotion_output_loss: 0.8476 - gender_output_acc: 0.8821 - image_quality_output_acc: 0.5934 - age_output_acc: 0.4465 - weight_output_acc: 0.6562 - bag_output_acc: 0.6964 - pose_output_acc: 0.8400 - footwear_output_acc: 0.6467 - emotion_output_acc: 0.7086 - val_loss: 6.6942 - val_gender_output_loss: 0.3569 - val_image_quality_output_loss: 1.0816 - val_age_output_loss: 1.3474 - val_weight_output_loss: 0.9224 - val_bag_output_loss: 0.8138 - val_pose_output_loss: 0.5177 - val_footwear_output_loss: 0.8139 - val_emotion_output_loss: 0.8404 - val_gender_output_acc: 0.8558 - val_image_quality_output_acc: 0.4572 - val_age_output_acc: 0.4032 - val_weight_output_acc: 0.6457 - val_bag_output_acc: 0.6623 - val_pose_output_acc: 0.8029 - val_footwear_output_acc: 0.6391 - val_emotion_output_acc: 0.7283\n",
            "Epoch 00047: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f147deb6198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9Qcz9_GQANw",
        "colab_type": "code",
        "outputId": "f24a8cfb-751e-47b8-8835-dbd7d668b1f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "def evaluate_model(model):\n",
        "  results = model.evaluate_generator(valid_gen, verbose=1)\n",
        "  accuracies = {}\n",
        "  losses = {}\n",
        "  for k, v in zip(model.metrics_names, results):\n",
        "    if k.endswith('acc'):\n",
        "      accuracies[k] = round(v * 100, 4)\n",
        "    else:\n",
        "      losses[k] = v\n",
        "  return accuracies\n",
        "\n",
        "evaluate_model(model)\n",
        "\n",
        "# results = model.evaluate_generator(valid_gen, verbose =1)\n",
        "# dict(zip(model.metrics_names, results))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31/31 [==============================] - 6s 196ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output_acc': 40.3226,\n",
              " 'bag_output_acc': 66.2298,\n",
              " 'emotion_output_acc': 72.8327,\n",
              " 'footwear_output_acc': 63.9113,\n",
              " 'gender_output_acc': 85.5847,\n",
              " 'image_quality_output_acc': 45.7157,\n",
              " 'pose_output_acc': 80.2923,\n",
              " 'weight_output_acc': 64.5665}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-nJYdSOQAGn",
        "colab_type": "code",
        "outputId": "0f772996-e58e-4b75-e05b-c1cc38acf4a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(1,1,figsize=(15,5))\n",
        "    # summarize history for accuracy\n",
        "    # axs[0].plot(range(1,len(model_history.history.history['acc'])+1),model_history.history['acc'])\n",
        "    # axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    # axs[0].set_title('Model Accuracy')\n",
        "    # axs[0].set_ylabel('Accuracy')\n",
        "    # axs[0].set_xlabel('Epoch')\n",
        "    # axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    # axs[0].legend(['train', 'val'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs.plot(range(1,len(model_history.history.history['loss'])+1),model_history.history.history['loss'])\n",
        "    axs.plot(range(1,len(model_history.history.history['val_loss'])+1),model_history.history.history['val_loss'])\n",
        "    axs.set_title('Model Loss')\n",
        "    axs.set_ylabel('Loss')\n",
        "    axs.set_xlabel('Epoch')\n",
        "    axs.set_xticks(np.arange(1,len(model_history.history.history['loss'])+1),len(model_history.history.history['loss'])/10)\n",
        "    axs.legend(['train', 'val'], loc='best')\n",
        "    plt.show()\n",
        "# plot model history\n",
        "plot_model_history(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3UAAAFNCAYAAACnuEbJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA7Vki4-QAQx",
        "colab_type": "code",
        "outputId": "2b67fd40-d362-4e1f-d643-0e2d51e8e6f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "  return round(lr * 1/(1 + decay_factor * epoch), 10)\n",
        "\n",
        "lr = 0.007\n",
        "decay_factor = 0.004\n",
        "epoch = 100\n",
        "for i in range(epoch):\n",
        "  lr_new = scheduler(i, lr)\n",
        "  lr = lr_new\n",
        "  if i%5 == 0:\n",
        "    print(\"the epoch number is: \" + str(i) + \" and LR is: \" + str(round(lr,10)))\n",
        "  i = i+1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the epoch number is: 0 and LR is: 0.007\n",
            "the epoch number is: 5 and LR is: 0.0065952219\n",
            "the epoch number is: 10 and LR is: 0.005634606\n",
            "the epoch number is: 15 and LR is: 0.0043733703\n",
            "the epoch number is: 20 and LR is: 0.0030893887\n",
            "the epoch number is: 25 and LR is: 0.0019897041\n",
            "the epoch number is: 30 and LR is: 0.0011702877\n",
            "the epoch number is: 35 and LR is: 0.0006296332\n",
            "the epoch number is: 40 and LR is: 0.0003103498\n",
            "the epoch number is: 45 and LR is: 0.0001403585\n",
            "the epoch number is: 50 and LR is: 5.83287e-05\n",
            "the epoch number is: 55 and LR is: 2.23045e-05\n",
            "the epoch number is: 60 and LR is: 7.859e-06\n",
            "the epoch number is: 65 and LR is: 2.5548e-06\n",
            "the epoch number is: 70 and LR is: 7.673e-07\n",
            "the epoch number is: 75 and LR is: 2.132e-07\n",
            "the epoch number is: 80 and LR is: 5.48e-08\n",
            "the epoch number is: 85 and LR is: 1.31e-08\n",
            "the epoch number is: 90 and LR is: 2.9e-09\n",
            "the epoch number is: 95 and LR is: 6e-10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1WB81PaN2ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgYCMe1chjKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}